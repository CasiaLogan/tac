{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229a73ca",
   "metadata": {},
   "source": [
    "# Analyse de la dist. du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c004a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9734c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\casia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') #liste pré-définie de stopwords\n",
    "from nltk.corpus import stopwords #attention de bien nettoyer les stopwords !! sinon résultats bizarres\n",
    "\n",
    "import os\n",
    "import yake\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud #besoin de compilateur c++ pour l'installer\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7441efd",
   "metadata": {},
   "source": [
    "### Gerer les stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6e5c",
   "metadata": {},
   "source": [
    "#### Créer une liste des stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "284e0c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 stopwords:\n",
      " ['-', ':', 'DRAPEAU', 'LE', 'ROUGE', 'a', 'abord', 'absolument', 'afin', 'ah', 'ai', 'aie', 'aient', 'aies', 'ailleurs', 'ainsi', 'ait', 'allaient', 'allo', 'allons', 'allô', 'alors', 'année', 'ans', 'anterieur', 'anterieure', 'anterieures', 'apres', 'après', 'as', 'assez', 'attendu', 'au', 'aucun', 'aucune', 'aucuns', 'aujourd', \"aujourd'hui\", 'aupres', 'auquel', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autant', 'autre', 'autrefois', 'autrement', 'autres', 'autrui', 'aux', 'auxquelles', 'auxquels', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayante', 'ayantes', 'ayants', 'ayez', 'ayons', 'bah', 'bas', 'basee', 'bat', 'beau', 'beaucoup', 'bien', 'bigre', 'bon', 'boum', 'bravo', 'c', 'car', 'ce', 'ceci', 'cela', 'celle', 'celle-ci', 'celle-là', 'celles', 'celles-ci', 'celles-là', 'celui', 'celui-ci', 'celui-là', 'celà', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'ces', 'cet', 'cette', 'ceux', 'ceux-ci', 'ceux-là', 'chacun', 'chacune', 'chaque', 'cher', 'chers', 'chez', 'chiche', 'chut', 'chère', 'chères', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantième', 'cinquième', 'clac', 'clic', 'com', 'combien', 'comme', 'comment', 'comparable', 'comparables', 'compris', 'concernant', 'contre', 'couic', 'crac', 'd', \"d'un\", \"d'une\", 'dans', 'de', 'debout', 'dedans', 'dehors', 'deja', 'delà', 'depuis', 'dernier', 'derniere', 'derriere', 'derrière', 'des', 'desormais', 'desquelles', 'desquels', 'dessous', 'dessus', 'deux', 'deuxième', 'deuxièmement', 'devant', 'devers', 'devra', 'devrait', 'different', 'differentes', 'differents', 'différent', 'différente', 'différentes', 'différents', 'dire', 'directe', 'directement', 'dit', 'dite', 'dits', 'divers', 'diverse', 'diverses', 'dix', 'dix-huit', 'dix-neuf', 'dix-sept', 'dixième', 'doit', 'doivent', 'donc', 'dont', 'dos', 'douze', 'douzième', 'drapeau', 'dring', 'droite', 'du', 'duquel', 'durant', 'dès', 'début', 'désormais', 'effet', 'egale', 'egalement', 'egales', 'eh', 'elle', 'elle-même', 'elles', 'elles-mêmes', 'en', 'encore', 'enfin', 'entre', 'envers', 'environ', 'es', 'essai', 'est', 'et', 'etant', 'etc', 'etre', 'eu', 'eue', 'eues', 'euh', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eux-mêmes', 'exactement', 'excepté', 'extenso', 'exterieur', 'eûmes', 'eût', 'eûtes', 'f', 'faire', 'fais', 'faisaient', 'faisant', 'fait', 'faites', 'faut', 'façon', 'feront', 'fi', 'flac', 'floc', 'fois', 'font', 'force', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'gens', 'haut', 'hein', 'hem', 'hep', 'het', 'holà', 'hop', 'hormis', 'hors', 'hou', 'houp', 'hue', 'hui', 'huit', 'huitième', 'hum', 'hurrah', 'hé', 'hélas', 'i', 'ici', 'il', 'ils', 'importe', 'j', 'je', 'jour', 'jusqu', 'jusque', 'juste', 'l', \"l'on\", 'la', 'laisser', 'laquelle', 'las', 'le', 'lequel', 'les', 'lesquelles', 'lesquels', 'leur', 'leurs', 'longtemps', 'lors', 'lorsque', 'lui', 'lui-meme', 'lui-même', 'là', 'lès', 'm', 'ma', 'maint', 'maintenant', 'mais', 'malgre', 'malgré', 'maximale', 'me', 'meme', 'memes', 'merci', 'mes', 'mien', 'mienne', 'miennes', 'miens', 'mille', 'mince', 'mine', 'minimale', 'moi', 'moi-meme', 'moi-même', 'moindres', 'moins', 'mon', 'mot', 'moyennant', 'multiple', 'multiples', 'même', 'mêmes', 'n', \"n'anaturel\", 'na', 'naturelle', 'naturelles', 'ne', 'neanmoins', 'necessaire', 'necessairement', 'neuf', 'neuvième', 'ni', 'nombreuses', 'nombreux', 'nommés', 'non', 'nos', 'notamment', 'notre', 'nous', 'nous-mêmes', 'nouveau', 'nouveaux', 'nul', 'néanmoins', 'nôtre', 'nôtres', 'oh', 'ohé', 'ollé', 'olé', 'on', 'ont', 'onze', 'onzième', 'ore', 'ou', 'ouf', 'ouias', 'oust', 'ouste', 'outre', 'ouvert', 'ouverte', 'ouverts', 'où', 'paf', 'par', 'parce', 'parfois', 'parle', 'parlent', 'parler', 'parmi', 'parole', 'parseme', 'partant', 'particulier', 'particulière', 'particulièrement', 'pas', 'passé', 'pendant', 'pense', 'permet', 'personne', 'personnes', 'peu', 'peut', 'peuvent', 'peux', 'pff', 'pfft', 'pfut', 'pif', 'pire', 'pièce', 'plein', 'plouf', 'plupart', 'plus', 'plusieurs', 'plutôt', 'possessif', 'possessifs', 'possible', 'possibles', 'pouah', 'pour', 'pourquoi', 'pourrais', 'pourrait', 'pouvait', 'prealable', 'precisement', 'premier', 'première', 'premièrement', 'pres', 'probable', 'probante', 'procedant', 'proche', 'près', 'psitt', 'pu', 'puis', 'puisque', 'pur', 'pure', 'qu', \"qu'elle\", \"qu'elles\", \"qu'il\", \"qu'ils\", 'quand', 'quant', 'quant-à-soi', 'quanta', 'quarante', 'quatorze', 'quatre', 'quatre-vingt', 'quatrième', 'quatrièmement', 'que', 'quel', 'quelconque', 'quelle', 'quelles', \"quelqu'un\", 'quelque', 'quelques', 'quels', 'qui', 'quiconque', 'quinze', 'quoi', 'quoique', 'r', 'rare', 'rarement', 'rares', 'relative', 'relativement', 'remarquable', 'rend', 'rendre', 'restant', 'reste', 'restent', 'restrictif', 'retour', 'revoici', 'revoilà', 'rien', 'rue', 's', 'sa', 'sacrebleu', 'sait', 'sans', 'sapristi', 'sauf', 'se', 'sein', 'seize', 'selon', 'semblable', 'semblaient', 'semble', 'semblent', 'sent', 'sept', 'septième', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seul', 'seule', 'seulement', 'si', 'sien', 'sienne', 'siennes', 'siens', 'sinon', 'six', 'sixième', 'soi', 'soi-même', 'soient', 'sois', 'soit', 'soixante', 'sommes', 'son', 'sont', 'sous', 'souvent', 'soyez', 'soyons', 'specifique', 'specifiques', 'speculatif', 'stop', 'strictement', 'subtiles', 'suffisant', 'suffisante', 'suffit', 'suis', 'suit', 'suivant', 'suivante', 'suivantes', 'suivants', 'suivre', 'sujet', 'superpose', 'sur', 'surtout', 't', 'ta', 'tac', 'tandis', 'tant', 'tardive', 'te', 'tel', 'telle', 'tellement', 'telles', 'tels', 'tenant', 'tend', 'tenir', 'tente', 'tes', 'tic', 'tien', 'tienne', 'tiennes', 'tiens', 'toc', 'toi', 'toi-même', 'ton', 'touchant', 'toujours', 'tous', 'tout', 'toute', 'toutefois', 'toutes', 'treize', 'trente', 'tres', 'trois', 'troisième', 'troisièmement', 'trop', 'très', 'tsoin', 'tsouin', 'tu', 'té', 'un', 'une', 'unes', 'uniformement', 'unique', 'uniques', 'uns', 'va', 'vais', 'valeur', 'van', 'vas', 'vers', 'via', 'vif', 'vifs', 'vingt', 'vivat', 'vive', 'vives', 'vlan', 'voici', 'voie', 'voient', 'voilà', 'voir', 'voire', 'vont', 'vos', 'votre', 'vous', 'vous-mêmes', 'vu', 'vé', 'vôtre', 'vôtres', 'y', 'zut', 'à', 'â', 'ça', 'ès', 'étaient', 'étais', 'était', 'étant', 'étante', 'étantes', 'étants', 'état', 'étiez', 'étions', 'été', 'étée', 'étées', 'étés', 'êtes', 'être', 'ô']\n"
     ]
    }
   ],
   "source": [
    "#jouer encore avec cela pour en ajouter des nouveaux en fonction du contexte, modifié de la liste de https://github.com/stopwords-iso/stopwords-fr?tab=readme-ov-file, à verifer sa fidelité\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \"elles\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \":\", \"même\", \"voir\", \"fois\", \"jour\", \"année\", \"ans\", \"faites\", \"le\", \"la\", \"de\"]\n",
    "\n",
    "# New stopwords to add\n",
    "git_stopwords = [\n",
    "    \"a\", \"abord\", \"absolument\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\", \"aies\", \"ailleurs\", \"ainsi\", \"ait\", \"allaient\", \"allo\", \"allons\", \"allô\", \"alors\", \"anterieur\", \"anterieure\", \"anterieures\", \"apres\", \"après\", \"as\", \"assez\", \"attendu\", \"au\", \"aucun\", \"aucune\", \"aucuns\", \"aujourd\", \"aujourd'hui\", \"aupres\", \"auquel\", \"aura\", \"aurai\", \"auraient\", \"aurais\", \"aurait\", \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autant\", \"autre\", \"autrefois\", \"autrement\", \"autres\", \"autrui\", \"aux\", \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\", \"avez\", \"aviez\", \"avions\", \"avoir\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"bah\", \"bas\", \"basee\", \"bat\", \"beau\", \"beaucoup\", \"bien\", \"bigre\", \"bon\", \"boum\", \"bravo\", \"car\", \"ce\", \"ceci\", \"cela\", \"celle\", \"celle-ci\", \"celle-là\", \"celles\", \"celles-ci\", \"celles-là\", \"celui\", \"celui-ci\", \"celui-là\", \"celà\", \"cent\", \"cependant\", \"certain\", \"certaine\", \"certaines\", \"certains\", \"certes\", \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\", \"chacun\", \"chacune\", \"chaque\", \"cher\", \"chers\", \"chez\", \"chiche\", \"chut\", \"chère\", \"chères\", \"ci\", \"cinq\", \"cinquantaine\", \"cinquante\", \"cinquantième\", \"cinquième\", \"clac\", \"clic\", \"combien\", \"comme\", \"comment\", \"comparable\", \"comparables\", \"compris\", \"concernant\", \"contre\", \"couic\", \"crac\", \"dans\", \"de\", \"debout\", \"dedans\", \"dehors\", \"deja\", \"delà\", \"depuis\", \"dernier\", \"derniere\", \"derriere\", \"derrière\", \"des\", \"desormais\", \"desquelles\", \"desquels\", \"dessous\", \"dessus\", \"deux\", \"deuxième\", \"deuxièmement\", \"devant\", \"devers\", \"devra\", \"devrait\", \"different\", \"differentes\", \"differents\", \"différent\", \"différente\", \"différentes\", \"différents\", \"dire\", \"directe\", \"directement\", \"dit\", \"dite\", \"dits\", \"divers\", \"diverse\", \"diverses\", \"dix\", \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dixième\", \"doit\", \"doivent\", \"donc\", \"dont\", \"dos\", \"douze\", \"douzième\", \"dring\", \"droite\", \"du\", \"duquel\", \"durant\", \"dès\", \"début\", \"désormais\", \"effet\", \"egale\", \"egalement\", \"egales\", \"eh\", \"elle\", \"elle-même\", \"elles\", \"elles-mêmes\", \"en\", \"encore\", \"enfin\", \"entre\", \"envers\", \"environ\", \"es\", \"essai\", \"est\", \"et\", \"etant\", \"etc\", \"etre\", \"eu\", \"eue\", \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\", \"eusses\", \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eux-mêmes\", \"exactement\", \"excepté\", \"extenso\", \"exterieur\", \"eûmes\", \"eût\", \"eûtes\", \"f\", \"fais\", \"faisaient\", \"faisant\", \"fait\", \"faites\", \"façon\", \"feront\", \"fi\", \"flac\", \"floc\", \"fois\", \"font\", \"force\", \"furent\", \"fus\", \"fusse\", \"fussent\", \"fusses\", \"fussiez\", \"fussions\", \"fut\", \"fûmes\", \"fût\", \"fûtes\", \"gens\", \"haut\", \"hein\", \"hem\", \"hep\", \"holà\", \"hop\", \"hormis\", \"hors\", \"hou\", \"houp\", \"hue\", \"hui\", \"huit\", \"huitième\", \"hum\", \"hurrah\", \"hé\", \"hélas\", \"i\", \"ici\", \"il\", \"ils\", \"importe\", \"j\", \"je\", \"jusqu\", \"jusque\", \"juste\", \"la\", \"laisser\", \"laquelle\", \"las\", \"le\", \"lequel\", \"les\", \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"longtemps\", \"lors\", \"lorsque\", \"lui\", \"lui-meme\", \"lui-même\", \"là\", \"lès\", \"ma\", \"maint\", \"maintenant\", \"mais\", \"malgre\", \"malgré\", \"maximale\", \"me\", \"meme\", \"memes\", \"merci\", \"mes\", \"mien\", \"mienne\", \"miennes\", \"miens\", \"mille\", \"mince\", \"mine\", \"minimale\", \"moi\", \"moi-meme\", \"moi-même\", \"moindres\", \"moins\", \"mon\", \"mot\", \"moyennant\", \"multiple\", \"multiples\", \"même\", \"mêmes\", \"na\", \"n'a\" \"naturel\", \"naturelle\", \"naturelles\", \"ne\", \"neanmoins\", \"necessaire\", \"necessairement\", \"neuf\", \"neuvième\", \"ni\", \"nombreuses\", \"nombreux\", \"nommés\", \"non\", \"nos\", \"notamment\", \"notre\", \"nous\", \"nous-mêmes\", \"nouveau\", \"nouveaux\", \"nul\", \"néanmoins\", \"nôtre\", \"nôtres\", \"oh\", \"ohé\", \"ollé\", \"olé\", \"on\", \"ont\", \"onze\", \"onzième\", \"ore\", \"ou\", \"ouf\", \"ouias\", \"oust\", \"ouste\", \"outre\", \"ouvert\", \"ouverte\", \"ouverts\", \"où\", \"paf\", \"par\", \"parce\", \"parfois\", \"parle\", \"parlent\", \"parler\", \"parmi\", \"parole\", \"parseme\", \"partant\", \"particulier\", \"particulière\", \"particulièrement\", \"pas\", \"passé\", \"pendant\", \"pense\", \"permet\", \"personne\", \"personnes\", \"peu\", \"peut\", \"peuvent\", \"peux\", \"pff\", \"pfft\", \"pfut\", \"pif\", \"pire\", \"pièce\", \"plein\", \"plouf\", \"plupart\", \"plus\", \"plusieurs\", \"plutôt\", \"possessif\", \"possessifs\", \"possible\", \"possibles\", \"pouah\", \"pour\", \"pourquoi\", \"pourrais\", \"pourrait\", \"pouvait\", \"prealable\", \"precisement\", \"premier\", \"première\", \"premièrement\", \"pres\", \"probable\", \"probante\", \"procedant\", \"proche\", \"près\", \"psitt\", \"pu\", \"puis\", \"puisque\", \"pur\", \"pure\", \"qu\", \"quand\", \"quant\", \"quant-à-soi\", \"quanta\", \"quarante\", \"quatorze\", \"quatre\", \"quatre-vingt\", \"quatrième\", \"quatrièmement\", \"que\", \"quel\", \"quelconque\", \"quelle\", \"quelles\", \"quelqu'un\", \"quelque\", \"quelques\", \"quels\", \"qui\", \"quiconque\", \"quinze\", \"quoi\", \"quoique\", \"r\", \"rare\", \"rarement\", \"rares\", \"relative\", \"relativement\", \"remarquable\", \"rend\", \"rendre\", \"restant\", \"reste\", \"restent\", \"restrictif\", \"retour\", \"revoici\", \"revoilà\", \"rien\", \"sa\", \"sacrebleu\", \"sait\", \"sans\", \"sapristi\", \"sauf\", \"se\", \"sein\", \"seize\", \"selon\", \"semblable\", \"semblaient\", \"semble\", \"semblent\", \"sent\", \"sept\", \"septième\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\", \"seriez\", \"serions\", \"serons\", \"seront\", \"ses\", \"seul\", \"seule\", \"seulement\", \"si\", \"sien\", \"sienne\", \"siennes\", \"siens\", \"sinon\", \"six\", \"sixième\", \"soi\", \"soi-même\", \"soient\", \"sois\", \"soit\", \"soixante\", \"sommes\", \"son\", \"sont\", \"sous\", \"souvent\", \"soyez\", \"soyons\", \"specifique\", \"specifiques\", \"speculatif\", \"stop\", \"strictement\", \"subtiles\", \"suffisant\", \"suffisante\", \"suffit\", \"suis\", \"suit\", \"suivant\", \"suivante\", \"suivantes\", \"suivants\", \"suivre\", \"sujet\", \"superpose\", \"sur\", \"surtout\", \"ta\", \"tac\", \"tandis\", \"tant\", \"tardive\", \"te\", \"tel\", \"telle\", \"tellement\", \"telles\", \"tels\", \"tenant\", \"tend\", \"tenir\", \"tente\", \"tes\", \"tic\", \"tien\", \"tienne\", \"tiennes\", \"tiens\", \"toc\", \"toi\", \"toi-même\", \"ton\", \"touchant\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\", \"treize\", \"trente\", \"tres\", \"trois\", \"troisième\", \"troisièmement\", \"trop\", \"très\", \"tsoin\", \"tsouin\", \"tu\", \"té\", \"un\", \"une\", \"unes\", \"uniformement\", \"unique\", \"uniques\", \"uns\", \"va\", \"vais\", \"valeur\", \"vas\", \"vers\", \"via\", \"vif\", \"vifs\", \"vingt\", \"vivat\", \"vive\", \"vives\", \"vlan\", \"voici\", \"voie\", \"voient\", \"voilà\", \"voire\", \"vont\", \"vos\", \"votre\", \"vous\", \"vous-mêmes\", \"vu\", \"vé\", \"vôtre\", \"vôtres\", \"zut\", \"à\", \"â\", \"ça\", \"ès\", \"étaient\", \"étais\", \"était\", \"étant\", \"état\", \"étiez\", \"étions\", \"été\", \"étée\", \"étées\", \"étés\", \"êtes\", \"être\", \"ô\"\n",
    "]\n",
    "\n",
    "#domaine specific \n",
    "specific_stopwords = [\n",
    "\"rue\", \"-\", \"drapeau\", \"DRAPEAU\", \"LE\", \"ROUGE\", \"com\", \"qu'il\", \"d'une\", \"d'un\", \"l'on\", \"qu'il\", \"qu'ils\", \"qu'elle\", \"qu'elles\"\n",
    "#garder les noms des villes pour faire une analyse spatiale? \n",
    "#does it also include common first names?\n",
    "#does it also include uppercase versions?\n",
    "#à voir : soir, matin = seront pe liés au nom du journal et non du temps de la journée, define what a stopword means in this context\n",
    "]\n",
    "\n",
    "sw += git_stopwords\n",
    "sw += specific_stopwords\n",
    "sw = set(sw)\n",
    "\n",
    "\n",
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b9054",
   "metadata": {},
   "source": [
    "#### Tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbb7d6",
   "metadata": {},
   "source": [
    "Créer un fichier bash qui concatène tous les fichiers txt de halley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19aa408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4858920 words found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['yF',\n",
       " ':',\n",
       " 'LE',\n",
       " 'DRAPEAU',\n",
       " 'ROUGE',\n",
       " 'Edison',\n",
       " 'revendique',\n",
       " 'la',\n",
       " 'priorité',\n",
       " 'de',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'invention',\n",
       " 'du',\n",
       " 'phonographe',\n",
       " '.-',\n",
       " 'On',\n",
       " 'a',\n",
       " 'signalé',\n",
       " 'dernièrement',\n",
       " 'la',\n",
       " 'mort',\n",
       " 'du',\n",
       " 'poète',\n",
       " 'ot',\n",
       " \"'\",\n",
       " 'savant',\n",
       " 'français',\n",
       " 'Charles',\n",
       " 'Croa',\n",
       " 'considéré',\n",
       " 'com',\n",
       " '-',\n",
       " ',',\n",
       " 'mo',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'inventeur',\n",
       " 'du',\n",
       " 'phonographe',\n",
       " ',',\n",
       " 'dont',\n",
       " 'il',\n",
       " 'aurait',\n",
       " 'décrit',\n",
       " ',',\n",
       " 'le',\n",
       " 'premier',\n",
       " ',',\n",
       " 'le']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merci les profs\n",
    "path = \"../../data/halley/halley_txt/\" # Path to the directory containing text files\n",
    "\n",
    "with open(\"../../data/halley/halley_all.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                output_file.write(f.read())\n",
    "\n",
    "# Récupération du contenu du fichier bash\n",
    "path = \"../../data/halley/halley_all.txt\"\n",
    "limit = 10**8\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()[:limit]\n",
    "\n",
    "# Tokenization\n",
    "words = nltk.wordpunct_tokenize(text)\n",
    "print(f\"{len(words)} words found\")\n",
    "words[:50] #avant le filtrage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f336a",
   "metadata": {},
   "source": [
    "#### Eliminer les stopwords et les termes non alphabétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f71e73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587064 words kept (220119 different word forms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rouge',\n",
       " 'edison',\n",
       " 'revendique',\n",
       " 'priorité',\n",
       " 'invention',\n",
       " 'phonographe',\n",
       " 'signalé',\n",
       " 'dernièrement',\n",
       " 'mort',\n",
       " 'poète',\n",
       " 'savant',\n",
       " 'français',\n",
       " 'charles',\n",
       " 'croa',\n",
       " 'considéré',\n",
       " 'inventeur',\n",
       " 'phonographe',\n",
       " 'décrit',\n",
       " 'mécanisme',\n",
       " 'edison']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows words after filtering stop words, much better\n",
    "kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "voc = set(kept)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")\n",
    "kept[:20] \n",
    "#these still aren't the words I'd expect... double check the source\n",
    "#AH maybe it's because the WHOLE page isn't about halleys comet, so maybe those become stop words? ie edison? \n",
    "# or do we go for an analysis of co-text as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbce334",
   "metadata": {},
   "source": [
    "#### Calculer la taille du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heures', 4175),\n",
       " ('bruxelles', 3613),\n",
       " ('mai', 3111),\n",
       " ('grand', 3095),\n",
       " ('prix', 2688),\n",
       " ('temps', 2602),\n",
       " ('lieu', 2568),\n",
       " ('comète', 2556),\n",
       " ('francs', 2549),\n",
       " ('soir', 2445)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#récupérer la fréquence des mots => compte les doublons? \n",
    "fdist = nltk.FreqDist(kept)\n",
    "fdist.most_common(10)\n",
    "\n",
    "#heures might be on the chopping block for sw \n",
    "\n",
    "#voisins plus proches to halley would be useful\n",
    "#or maybe we do a co-text analysis of words around halley?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f98710",
   "metadata": {},
   "source": [
    "#### Variation from structure of modules \n",
    "I don't see what the plot adds, so I'm leaving it out for now\n",
    "I'm also leaving out les mots qui n'apparaissent qu'une fois dans le corpus et les plus longs pour l'instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36610332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les mots les plus longs\n",
    "n = 50\n",
    "sorted(voc, key=len, reverse=True)[:n]\n",
    "#oh TERRIBLE OCR, not a single word, but a couple of conjuctions at the end "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e8806",
   "metadata": {},
   "source": [
    "### Keywords Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66651306",
   "metadata": {},
   "source": [
    "#### Extraire les keywords du texte sans stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a401d0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yake.yake.KeywordExtractor at 0x21a8c899c50>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2ad7629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lister les fichiers, advantage is that we can see which files mention which keywords but we can also do the same with a bash eventually\n",
    "data_path = \"../../data/halley/halley_txt/\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36321d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_JB1051_1927-08-25_01-00004.txt mentions these keywords: août, heures, rayon, c'est, Jeudi, Edison, FEDERATION, étoiles, Tomes, local, soir, Charles, terre, filantes, grand, d'une, Vendredi, Samedi, Ghetto, Dimanche, Croa, phonographe, fer, habituel, rue...\n",
      "KB_JB1051_1937-11-14_01-00006.txt mentions these keywords: Baleine, Soviétique, comète, comètes, record, Boïtchenko, temps, nageur, C'est, d'une, filantes, pigeons, nage, brasse, mètres, point, Péchons, mondial, monde, ans, natation, mer, concours, Aleoute, Jour, Jean, soleil, étoiles, Moscou, novembre, grand, GRIT, qu'il, kilomètres, d'un, visibles, fois...\n",
      "KB_JB1051_1939-08-06_01-00005.txt mentions these keywords: Charles, Grandet, d'une, Eugénie, d'un, froment, faut, dit, Pavillon, PRIX, l'amour, traitement, août, mieux, gale, faire, Moscou, c'est, qualité, moyen, jour, Avoine, Tourteaux, PAGE, MEDICALE, BALLONNETS, Snfants, CHRONIQUE, LACHER, MONSTRE, voir, enfin, TERRE, garçon...\n",
      "KB_JB1051_1957-04-18_01-00004.txt mentions these keywords: Monseigneur, duc, Chine, Van, dit, Poulain, d'une, oui, mort, ment, d'un, Monsieur, qu'il, roi, avril, questions, s'est, comète, C'est, heures, l'on, ans, Mari, Paris, Carthy, congrès, fois, voir, Bruxelles, Belgique, POINT, Jours, Scheutistes, Libre, CAS, Scheut, fédéral, pilule, fut, parti...\n",
      "KB_JB1051_1957-10-14_01-00001.txt mentions these keywords: Congo, octobre, COMMUNISTE, ment, JACQUEMOTTE, presse, heures, Terre, ministre, Part, Hier, fusée, Kanze, SPOUTNIK, CAPITALISTES, soviétique, qu'il, qu'ils, prise, Comité, con, Tubize, tre, Belgique, également, Ekatou, Moulin, billets, s'est, pays, question, guerre, faire, travail, déjà, tion, PARTI, nom, n'est, matin, BRUXELLES...\n",
      "KB_JB421_1902-10-19_01-00001.txt mentions these keywords: C'est, ligne, mort, qu'on, boers, d'un, Belgique, Luxembourg, Frassem, ABONNEMENT, Bruxelles, Hier, ville, page, Zola, Paris, journal, Comète, roi, paix, jour, coup, francs, juges, Henry, ANNONCES, Eglise, grande, bon...\n",
      "KB_JB421_1907-10-26_01-00002.txt mentions these keywords: Rubens, C'est, jours, Renaissance, grand, dos, peintre, fut, jour, los, l'art, pays, religieux, peinture, temps, quo, d'un, van, grands, nouveau, uno, religion, d'une, grande, Mites, n'est, maison, artistes, soleil, chrétien, Belgique, fin, furent, haute, dernier...\n",
      "KB_JB421_1909-05-07_01-00003.txt mentions these keywords: mais, D'un, d'une, Arlon, ans, jour, matin, jours, qu'elle, C'est, temps, qu'il, grande, s'est, saint, Point, ENFANT, Chronique, Régionale, Mémorandum, jeune, marche, MAI, grand, soleil, nuit, police, heures, Belgique, pris, Servin, hier, bourgmestre, Epinettes, mort, Vaucoret, midi...\n",
      "KB_JB421_1909-08-06_01-00001.txt mentions these keywords: Cologne, c'est, Congrès, qu'il, s'est, francs, Roi, marks, Belgique, Salésiens, cathédrale, heures, vue, NONCES, ville, monde, d'une, ministre, temps, jour, Albert, Bruxelles, cardinal, catholiques, d'un, nom, faire, traitement, chancelier, qu'on, Dom, maisons, société, année, grande, nouvelles, villes, pension, conseil...\n",
      "KB_JB421_1909-10-23_01-00001.txt mentions these keywords: ligne, d'un, qu'il, rue, Belgique, service, qu'ils, bons, faire, bon, prince, Albert, c'est, d'une, l'on, VIEUX, Faits-drt, Jugements, qu'elle, fut, visiteurs, hommes, rues, fois, maîtres, Ferrer, Palais, comète...\n"
     ]
    }
   ],
   "source": [
    "for f in sorted(files)[:10]: # Limiter à 10 fichiers pour l'exemple, need to do a bash of all? after checking stop words?\n",
    "    text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read() # Récupérer le texte du fichier\n",
    "    keywords = kw_extractor.extract_keywords(text) # Extraire les mots clés de ce texte\n",
    "    \n",
    "    kept = [] \n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 1: # Ne garder que les bigrammes\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")\n",
    "\n",
    "#what would be the benefit of having a big list of all of the keywords from all of the files, and then do a frequency analysis on that list to see what are the most common keywords across all of the texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef281a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to get all of the keywords from all of the texts into a single list, so they can then be filtered for stopwords and analysed for frequency\n",
    "#this seemed to big, it was taking ages to run, so maybe we do it file by file and then combine the results after checking stop words\n",
    "all_keywords = []\n",
    "for f in sorted(files):\n",
    "    text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read() # Récupérer le texte du fichier\n",
    "    keywords = kw_extractor.extract_keywords(text) # Extraire les mots clés de ce texte\n",
    "    \n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 1: # Ne garder que les bigrammes\n",
    "            all_keywords.append(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ligne', 0.006576956516920981),\n",
       " ('grande ligne', 0.010414814932783081),\n",
       " ('jjjjgjs et Jugements', 0.012405281434844163),\n",
       " (\"d'un\", 0.012768895090072173),\n",
       " (\"qu'il\", 0.01404661186692881),\n",
       " ('rue', 0.018646918396135623),\n",
       " ('petite ligne', 0.022057795950928976),\n",
       " ('Belgique', 0.022894960513796105),\n",
       " ('Faits-drt ANNONCES', 0.02459954981037721),\n",
       " ('service', 0.02593999816391375),\n",
       " (\"qu'ils\", 0.032775427689500564),\n",
       " ('bons', 0.03291445281262834),\n",
       " ('faire', 0.03583407481960633),\n",
       " ('bons vieux serviteurs', 0.03631056971969405),\n",
       " ('TRAITE A FORFAIT', 0.03783214099594524),\n",
       " ('bon', 0.0384001949480664),\n",
       " ('prince', 0.039173282293782995),\n",
       " ('Albert', 0.041280351933826676),\n",
       " ('prince Albert', 0.04207603403127464),\n",
       " (\"c'est\", 0.042311261418854614),\n",
       " (\"d'une\", 0.044920043564026534),\n",
       " ('rue Clément XIV', 0.04628643594921162),\n",
       " (\"qu'il possède rue\", 0.05113952925236211),\n",
       " (\"l'on\", 0.053223547434741274),\n",
       " ('VIEUX', 0.054648719572990195),\n",
       " (\"service d'un\", 0.05596176939282363),\n",
       " (\"service d'un réel\", 0.06172615335668248),\n",
       " ('Faits-drt', 0.06344636920368098),\n",
       " ('Jugements', 0.06344636920368098),\n",
       " (\"qu'elle\", 0.06537325387176664),\n",
       " (\"maîtres qu'ils\", 0.06571725664248096),\n",
       " (\"qu'ils out Sru\", 0.06646114148363362),\n",
       " ('fut', 0.06771678728633244),\n",
       " (\"rue d'Argeni ixellea\", 0.06795878962731165),\n",
       " ('visiteurs', 0.0700874380461356),\n",
       " ('hommes', 0.07198343152781589),\n",
       " (\"maîtres qu'ils obtien\", 0.0732270707670453),\n",
       " (\"nom d'un\", 0.07344637932889118),\n",
       " ('rues', 0.07458767358454249),\n",
       " ('VIEUX DOMESTIQUES', 0.07481074021820261),\n",
       " ('vieux braves serviteurs', 0.0759595325253067),\n",
       " ('fois', 0.07617135785874536),\n",
       " (\"vieux hasselt qu'il\", 0.07677189715384393),\n",
       " ('maîtres', 0.07839309630159925),\n",
       " ('service général', 0.07841787651101065),\n",
       " ('Ferrer', 0.07912366810476722),\n",
       " ('bons vieux', 0.08172049304546637),\n",
       " ('Palais', 0.08261737850588843),\n",
       " ('comète', 0.08294105788268245),\n",
       " (\"Bruxelles D'un relevé\", 0.08302961467871471)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraire les mots clés du text bash \n",
    "\n",
    "keywords = kw_extractor.extract_keywords(text) #I think text here is the whole halley text\n",
    "#doesn't look like stopwords were removed here\n",
    "\n",
    "keywords #ummm how does it decide what is a keyword? See the yake documentation for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb771d6",
   "metadata": {},
   "source": [
    "Keywords Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecc000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yF: LE DRAPEAU ROUGE Edison revendique la priorité de l'invention du phonographe .- On a signalé dernièrement la mort du poète ot'savant français Charles Croa considéré com- , mo l'inventeur du phonographe, dont il aurait décrit, le premier, le mécanisme. Edison n'aurait l'ait qu'appliquer et développer l'invention du savant fronçais. Interrogé sur ce point, Edison vient de donne! les précisions suivantes : « — Jo no puis vous dire que ceci : c'est en ■ juillet 1977 que je conçus l'idée de mon p\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choisir un fichier => actually I think I want this to do all of the files, if this is the analysis I'm thinking of but let's test as is for now\n",
    "#maybe I could do just the texts during may 1910? I could filter the files based on date either using the setup from preparer_halley or just split the names again\n",
    "\n",
    "this_file = files[0]\n",
    "this_file\n",
    "\n",
    "# Récupérer le texte du fichier\n",
    "text = open(os.path.join(data_path, this_file), 'r', encoding='utf-8').read()\n",
    "text[:500] #gets first 500 characters, Just to check the beginning of the text, I can change this later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ts6rq1nt5ck",
   "metadata": {},
   "source": [
    "### Extract Keywords from All Files with Time Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "k0mqw6guetm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_from_halley_texts(specific_years=None, specific_months=None, ngram_size=1):\n",
    "    \"\"\"\n",
    "    Extract keywords from Halley texts with optional time filtering.\n",
    "\n",
    "    Based on the structure from preparer_halley.ipynb for filename parsing\n",
    "    and filtering by time period. Processes files individually (not halley_all.txt)\n",
    "    to maintain document boundaries for YAKE and enable time-based filtering.\n",
    "\n",
    "    Parameters:\n",
    "    - specific_years: list of year strings, e.g., ['1910', '1911']\n",
    "    - specific_months: list of (year, month) tuples, e.g., [('1910', '05'), ('1910', '06')]\n",
    "    - ngram_size: int, controls n-gram filtering (1=unigrams, 2=bigrams, etc.)\n",
    "                  Change this parameter to easily switch between unigrams/bigrams/trigrams\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary: {filename: [(keyword, score), ...]}\n",
    "    \"\"\"\n",
    "    keywords_by_file = {}\n",
    "\n",
    "    # Iterate through all text files in the Halley directory\n",
    "    for f in sorted(files):\n",
    "        if \"_\" in f and f.endswith(\"txt\"):\n",
    "            # Parse filename to extract date metadata (same logic as preparer_halley.ipynb)\n",
    "            elems = f.split(\"_\")\n",
    "            year = elems[2].split(\"-\")[0]\n",
    "            month = elems[2].split(\"-\")[1]\n",
    "\n",
    "            # Apply time filters if specified\n",
    "            if specific_years and year not in specific_years:\n",
    "                continue\n",
    "            if specific_months and (year, month) not in specific_months:\n",
    "                continue\n",
    "\n",
    "            # Read the text file\n",
    "            text = open(os.path.join(data_path, f), 'r', encoding='utf-8').read()\n",
    "\n",
    "            # Extract keywords using YAKE\n",
    "            keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "            # Filter keywords by n-gram size (number of words in the keyword)\n",
    "            # To change from unigrams to bigrams/trigrams, just modify ngram_size parameter\n",
    "            kept = [(kw, score) for kw, score in keywords if len(kw.split()) == ngram_size]\n",
    "\n",
    "            # Store in dictionary organized by filename\n",
    "            keywords_by_file[f] = kept\n",
    "\n",
    "    return keywords_by_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "mog3i0hdll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords from 551 files\n"
     ]
    }
   ],
   "source": [
    "# Extract unigram keywords from all Halley texts, organized by file\n",
    "# To extract bigrams instead, change ngram_size=2\n",
    "keywords_by_file = extract_keywords_from_halley_texts(ngram_size=1)\n",
    "print(f\"Extracted keywords from {len(keywords_by_file)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "husjk9k2cwj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total keyword occurrences: 19655\n",
      "Unique keywords: 4842\n",
      "\n",
      "Top 20 most frequent keywords:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"qu'il\", 274),\n",
       " (\"d'une\", 265),\n",
       " (\"d'un\", 261),\n",
       " ('heures', 242),\n",
       " ('Bruxelles', 228),\n",
       " ('faire', 225),\n",
       " ('rue', 185),\n",
       " ('ans', 185),\n",
       " ('fut', 184),\n",
       " ('jour', 170),\n",
       " ('grand', 156),\n",
       " ('Paris', 153),\n",
       " (\"C'est\", 147),\n",
       " (\"c'est\", 145),\n",
       " ('Belgique', 138),\n",
       " ('francs', 138),\n",
       " ('temps', 132),\n",
       " ('jours', 132),\n",
       " ('lieu', 125),\n",
       " ('dit', 118)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten keywords from all files (keeping duplicates for frequency counting)\n",
    "all_keywords_flat = [kw for kws in keywords_by_file.values() for kw, score in kws]\n",
    "\n",
    "# Calculate frequency distribution\n",
    "keyword_freq = Counter(all_keywords_flat)\n",
    "\n",
    "# Display results\n",
    "print(f\"Total keyword occurrences: {len(all_keywords_flat)}\")\n",
    "print(f\"Unique keywords: {len(keyword_freq)}\")\n",
    "print(f\"\\nTop 20 most frequent keywords:\")\n",
    "keyword_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cmbmqdq8xxk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files from 1910: 242\n",
      "Files from May 1910: 98\n",
      "\n",
      "May 1910 top keywords: [('heures', 61), (\"d'un\", 60), ('Bruxelles', 58), (\"d'une\", 58), (\"qu'il\", 55), ('jour', 43), ('faire', 41), (\"C'est\", 36), ('rue', 35), ('ans', 34)]\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Keywords from 1910 only\n",
    "keywords_1910 = extract_keywords_from_halley_texts(specific_years=['1910'], ngram_size=1)\n",
    "print(f\"Files from 1910: {len(keywords_1910)}\")\n",
    "\n",
    "# Example 2: Keywords from May 1910 (peak Halley's passage)\n",
    "may_1910_keywords = extract_keywords_from_halley_texts(specific_months=[('1910', '05')], ngram_size=1)\n",
    "print(f\"Files from May 1910: {len(may_1910_keywords)}\")\n",
    "\n",
    "# Example 3: Compare May vs. all of 1910\n",
    "freq_may = Counter([kw for kws in may_1910_keywords.values() for kw, score in kws])\n",
    "print(f\"\\nMay 1910 top keywords: {freq_may.most_common(10)}\")\n",
    "\n",
    "#ok need to figure out why the stopwords lists aren't working here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gwinuarb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 3 files and their keywords to verify results\n",
    "for i, (filename, keywords) in enumerate(list(keywords_by_file.items())[:3]):\n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(f\"  Top 10 keywords: {keywords[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
