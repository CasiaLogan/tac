{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229a73ca",
   "metadata": {},
   "source": [
    "# Analyse de la dist. du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c004a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9734c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\casia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') #liste pré-définie de stopwords\n",
    "from nltk.corpus import stopwords #attention de bien nettoyer les stopwords !! sinon résultats bizarres\n",
    "\n",
    "import os\n",
    "import yake #(Yet Another Keyword Extractor) \"that uses text statistical features to select the most important keywords from a document\"\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud #besoin de compilateur c++ pour l'installer\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import re #pour les expressions régulières afin de nettoyer l OCRisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7441efd",
   "metadata": {},
   "source": [
    "### Gerer les stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6e5c",
   "metadata": {},
   "source": [
    "#### Créer une liste des stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "284e0c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 stopwords:\n",
      " ['-', ':', 'DRAPEAU', 'LE', 'ROUGE', 'a', 'abord', 'absolument', 'afin', 'ah', 'ai', 'aie', 'aient', 'aies', 'ailleurs', 'ainsi', 'ait', 'allaient', 'allo', 'allons', 'allô', 'alors', 'année', 'ans', 'anterieur', 'anterieure', 'anterieures', 'apres', 'après', 'as', 'assez', 'attendu', 'au', 'aucun', 'aucune', 'aucuns', 'aujourd', \"aujourd'hui\", 'aupres', 'auquel', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autant', 'autre', 'autrefois', 'autrement', 'autres', 'autrui', 'aux', 'auxquelles', 'auxquels', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayante', 'ayantes', 'ayants', 'ayez', 'ayons', 'bah', 'bas', 'basee', 'bat', 'beau', 'beaucoup', 'bien', 'bigre', 'bon', 'boum', 'bravo', 'c', 'car', 'ce', 'ceci', 'cela', 'celle', 'celle-ci', 'celle-là', 'celles', 'celles-ci', 'celles-là', 'celui', 'celui-ci', 'celui-là', 'celà', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'ces', 'cet', 'cette', 'ceux', 'ceux-ci', 'ceux-là', 'chacun', 'chacune', 'chaque', 'cher', 'chers', 'chez', 'chiche', 'chut', 'chère', 'chères', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantième', 'cinquième', 'clac', 'clic', 'com', 'combien', 'comme', 'comment', 'comparable', 'comparables', 'compris', 'concernant', 'contre', 'couic', 'crac', 'd', \"d'un\", \"d'une\", 'dans', 'de', 'debout', 'dedans', 'dehors', 'deja', 'delà', 'depuis', 'dernier', 'derniere', 'derriere', 'derrière', 'des', 'desormais', 'desquelles', 'desquels', 'dessous', 'dessus', 'deux', 'deuxième', 'deuxièmement', 'devant', 'devers', 'devra', 'devrait', 'different', 'differentes', 'differents', 'différent', 'différente', 'différentes', 'différents', 'dire', 'directe', 'directement', 'dit', 'dite', 'dits', 'divers', 'diverse', 'diverses', 'dix', 'dix-huit', 'dix-neuf', 'dix-sept', 'dixième', 'doit', 'doivent', 'donc', 'dont', 'dos', 'douze', 'douzième', 'drapeau', 'dring', 'droite', 'du', 'duquel', 'durant', 'dès', 'début', 'désormais', 'effet', 'egale', 'egalement', 'egales', 'eh', 'elle', 'elle-même', 'elles', 'elles-mêmes', 'en', 'encore', 'enfin', 'entre', 'envers', 'environ', 'es', 'essai', 'est', 'et', 'etant', 'etc', 'etre', 'eu', 'eue', 'eues', 'euh', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eux-mêmes', 'exactement', 'excepté', 'extenso', 'exterieur', 'eûmes', 'eût', 'eûtes', 'f', 'faire', 'fais', 'faisaient', 'faisant', 'fait', 'faites', 'faut', 'façon', 'feront', 'fi', 'flac', 'floc', 'fois', 'font', 'force', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'gens', 'haut', 'hein', 'hem', 'hep', 'het', 'holà', 'hop', 'hormis', 'hors', 'hou', 'houp', 'hue', 'hui', 'huit', 'huitième', 'hum', 'hurrah', 'hé', 'hélas', 'i', 'ici', 'il', 'ils', 'importe', 'j', 'je', 'jour', 'jusqu', 'jusque', 'juste', 'l', \"l'on\", 'la', 'laisser', 'laquelle', 'las', 'le', 'lequel', 'les', 'lesquelles', 'lesquels', 'leur', 'leurs', 'longtemps', 'lors', 'lorsque', 'lui', 'lui-meme', 'lui-même', 'là', 'lès', 'm', 'ma', 'maint', 'maintenant', 'mais', 'malgre', 'malgré', 'maximale', 'me', 'meme', 'memes', 'merci', 'mes', 'mien', 'mienne', 'miennes', 'miens', 'mille', 'mince', 'mine', 'minimale', 'moi', 'moi-meme', 'moi-même', 'moindres', 'moins', 'mon', 'mot', 'moyennant', 'multiple', 'multiples', 'même', 'mêmes', 'n', \"n'anaturel\", 'na', 'naturelle', 'naturelles', 'ne', 'neanmoins', 'necessaire', 'necessairement', 'neuf', 'neuvième', 'ni', 'nombreuses', 'nombreux', 'nommés', 'non', 'nos', 'notamment', 'notre', 'nous', 'nous-mêmes', 'nouveau', 'nouveaux', 'nul', 'néanmoins', 'nôtre', 'nôtres', 'oh', 'ohé', 'ollé', 'olé', 'on', 'ont', 'onze', 'onzième', 'ore', 'ou', 'ouf', 'ouias', 'oust', 'ouste', 'outre', 'ouvert', 'ouverte', 'ouverts', 'où', 'paf', 'par', 'parce', 'parfois', 'parle', 'parlent', 'parler', 'parmi', 'parole', 'parseme', 'partant', 'particulier', 'particulière', 'particulièrement', 'pas', 'passé', 'pendant', 'pense', 'permet', 'personne', 'personnes', 'peu', 'peut', 'peuvent', 'peux', 'pff', 'pfft', 'pfut', 'pif', 'pire', 'pièce', 'plein', 'plouf', 'plupart', 'plus', 'plusieurs', 'plutôt', 'possessif', 'possessifs', 'possible', 'possibles', 'pouah', 'pour', 'pourquoi', 'pourrais', 'pourrait', 'pouvait', 'prealable', 'precisement', 'premier', 'première', 'premièrement', 'pres', 'probable', 'probante', 'procedant', 'proche', 'près', 'psitt', 'pu', 'puis', 'puisque', 'pur', 'pure', 'qu', \"qu'elle\", \"qu'elles\", \"qu'il\", \"qu'ils\", 'quand', 'quant', 'quant-à-soi', 'quanta', 'quarante', 'quatorze', 'quatre', 'quatre-vingt', 'quatrième', 'quatrièmement', 'que', 'quel', 'quelconque', 'quelle', 'quelles', \"quelqu'un\", 'quelque', 'quelques', 'quels', 'qui', 'quiconque', 'quinze', 'quoi', 'quoique', 'r', 'rare', 'rarement', 'rares', 'relative', 'relativement', 'remarquable', 'rend', 'rendre', 'restant', 'reste', 'restent', 'restrictif', 'retour', 'revoici', 'revoilà', 'rien', 'rue', 's', 'sa', 'sacrebleu', 'sait', 'sans', 'sapristi', 'sauf', 'se', 'sein', 'seize', 'selon', 'semblable', 'semblaient', 'semble', 'semblent', 'sent', 'sept', 'septième', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seul', 'seule', 'seulement', 'si', 'sien', 'sienne', 'siennes', 'siens', 'sinon', 'six', 'sixième', 'soi', 'soi-même', 'soient', 'sois', 'soit', 'soixante', 'sommes', 'son', 'sont', 'sous', 'souvent', 'soyez', 'soyons', 'specifique', 'specifiques', 'speculatif', 'stop', 'strictement', 'subtiles', 'suffisant', 'suffisante', 'suffit', 'suis', 'suit', 'suivant', 'suivante', 'suivantes', 'suivants', 'suivre', 'sujet', 'superpose', 'sur', 'surtout', 't', 'ta', 'tac', 'tandis', 'tant', 'tardive', 'te', 'tel', 'telle', 'tellement', 'telles', 'tels', 'tenant', 'tend', 'tenir', 'tente', 'tes', 'tic', 'tien', 'tienne', 'tiennes', 'tiens', 'toc', 'toi', 'toi-même', 'ton', 'touchant', 'toujours', 'tous', 'tout', 'toute', 'toutefois', 'toutes', 'treize', 'trente', 'tres', 'trois', 'troisième', 'troisièmement', 'trop', 'très', 'tsoin', 'tsouin', 'tu', 'té', 'un', 'une', 'unes', 'uniformement', 'unique', 'uniques', 'uns', 'va', 'vais', 'valeur', 'van', 'vas', 'vers', 'via', 'vif', 'vifs', 'vingt', 'vivat', 'vive', 'vives', 'vlan', 'voici', 'voie', 'voient', 'voilà', 'voir', 'voire', 'vont', 'vos', 'votre', 'vous', 'vous-mêmes', 'vu', 'vé', 'vôtre', 'vôtres', 'y', 'zut', 'à', 'â', 'ça', 'ès', 'étaient', 'étais', 'était', 'étant', 'étante', 'étantes', 'étants', 'état', 'étiez', 'étions', 'été', 'étée', 'étées', 'étés', 'êtes', 'être', 'ô']\n"
     ]
    }
   ],
   "source": [
    "#jouer encore avec cela pour en ajouter des nouveaux en fonction du contexte, modifié de la liste de https://github.com/stopwords-iso/stopwords-fr?tab=readme-ov-file, à verifer sa fidelité\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \"elles\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \":\", \"même\", \"voir\", \"fois\", \"jour\", \"année\", \"ans\", \"faites\", \"le\", \"la\", \"de\"]\n",
    "\n",
    "# New stopwords to add\n",
    "git_stopwords = [\n",
    "    \"a\", \"abord\", \"absolument\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\", \"aies\", \"ailleurs\", \"ainsi\", \"ait\", \"allaient\", \"allo\", \"allons\", \"allô\", \"alors\", \"anterieur\", \"anterieure\", \"anterieures\", \"apres\", \"après\", \"as\", \"assez\", \"attendu\", \"au\", \"aucun\", \"aucune\", \"aucuns\", \"aujourd\", \"aujourd'hui\", \"aupres\", \"auquel\", \"aura\", \"aurai\", \"auraient\", \"aurais\", \"aurait\", \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autant\", \"autre\", \"autrefois\", \"autrement\", \"autres\", \"autrui\", \"aux\", \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\", \"avez\", \"aviez\", \"avions\", \"avoir\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"bah\", \"bas\", \"basee\", \"bat\", \"beau\", \"beaucoup\", \"bien\", \"bigre\", \"bon\", \"boum\", \"bravo\", \"car\", \"ce\", \"ceci\", \"cela\", \"celle\", \"celle-ci\", \"celle-là\", \"celles\", \"celles-ci\", \"celles-là\", \"celui\", \"celui-ci\", \"celui-là\", \"celà\", \"cent\", \"cependant\", \"certain\", \"certaine\", \"certaines\", \"certains\", \"certes\", \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\", \"chacun\", \"chacune\", \"chaque\", \"cher\", \"chers\", \"chez\", \"chiche\", \"chut\", \"chère\", \"chères\", \"ci\", \"cinq\", \"cinquantaine\", \"cinquante\", \"cinquantième\", \"cinquième\", \"clac\", \"clic\", \"combien\", \"comme\", \"comment\", \"comparable\", \"comparables\", \"compris\", \"concernant\", \"contre\", \"couic\", \"crac\", \"dans\", \"de\", \"debout\", \"dedans\", \"dehors\", \"deja\", \"delà\", \"depuis\", \"dernier\", \"derniere\", \"derriere\", \"derrière\", \"des\", \"desormais\", \"desquelles\", \"desquels\", \"dessous\", \"dessus\", \"deux\", \"deuxième\", \"deuxièmement\", \"devant\", \"devers\", \"devra\", \"devrait\", \"different\", \"differentes\", \"differents\", \"différent\", \"différente\", \"différentes\", \"différents\", \"dire\", \"directe\", \"directement\", \"dit\", \"dite\", \"dits\", \"divers\", \"diverse\", \"diverses\", \"dix\", \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dixième\", \"doit\", \"doivent\", \"donc\", \"dont\", \"dos\", \"douze\", \"douzième\", \"dring\", \"droite\", \"du\", \"duquel\", \"durant\", \"dès\", \"début\", \"désormais\", \"effet\", \"egale\", \"egalement\", \"egales\", \"eh\", \"elle\", \"elle-même\", \"elles\", \"elles-mêmes\", \"en\", \"encore\", \"enfin\", \"entre\", \"envers\", \"environ\", \"es\", \"essai\", \"est\", \"et\", \"etant\", \"etc\", \"etre\", \"eu\", \"eue\", \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\", \"eusses\", \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eux-mêmes\", \"exactement\", \"excepté\", \"extenso\", \"exterieur\", \"eûmes\", \"eût\", \"eûtes\", \"f\", \"fais\", \"faisaient\", \"faisant\", \"fait\", \"faites\", \"façon\", \"feront\", \"fi\", \"flac\", \"floc\", \"fois\", \"font\", \"force\", \"furent\", \"fus\", \"fusse\", \"fussent\", \"fusses\", \"fussiez\", \"fussions\", \"fut\", \"fûmes\", \"fût\", \"fûtes\", \"gens\", \"haut\", \"hein\", \"hem\", \"hep\", \"holà\", \"hop\", \"hormis\", \"hors\", \"hou\", \"houp\", \"hue\", \"hui\", \"huit\", \"huitième\", \"hum\", \"hurrah\", \"hé\", \"hélas\", \"i\", \"ici\", \"il\", \"ils\", \"importe\", \"j\", \"je\", \"jusqu\", \"jusque\", \"juste\", \"la\", \"laisser\", \"laquelle\", \"las\", \"le\", \"lequel\", \"les\", \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"longtemps\", \"lors\", \"lorsque\", \"lui\", \"lui-meme\", \"lui-même\", \"là\", \"lès\", \"ma\", \"maint\", \"maintenant\", \"mais\", \"malgre\", \"malgré\", \"maximale\", \"me\", \"meme\", \"memes\", \"merci\", \"mes\", \"mien\", \"mienne\", \"miennes\", \"miens\", \"mille\", \"mince\", \"mine\", \"minimale\", \"moi\", \"moi-meme\", \"moi-même\", \"moindres\", \"moins\", \"mon\", \"mot\", \"moyennant\", \"multiple\", \"multiples\", \"même\", \"mêmes\", \"na\", \"n'a\" \"naturel\", \"naturelle\", \"naturelles\", \"ne\", \"neanmoins\", \"necessaire\", \"necessairement\", \"neuf\", \"neuvième\", \"ni\", \"nombreuses\", \"nombreux\", \"nommés\", \"non\", \"nos\", \"notamment\", \"notre\", \"nous\", \"nous-mêmes\", \"nouveau\", \"nouveaux\", \"nul\", \"néanmoins\", \"nôtre\", \"nôtres\", \"oh\", \"ohé\", \"ollé\", \"olé\", \"on\", \"ont\", \"onze\", \"onzième\", \"ore\", \"ou\", \"ouf\", \"ouias\", \"oust\", \"ouste\", \"outre\", \"ouvert\", \"ouverte\", \"ouverts\", \"où\", \"paf\", \"par\", \"parce\", \"parfois\", \"parle\", \"parlent\", \"parler\", \"parmi\", \"parole\", \"parseme\", \"partant\", \"particulier\", \"particulière\", \"particulièrement\", \"pas\", \"passé\", \"pendant\", \"pense\", \"permet\", \"personne\", \"personnes\", \"peu\", \"peut\", \"peuvent\", \"peux\", \"pff\", \"pfft\", \"pfut\", \"pif\", \"pire\", \"pièce\", \"plein\", \"plouf\", \"plupart\", \"plus\", \"plusieurs\", \"plutôt\", \"possessif\", \"possessifs\", \"possible\", \"possibles\", \"pouah\", \"pour\", \"pourquoi\", \"pourrais\", \"pourrait\", \"pouvait\", \"prealable\", \"precisement\", \"premier\", \"première\", \"premièrement\", \"pres\", \"probable\", \"probante\", \"procedant\", \"proche\", \"près\", \"psitt\", \"pu\", \"puis\", \"puisque\", \"pur\", \"pure\", \"qu\", \"quand\", \"quant\", \"quant-à-soi\", \"quanta\", \"quarante\", \"quatorze\", \"quatre\", \"quatre-vingt\", \"quatrième\", \"quatrièmement\", \"que\", \"quel\", \"quelconque\", \"quelle\", \"quelles\", \"quelqu'un\", \"quelque\", \"quelques\", \"quels\", \"qui\", \"quiconque\", \"quinze\", \"quoi\", \"quoique\", \"r\", \"rare\", \"rarement\", \"rares\", \"relative\", \"relativement\", \"remarquable\", \"rend\", \"rendre\", \"restant\", \"reste\", \"restent\", \"restrictif\", \"retour\", \"revoici\", \"revoilà\", \"rien\", \"sa\", \"sacrebleu\", \"sait\", \"sans\", \"sapristi\", \"sauf\", \"se\", \"sein\", \"seize\", \"selon\", \"semblable\", \"semblaient\", \"semble\", \"semblent\", \"sent\", \"sept\", \"septième\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\", \"seriez\", \"serions\", \"serons\", \"seront\", \"ses\", \"seul\", \"seule\", \"seulement\", \"si\", \"sien\", \"sienne\", \"siennes\", \"siens\", \"sinon\", \"six\", \"sixième\", \"soi\", \"soi-même\", \"soient\", \"sois\", \"soit\", \"soixante\", \"sommes\", \"son\", \"sont\", \"sous\", \"souvent\", \"soyez\", \"soyons\", \"specifique\", \"specifiques\", \"speculatif\", \"stop\", \"strictement\", \"subtiles\", \"suffisant\", \"suffisante\", \"suffit\", \"suis\", \"suit\", \"suivant\", \"suivante\", \"suivantes\", \"suivants\", \"suivre\", \"sujet\", \"superpose\", \"sur\", \"surtout\", \"ta\", \"tac\", \"tandis\", \"tant\", \"tardive\", \"te\", \"tel\", \"telle\", \"tellement\", \"telles\", \"tels\", \"tenant\", \"tend\", \"tenir\", \"tente\", \"tes\", \"tic\", \"tien\", \"tienne\", \"tiennes\", \"tiens\", \"toc\", \"toi\", \"toi-même\", \"ton\", \"touchant\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\", \"treize\", \"trente\", \"tres\", \"trois\", \"troisième\", \"troisièmement\", \"trop\", \"très\", \"tsoin\", \"tsouin\", \"tu\", \"té\", \"un\", \"une\", \"unes\", \"uniformement\", \"unique\", \"uniques\", \"uns\", \"va\", \"vais\", \"valeur\", \"vas\", \"vers\", \"via\", \"vif\", \"vifs\", \"vingt\", \"vivat\", \"vive\", \"vives\", \"vlan\", \"voici\", \"voie\", \"voient\", \"voilà\", \"voire\", \"vont\", \"vos\", \"votre\", \"vous\", \"vous-mêmes\", \"vu\", \"vé\", \"vôtre\", \"vôtres\", \"zut\", \"à\", \"â\", \"ça\", \"ès\", \"étaient\", \"étais\", \"était\", \"étant\", \"état\", \"étiez\", \"étions\", \"été\", \"étée\", \"étées\", \"étés\", \"êtes\", \"être\", \"ô\"\n",
    "]\n",
    "\n",
    "#domaine specific \n",
    "specific_stopwords = [\n",
    "\"rue\", \"-\", \"drapeau\", \"DRAPEAU\", \"LE\", \"ROUGE\", \"com\", \"qu'il\", \"d'une\", \"d'un\", \"l'on\", \"qu'il\", \"qu'ils\", \"qu'elle\", \"qu'elles\", \"qu\", \"il\", \"elle\", \"c\", \"est\", \"elles\", \"ils\"\n",
    "#garder les noms des villes pour faire une analyse spatiale? \n",
    "#does it also include common first names?\n",
    "#does it also include uppercase versions?\n",
    "#à voir : soir, matin = seront pe liés au nom du journal et non du temps de la journée, define what a stopword means in this context\n",
    "]\n",
    "\n",
    "sw += git_stopwords\n",
    "sw += specific_stopwords\n",
    "sw = set(sw)\n",
    "\n",
    "\n",
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b9054",
   "metadata": {},
   "source": [
    "#### Tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbb7d6",
   "metadata": {},
   "source": [
    "Créer un fichier bash qui concatène tous les fichiers txt de halley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19aa408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4858920 words found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['yF',\n",
       " ':',\n",
       " 'LE',\n",
       " 'DRAPEAU',\n",
       " 'ROUGE',\n",
       " 'Edison',\n",
       " 'revendique',\n",
       " 'la',\n",
       " 'priorité',\n",
       " 'de',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'invention',\n",
       " 'du',\n",
       " 'phonographe',\n",
       " '.-',\n",
       " 'On',\n",
       " 'a',\n",
       " 'signalé',\n",
       " 'dernièrement',\n",
       " 'la',\n",
       " 'mort',\n",
       " 'du',\n",
       " 'poète',\n",
       " 'ot',\n",
       " \"'\",\n",
       " 'savant',\n",
       " 'français',\n",
       " 'Charles',\n",
       " 'Croa',\n",
       " 'considéré',\n",
       " 'com',\n",
       " '-',\n",
       " ',',\n",
       " 'mo',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'inventeur',\n",
       " 'du',\n",
       " 'phonographe',\n",
       " ',',\n",
       " 'dont',\n",
       " 'il',\n",
       " 'aurait',\n",
       " 'décrit',\n",
       " ',',\n",
       " 'le',\n",
       " 'premier',\n",
       " ',',\n",
       " 'le']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merci les profs\n",
    "path = \"../../data/halley/halley_txt/\" # Path to the directory containing text files\n",
    "\n",
    "with open(\"../../data/halley/halley_all.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                output_file.write(f.read())\n",
    "\n",
    "# Récupération du contenu du fichier bash\n",
    "path = \"../../data/halley/halley_all.txt\"\n",
    "limit = 10**8\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()[:limit]\n",
    "\n",
    "# Tokenization, sw pas encore appliqués\n",
    "words = nltk.wordpunct_tokenize(text) #tokenized words from the concatenated Halley corpus (halley_all.txt)\n",
    "print(f\"{len(words)} words found\")\n",
    "words[:50] \n",
    "\n",
    "# a big list of all of the keywords from all of the files, and then do a frequency analysis on that list to see what are the most common keywords across all of the texts = kinda loses all temporal analysis but ok for the overarching ALL keywords, \n",
    "# we should expect only the most common elements to come out? but key words isn't most common... careful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f336a",
   "metadata": {},
   "source": [
    "#### Eliminer les stopwords et les termes non alphabétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a87720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584198 words kept (217992 different word forms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rouge',\n",
       " 'edison',\n",
       " 'revendique',\n",
       " 'priorité',\n",
       " 'invention',\n",
       " 'phonographe',\n",
       " 'signalé',\n",
       " 'dernièrement',\n",
       " 'mort',\n",
       " 'poète',\n",
       " 'savant',\n",
       " 'français',\n",
       " 'charles',\n",
       " 'croa',\n",
       " 'considéré',\n",
       " 'inventeur',\n",
       " 'phonographe',\n",
       " 'décrit',\n",
       " 'mécanisme',\n",
       " 'edison']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows words after filtering stop words, much better\n",
    "kept = [w.lower() for w in words \n",
    "        if len(w) > 2 \n",
    "        and w.isalpha() \n",
    "        and w.lower() not in sw\n",
    "        and not re.search(r'(.)\\1{2,}', w.lower())]\n",
    "\n",
    "voc = set(kept) #creates a unique vocabulary (no duplicates)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")\n",
    "kept[:20] \n",
    "\n",
    "#filtering iterations (notes)\n",
    "#1587064 words kept (220119 different word forms) \n",
    "#seems like a small difference but it's actually around 1500 words removed by the new stopword list\n",
    "\n",
    "#avec 5 caractères le même, 1586939 words kept (220004 different word forms) quand même une différence mais elle est assez minimale, dc environs 100 mots en moins \n",
    "#avec 3 carartères, 1586631 words kept (219739 different word forms)\n",
    "#avec 2, 1584198 words kept (217992 different word forms)\n",
    "\n",
    "\n",
    "#so these words are still the WHOLE page of ALL the pages isn't about halleys comet \n",
    "# do we go for an analysis of co-text as well then? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbce334",
   "metadata": {},
   "source": [
    "#### Calculer la taille du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfac870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heures', 4175),\n",
       " ('bruxelles', 3613),\n",
       " ('mai', 3111),\n",
       " ('grand', 3095),\n",
       " ('prix', 2688),\n",
       " ('temps', 2602),\n",
       " ('lieu', 2568),\n",
       " ('comète', 2556),\n",
       " ('francs', 2549),\n",
       " ('soir', 2445)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#récupérer la fréquence des mots sur toute le dataset \n",
    "fdist = nltk.FreqDist(kept) #yes, kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "fdist.most_common(10)\n",
    "\n",
    "#heures might be on the chopping block for sw \n",
    "\n",
    "#voisins plus proches to halley would be useful\n",
    "#or maybe we do a co-text analysis of words around halley?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f98710",
   "metadata": {},
   "source": [
    "#### Variation from structure of modules \n",
    "I don't see what the plot adds, so I'm leaving it out for now\n",
    "I'm also leaving out les mots qui n'apparaissent qu'une fois dans le corpus et les plus longs pour l'instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36610332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les mots les plus longs\n",
    "n = 150\n",
    "sorted(voc, key=len, reverse=True)[:n]\n",
    "\n",
    "#oh TERRIBLE OCR without the regex, not a single word, but a couple of conjuctions at the end \n",
    "\n",
    "#quand meme une grosse différence avec le regex pour enlever les mots avec des lettres répétées\n",
    "#peut être qu'on peut faire un truc plus fin pour garder les mots légitimes, c'est un peu dommage que c'est mots ne seront pas analysés dans l'état quelles sont mtn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e8806",
   "metadata": {},
   "source": [
    "### Keywords Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66651306",
   "metadata": {},
   "source": [
    "#### Extraire les keywords du texte sans stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a401d0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yake.yake.KeywordExtractor at 0x1425565eed0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50) #keeping 50 for now, but it could be more concise to do less per document \n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2ad7629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lister les fichiers, advantage is that we can see which files mention which keywords but we can also do the same with a bash eventually\n",
    "data_path = \"../../data/halley/halley_txt/\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80f03b",
   "metadata": {},
   "source": [
    "## Combiner les étapes d'avant afin d'analyser tout les fichiers à la fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15d606f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 551 files\n",
      "Total tokens across all files: 1584196\n",
      "551\n",
      "\n",
      "KB_JB1051_1927-08-25_01-00004.txt: 1677 tokens\n",
      "First 20 tokens: ['rouge', 'edison', 'revendique', 'priorité', 'invention', 'phonographe', 'signalé', 'dernièrement', 'mort', 'poète', 'savant', 'français', 'charles', 'croa', 'considéré', 'inventeur', 'phonographe', 'décrit', 'mécanisme', 'edison']\n",
      "\n",
      "KB_JB1051_1937-11-14_01-00006.txt: 1305 tokens\n",
      "First 20 tokens: ['péchons', 'baleine', 'mini', 'îuii', 'iin', 'iriririnur', 'rateiinier', 'soviétiques', 'baleine', 'pèse', 'tonnes', 'bord', 'bateau', 'immédiatement', 'dépecée', 'préparée', 'etoiles', 'filantes', 'novembre', 'temps']\n",
      "\n",
      "KB_JB1051_1939-08-06_01-00005.txt: 2164 tokens\n",
      "First 20 tokens: ['page', 'famille', 'snfants', 'chronique', 'medicale', 'enfants', 'lacher', 'monstre', 'ballonnets', 'naangerca', 'excel', 'lento', 'crème', 'glace', 'vou', 'coin', 'amuser', 'mickcy', 'ravi', 'revoir']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize each file individually\n",
    "#I think it's best to first see all of they keywords per file before aggregating and then doing one big word cloud for the whole set, or just a certain period. I would've wanted to compare the wordcloud of the different periods ngl. I mean, at least I could do that for la libre belgique or whatever it was that was also reporting in 1835\n",
    "\n",
    "data_path = \"../../data/halley/halley_txt/\"\n",
    "tokenised_files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "\n",
    "tokens_by_file = {}\n",
    "\n",
    "for filename in tokenised_files:\n",
    "    filepath = os.path.join(data_path, filename)\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read() # Récupérer le texte du fichier individualement \n",
    "\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.wordpunct_tokenize(text) # Extraire les tokens du texte en cours\n",
    "    \n",
    "    # Filtrer\n",
    "    filtered_tokens = [w.lower() for w in tokens \n",
    "                      if len(w) > 2 # Ne garder que les bigrammes\n",
    "                      and w.isalpha() \n",
    "                      and w.lower() not in sw #il est censé enlevé les stopwords, mais \"c'est\" et \"qu'il\" apparait tjrs?? \n",
    "                      and not re.search(r'(.)\\1{2,}', w.lower())]\n",
    "    \n",
    "    tokens_by_file[filename] = filtered_tokens\n",
    "\n",
    "print(f\"Tokenized {len(tokens_by_file)} files\")\n",
    "print(f\"Total tokens across all files: {sum(len(tokens) for tokens in tokens_by_file.values())}\")\n",
    "print(len(tokenised_files)) #just for me to be sure that indeed they were all read\n",
    "\n",
    "# Show example for first files\n",
    "first_file = tokenised_files[0] # Limiter à 1 fichier pour l'exemple, need to do a bash of all? after checking stop words?\n",
    "second_file = tokenised_files[1]\n",
    "third_file = tokenised_files[2]\n",
    "print(f\"\\n{first_file}: {len(tokens_by_file[first_file])} tokens\")\n",
    "print(f\"First 20 tokens: {tokens_by_file[first_file][:20]}\") \n",
    "\n",
    "print(f\"\\n{second_file}: {len(tokens_by_file[second_file])} tokens\")\n",
    "print(f\"First 20 tokens: {tokens_by_file[second_file][:20]}\") \n",
    "\n",
    "print(f\"\\n{third_file}: {len(tokens_by_file[third_file])} tokens\")\n",
    "print(f\"First 20 tokens: {tokens_by_file[third_file][:20]}\") #ok I had to check that I was actually getting tokens by file, and so far it looks good even if snfants is in there lol "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4282080",
   "metadata": {},
   "source": [
    "#### Keyword extractor\n",
    "Statistical approach: the lower the score, the more relevant the keyword is.\n",
    "import yake\n",
    "\n",
    "##### Custom parameters\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"en\",              # language\n",
    "    n=3,                   # ngram size\n",
    "    dedupLim=0.9,          # deduplication threshold\n",
    "    dedupFunc='seqm',      # deduplication function\n",
    "    windowsSize=1,         # context window\n",
    "    top=10,                # number of keywords to extract\n",
    "    features=None          # custom features\n",
    ")\n",
    "\n",
    "keywords = custom_kw_extractor.extract_keywords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa664052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_JB1051_1927-08-25_01-00004.txt mentions these bigrams: ...\n",
      "KB_JB1051_1937-11-14_01-00006.txt mentions these bigrams: ...\n",
      "KB_JB1051_1939-08-06_01-00005.txt mentions these bigrams: ...\n",
      "KB_JB1051_1957-04-18_01-00004.txt mentions these bigrams: ...\n",
      "KB_JB1051_1957-10-14_01-00001.txt mentions these bigrams: ...\n",
      "KB_JB421_1902-10-19_01-00001.txt mentions these bigrams: ...\n",
      "KB_JB421_1907-10-26_01-00002.txt mentions these bigrams: ...\n",
      "KB_JB421_1909-05-07_01-00003.txt mentions these bigrams: ...\n",
      "KB_JB421_1909-08-06_01-00001.txt mentions these bigrams: ...\n",
      "KB_JB421_1909-10-23_01-00001.txt mentions these bigrams: ...\n"
     ]
    }
   ],
   "source": [
    "# Recreate keyword extractor with stopwords\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"fr\", \n",
    "    top=50,\n",
    "    stopwords=sw  # Still helps YAKE's internal scoring\n",
    ")\n",
    "\n",
    "all_token_keywords = {}\n",
    "\n",
    "for filename in sorted(tokens_by_file.keys())[:10]:\n",
    "    filtered_tokens = tokens_by_file[filename] # Get pre-filtered tokens (already has stopwords, length, OCR filters)\n",
    "    \n",
    "    \n",
    "    text_from_tokens = ' '.join(filtered_tokens) # Convert token list back to text for YAKE\n",
    "    \n",
    "    \n",
    "    token_keywords = kw_extractor.extract_keywords(text_from_tokens) # Extract keywords from filtered text\n",
    "    all_token_keywords[filename] = token_keywords\n",
    "    \n",
    "    \n",
    "    kept_tokens = [] # Filter by n-gram size (change the number to get different sizes)\n",
    "    for kw, score in token_keywords:\n",
    "        token_words = kw.split()\n",
    "        if len(token_words) == 2:  # Change to 1 for unigrams, 3 for trigrams\n",
    "            kept_tokens.append(kw)\n",
    "    \n",
    "    print(f\"{filename} mentions these bigrams: {', '.join(kept_tokens)}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c89cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_JB1051_1927-08-25_01-00004.txt mentions these keywords: Charles Croa, Croa considéré, étoiles filantes, Edison revendique, local habituel, signalé dernièrement, poète ot'savant, Thomas EDISON, ROUGE Edison, Edison vient, Edison n'aurait...\n",
      "KB_JB1051_1937-11-14_01-00006.txt mentions these keywords: record mondial, étoiles filantes, l'Union Soviétique, iiiiii ggggggg, d'étoiles filantes, record soviétique, nageur soviétique, meilleurs nageurs, capitaine Sarva, Jean Blume, mètres brasse...\n",
      "KB_JB1051_1939-08-06_01-00005.txt mentions these keywords: CHRONIQUE MEDICALE, Snfants CHRONIQUE, LACHER MONSTRE, prix moyen, bonne qualité, qualité commerciale, répondit Charles, père Grandet...\n",
      "KB_JB1051_1957-04-18_01-00004.txt mentions these keywords: duc Henri, congrès fédéral, Chine populaire, Maison Blanche, foi catholique...\n",
      "KB_JB1051_1957-10-14_01-00001.txt mentions these keywords: Presse Communiste, PARTI COMMUNISTE, Joseph Jacquemotte, REVENDICATIONS OUVRIERES, l'Harmonie ouvrière, Isa Ufania, Gaston Moulin...\n",
      "KB_JB421_1902-10-19_01-00001.txt mentions these keywords: grande ligne, petite ligne, généraux boers, qu'on annonce, qu'on vient, Quint C'est, Zola qu'on, nouvelle Eglise, c'est décidément, oen Exploit, Semplairea parviendront, Exploit d'huissier...\n",
      "KB_JB421_1907-10-26_01-00002.txt mentions these keywords: grand peintre, l'art chrétien, inoins sombre, vitrail vit, grand artiste, grands artistes, quo c'est, peintre religieux, qu'une teinto, enfants Rubens, Fra Angelico, sombre d'après...\n",
      "KB_JB421_1909-05-07_01-00003.txt mentions these keywords: Chronique Régionale, Régionale Mémorandum, Mémorandum VENDREDI, jeune Vaucoret, lundi matin, jeudi matin, parti catholique...\n",
      "KB_JB421_1909-08-06_01-00001.txt mentions these keywords: Dom Bosco, prince Albert, affaires étrangères, cardinal Fischer, Moyen Age, congrès eucharistique, Caaille JOSET...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(tokens_by_file.keys())[:\u001b[32m10\u001b[39m]:\n\u001b[32m     12\u001b[39m     tokenised_text = \u001b[38;5;28mopen\u001b[39m(os.path.join(data_path, f), \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m).read()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     token_keywords = \u001b[43mkw_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenised_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     all_token_keywords[f] = token_keywords  \u001b[38;5;66;03m# Save to dictionary\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m#print(token_keywords) #this seems to have the scores associated with it, and seems to consider trigrams as well, idk what to do with that yet but I'm leaving it here for now \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\yake\\yake.py:76\u001b[39m, in \u001b[36mKeywordExtractor.extract_keywords\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     74\u001b[39m toadd = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (h, candResult) \u001b[38;5;129;01min\u001b[39;00m resultSet:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     dist = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdedu_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique_kw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandResult\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dist > \u001b[38;5;28mself\u001b[39m.dedupLim:\n\u001b[32m     78\u001b[39m         toadd = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\yake\\yake.py:56\u001b[39m, in \u001b[36mKeywordExtractor.seqm\u001b[39m\u001b[34m(self, cand1, cand2)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mseqm\u001b[39m(\u001b[38;5;28mself\u001b[39m, cand1, cand2):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLevenshtein\u001b[49m\u001b[43m.\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcand2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\yake\\Levenshtein.py:11\u001b[39m, in \u001b[36mLevenshtein.ratio\u001b[39m\u001b[34m(seq1, seq2)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mratio\u001b[39m(seq1, seq2):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     str_distance = \u001b[43mLevenshtein\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseq2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     str_length = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq1),\u001b[38;5;28mlen\u001b[39m(seq2))\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Levenshtein.__ratio(str_distance,str_length)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\yake\\Levenshtein.py:19\u001b[39m, in \u001b[36mLevenshtein.distance\u001b[39m\u001b[34m(seq1, seq2)\u001b[39m\n\u001b[32m     17\u001b[39m size_x = \u001b[38;5;28mlen\u001b[39m(seq1) + \u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m size_y = \u001b[38;5;28mlen\u001b[39m(seq2) + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m matrix = np.zeros ((size_x, size_y))\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(size_x):\n\u001b[32m     21\u001b[39m     matrix [x, \u001b[32m0\u001b[39m] = x\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#start here\n",
    "# Recreate keyword extractor with stopwords\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"fr\", \n",
    "    top=50,\n",
    "    stopwords=sw  # Still helps YAKE's internal scoring\n",
    ")\n",
    "\n",
    "all_token_keywords = {}  # Store keywords for each file in a dictionary \n",
    "\n",
    "for f in sorted(tokens_by_file.keys())[:10]:\n",
    "    tokenised_text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    token_keywords = kw_extractor.extract_keywords(tokenised_text)\n",
    "    all_token_keywords[f] = token_keywords  # Save to dictionary\n",
    "    #print(token_keywords) #this seems to have the scores associated with it, and seems to consider trigrams as well, idk what to do with that yet but I'm leaving it here for now \n",
    "    \n",
    "    kept_tokens = []\n",
    "    for kw, score in token_keywords:\n",
    "        token_words = kw.split()\n",
    "        if len(token_words) == 2: # when i tried all_token_keywords here AND when i use token words again it gives it to me in a different order, wtf\n",
    "            kept_tokens.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join((kept_tokens))}...\")\n",
    "    \n",
    "\n",
    "#fixed! I used kept and not kept_tokens lol \n",
    "\n",
    "\n",
    "#see the kw and score of a \n",
    "all_token_keywords['KB_JB1051_1927-08-25_01-00004.txt']\n",
    "\n",
    "\n",
    "#oh this is SO weird, KB_JB1051_1937-11-14_01-00006.txt is showing kw (first ones Baleine, Soviétique, record mondial, comète,) \n",
    "#BUT none of the others are, and that's not even one that I've ran individually before as far as I'm aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_JB1051_1927-08-25_01-00004.txt mentions these keywords: Charles Croa, Croa considéré, étoiles filantes, Edison revendique, local habituel, signalé dernièrement, poète ot'savant, Thomas EDISON, ROUGE Edison, Edison vient, Edison n'aurait...\n",
      "KB_JB1051_1937-11-14_01-00006.txt mentions these keywords: record mondial, étoiles filantes, l'Union Soviétique, iiiiii ggggggg, d'étoiles filantes, record soviétique, nageur soviétique, meilleurs nageurs, capitaine Sarva, Jean Blume, mètres brasse...\n",
      "KB_JB1051_1939-08-06_01-00005.txt mentions these keywords: CHRONIQUE MEDICALE, Snfants CHRONIQUE, LACHER MONSTRE, prix moyen, bonne qualité, qualité commerciale, répondit Charles, père Grandet...\n",
      "KB_JB1051_1957-04-18_01-00004.txt mentions these keywords: duc Henri, congrès fédéral, Chine populaire, Maison Blanche, foi catholique...\n",
      "KB_JB1051_1957-10-14_01-00001.txt mentions these keywords: Presse Communiste, PARTI COMMUNISTE, Joseph Jacquemotte, REVENDICATIONS OUVRIERES, l'Harmonie ouvrière, Isa Ufania, Gaston Moulin...\n",
      "KB_JB421_1902-10-19_01-00001.txt mentions these keywords: grande ligne, petite ligne, généraux boers, qu'on annonce, qu'on vient, Quint C'est, Zola qu'on, nouvelle Eglise, c'est décidément, oen Exploit, Semplairea parviendront, Exploit d'huissier...\n",
      "KB_JB421_1907-10-26_01-00002.txt mentions these keywords: grand peintre, l'art chrétien, inoins sombre, vitrail vit, grand artiste, grands artistes, quo c'est, peintre religieux, qu'une teinto, enfants Rubens, Fra Angelico, sombre d'après...\n",
      "KB_JB421_1909-05-07_01-00003.txt mentions these keywords: Chronique Régionale, Régionale Mémorandum, Mémorandum VENDREDI, jeune Vaucoret, lundi matin, jeudi matin, parti catholique...\n",
      "KB_JB421_1909-08-06_01-00001.txt mentions these keywords: Dom Bosco, prince Albert, affaires étrangères, cardinal Fischer, Moyen Age, congrès eucharistique, Caaille JOSET...\n",
      "KB_JB421_1909-10-23_01-00001.txt mentions these keywords: grande ligne, petite ligne, Faits-drt ANNONCES, prince Albert, VIEUX DOMESTIQUES, service général, vrais noms, Clément XIV, o'bscm Havas, FORFAIT ttffett, Camille ciOSET, vieux serviteurs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Charles Croa considéré', 0.0029370312473949597),\n",
       " ('français Charles Croa', 0.007372207109217219),\n",
       " ('août', 0.009226624320164533),\n",
       " ('heures', 0.011097201220840047),\n",
       " ('rayon', 0.012074540896682629),\n",
       " ('Charles Croa', 0.013159482370496954),\n",
       " ('Croa considéré', 0.0141123525169677),\n",
       " ('ROUGE Edison revendique', 0.014362501024402955),\n",
       " (\"c'est\", 0.014604325631629743),\n",
       " (\"ot'savant français Charles\", 0.025929797378944734),\n",
       " ('étoiles filantes', 0.02748617855962668),\n",
       " ('Jeudi', 0.027524975695742946),\n",
       " (\"poète ot'savant français\", 0.0280275387270016),\n",
       " ('Edison', 0.0302702276085155),\n",
       " ('Edison revendique', 0.033120346362984984),\n",
       " ('local habituel', 0.033341098927339774),\n",
       " (\"étoiles filantes C'est\", 0.03656490903713678),\n",
       " ('FEDERATION', 0.03786731375020705),\n",
       " ('étoiles', 0.03877539687205935),\n",
       " ('Charles Cros déposa', 0.045843631060012185),\n",
       " ('revendique la priorité', 0.046973420429231486),\n",
       " ('signalé dernièrement', 0.046973420429231486),\n",
       " ('dernièrement la mort', 0.046973420429231486),\n",
       " ('mort du poète', 0.046973420429231486),\n",
       " (\"poète ot'savant\", 0.046973420429231486),\n",
       " ('Tomes', 0.047795612382732625),\n",
       " ('synthétiques Thomas EDISON', 0.04821109142895892),\n",
       " (\"Charles Cros n'ait\", 0.048380820835442234),\n",
       " ('local', 0.04860618682510445),\n",
       " ('soir', 0.04869149279040057),\n",
       " ('Charles', 0.04942491403235117),\n",
       " (\"Edison n'aurait l'ait\", 0.05102132621611343),\n",
       " ('terre', 0.055363574389345194),\n",
       " ('filantes', 0.055643580740740654),\n",
       " ('grand', 0.05623230341585566),\n",
       " ('Thomas EDISON', 0.056568617763353486),\n",
       " ('Vendredi', 0.059756209267992924),\n",
       " ('ROUGE Edison', 0.06143413884031184),\n",
       " ('Samedi', 0.0625959614009114),\n",
       " ('Ghetto', 0.06331775737426223),\n",
       " ('Dimanche', 0.06486475967446324),\n",
       " ('Croa', 0.0656984433006752),\n",
       " ('phonographe', 0.07058045846764091),\n",
       " ('fer', 0.07079739875843562),\n",
       " ('habituel', 0.07144967584574388),\n",
       " ('Edison vient', 0.07227825083461525),\n",
       " (\"Edison n'aurait\", 0.07882621789555411),\n",
       " ('chant', 0.07891067186951502),\n",
       " ('fragments', 0.08030238131967903),\n",
       " ('rêveurs du Ghetto', 0.08117688802300661)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OLDEST version bc I'm scared even if versionning exists I like having it here \n",
    "\n",
    "all_token_keywords = {}  # Store keywords for each file in a dictionary \n",
    "\n",
    "for f in sorted(tokenised_files)[:10]:\n",
    "    tokenised_text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    token_keywords = kw_extractor.extract_keywords(tokenised_text)\n",
    "    all_token_keywords[f] = token_keywords  # Save to dictionary\n",
    "    #print(token_keywords) #this seems to have the scores associated with it, and seems to consider trigrams as well, idk what to do with that yet but I'm leaving it here for now \n",
    "    \n",
    "    kept_tokens = []\n",
    "    for kw, score in token_keywords:\n",
    "        token_words = kw.split()\n",
    "        if len(token_words) == 2: # when i tried all_token_keywords here AND when i use token words again it gives it to me in a different order, wtf\n",
    "            kept_tokens.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join((kept_tokens))}...\")\n",
    "    \n",
    "\n",
    "#fixed! I used kept and not kept_tokens lol \n",
    "\n",
    "\n",
    "#see the kw and score of a \n",
    "all_token_keywords['KB_JB1051_1927-08-25_01-00004.txt']\n",
    "\n",
    "\n",
    "#oh this is SO weird, KB_JB1051_1937-11-14_01-00006.txt is showing kw (first ones Baleine, Soviétique, record mondial, comète,) \n",
    "#BUT none of the others are, and that's not even one that I've ran individually before as far as I'm aware\n",
    "#ah yeah bc I wasn't paying attention and I used the wrong variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137821d1",
   "metadata": {},
   "source": [
    "# FROM HERE DOWN WE'RE IN DRAFT MODE \n",
    "Tokenised all the files, now I'm working on kw extraction for each of the files so we can start comparing, maybe then only take the lowest vectors from the dataset and start making the word cloud from there (that way the keywords of EACH file are preserved, not just the bashed kw (actually wait it'd be interesting to see if they'd be different(I should include that in the analysis as the limits of NLP???)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KB_JB421_1909-05-07_01-00003.txt': [('Chronique Régionale Mémorandum',\n",
       "   0.0002689124983344946),\n",
       "  ('Régionale Mémorandum VENDREDI', 0.002386733429198891),\n",
       "  ('Chronique Régionale', 0.004150330458462885),\n",
       "  ('Régionale Mémorandum', 0.004150330458462885),\n",
       "  ('mais', 0.013958866031261659),\n",
       "  (\"D'un\", 0.01929719188917196),\n",
       "  (\"d'une\", 0.02018759615058267),\n",
       "  ('Arlon', 0.023584878032698226),\n",
       "  ('ans', 0.0246003628612249),\n",
       "  ('jour', 0.02690405234751007),\n",
       "  ('matin', 0.027702988285563952),\n",
       "  ('jours', 0.030267058890948825),\n",
       "  (\"qu'elle\", 0.032446463309300795),\n",
       "  (\"C'est\", 0.032944929355833284),\n",
       "  ('temps', 0.035042085839226515),\n",
       "  ('Mémorandum VENDREDI', 0.035807270629173875),\n",
       "  (\"qu'il\", 0.0382299828576165),\n",
       "  ('grande', 0.041327147604164755),\n",
       "  (\"s'est\", 0.047010353723982996),\n",
       "  ('saint', 0.049186221326980764),\n",
       "  (\"d'une grande\", 0.05153615966742675),\n",
       "  ('jeune Vaucoret', 0.05160650341857592),\n",
       "  ('Point', 0.05243635803513611),\n",
       "  (\"d'une grande maison\", 0.056975265432775514),\n",
       "  ('lundi matin', 0.05727133296873441),\n",
       "  ('ENFANT', 0.060799705865179444),\n",
       "  ('Chronique', 0.06428978465134347),\n",
       "  ('Régionale', 0.06428978465134347),\n",
       "  ('Mémorandum', 0.06428978465134347),\n",
       "  ('jeune', 0.06555799422479636),\n",
       "  ('marche', 0.06630093637373444),\n",
       "  ('jeudi matin', 0.06963786072563034),\n",
       "  ('MAI', 0.06979433015630829),\n",
       "  ('grand', 0.06995185316632929),\n",
       "  ('soleil', 0.07160622628873936),\n",
       "  ('nuit', 0.07161723195027286),\n",
       "  ('police', 0.07190063449125743),\n",
       "  ('heures', 0.07257092823245526),\n",
       "  (\"suite d'un\", 0.0726277018112244),\n",
       "  ('Belgique', 0.07282867340359367),\n",
       "  ('pris', 0.07331881367994549),\n",
       "  ('Servin', 0.0769212680012506),\n",
       "  (\"bourgmestre d'un petit\", 0.08020818032111693),\n",
       "  ('hier', 0.08190373494434151),\n",
       "  ('bourgmestre', 0.08211939626804769),\n",
       "  ('Epinettes', 0.08304424139003161),\n",
       "  (\"qu'il fut\", 0.084073682760569),\n",
       "  ('mort', 0.08488205448059288),\n",
       "  ('Vaucoret', 0.08520688279873276),\n",
       "  ('midi', 0.08528643295740043)],\n",
       " 'KB_JB421_1909-08-06_01-00001.txt': [('Cologne', 0.01278982588241806),\n",
       "  (\"c'est\", 0.015946951287173276),\n",
       "  ('Congrès', 0.025073930485691523),\n",
       "  (\"qu'il\", 0.03273510258557793),\n",
       "  (\"s'est\", 0.034325348044540634),\n",
       "  ('francs', 0.03965049046173783),\n",
       "  ('Roi', 0.04138784142488191),\n",
       "  ('LUXEM Organe Quotidien', 0.04334744877318239),\n",
       "  ('Organe Quotidien Catholique', 0.04412679789736237),\n",
       "  ('marks', 0.04553426957268652),\n",
       "  ('Belgique', 0.04764405050497673),\n",
       "  ('Salésiens', 0.05152114030001903),\n",
       "  ('Dom Bosco', 0.05285040669573859),\n",
       "  ('cathédrale', 0.05586989452566482),\n",
       "  ('heures', 0.057584849334965216),\n",
       "  ('vue', 0.060910628088114954),\n",
       "  ('NONCES', 0.06241272328876263),\n",
       "  ('ville', 0.0625867102955465),\n",
       "  ('monde', 0.06349367165792075),\n",
       "  (\"d'une\", 0.06523273091111352),\n",
       "  ('ministre', 0.06588428605805847),\n",
       "  ('prince Albert', 0.06716601407686225),\n",
       "  ('temps', 0.0674437987327909),\n",
       "  ('jour', 0.07336326246183583),\n",
       "  ('cardinal Fischer', 0.07738016163387736),\n",
       "  ('Albert', 0.07762035033220036),\n",
       "  ('affaires étrangères', 0.07801557482839135),\n",
       "  ('Bruxelles', 0.0785700352326191),\n",
       "  ('cardinal', 0.079095212877052),\n",
       "  ('catholiques', 0.08128388197047456),\n",
       "  (\"d'un\", 0.08402124822090382),\n",
       "  ('nom', 0.08461229083323706),\n",
       "  ('faire', 0.08553554172019345),\n",
       "  ('Moyen Age', 0.08590291283247567),\n",
       "  ('traitement', 0.08706338384655588),\n",
       "  ('chancelier', 0.08820857671854174),\n",
       "  (\"qu'on\", 0.08853593427838431),\n",
       "  ('TKAITB A FORFAIT', 0.09294821752900179),\n",
       "  ('congrès eucharistique', 0.09345579846010814),\n",
       "  ('Dom', 0.0944817150847845),\n",
       "  ('maisons', 0.0945563106070927),\n",
       "  ('société', 0.09626309888751855),\n",
       "  ('année', 0.09781279595821184),\n",
       "  ('grande', 0.09857105319762716),\n",
       "  ('nouvelles', 0.1031491917706524),\n",
       "  ('villes', 0.10431118382591081),\n",
       "  ('monde Hpr Dar', 0.10481458057256489),\n",
       "  ('pension', 0.1068147402240219),\n",
       "  ('conseil', 0.11252698130215232),\n",
       "  ('arable Dom Bosco', 0.11268344674415999)],\n",
       " 'KB_JB421_1909-10-23_01-00001.txt': [('ligne', 0.006576956516920981),\n",
       "  ('grande ligne', 0.010414814932783081),\n",
       "  ('jjjjgjs et Jugements', 0.012405281434844163),\n",
       "  (\"d'un\", 0.012768895090072173),\n",
       "  (\"qu'il\", 0.01404661186692881),\n",
       "  ('rue', 0.018646918396135623),\n",
       "  ('petite ligne', 0.022057795950928976),\n",
       "  ('Belgique', 0.022894960513796105),\n",
       "  ('Faits-drt ANNONCES', 0.02459954981037721),\n",
       "  ('service', 0.02593999816391375),\n",
       "  (\"qu'ils\", 0.032775427689500564),\n",
       "  ('bons', 0.03291445281262834),\n",
       "  ('faire', 0.03583407481960633),\n",
       "  ('bons vieux serviteurs', 0.03631056971969405),\n",
       "  ('TRAITE A FORFAIT', 0.03783214099594524),\n",
       "  ('bon', 0.0384001949480664),\n",
       "  ('prince', 0.039173282293782995),\n",
       "  ('Albert', 0.041280351933826676),\n",
       "  ('prince Albert', 0.04207603403127464),\n",
       "  (\"c'est\", 0.042311261418854614),\n",
       "  (\"d'une\", 0.044920043564026534),\n",
       "  ('rue Clément XIV', 0.04628643594921162),\n",
       "  (\"qu'il possède rue\", 0.05113952925236211),\n",
       "  (\"l'on\", 0.053223547434741274),\n",
       "  ('VIEUX', 0.054648719572990195),\n",
       "  (\"service d'un\", 0.05596176939282363),\n",
       "  (\"service d'un réel\", 0.06172615335668248),\n",
       "  ('Faits-drt', 0.06344636920368098),\n",
       "  ('Jugements', 0.06344636920368098),\n",
       "  (\"qu'elle\", 0.06537325387176664),\n",
       "  (\"maîtres qu'ils\", 0.06571725664248096),\n",
       "  (\"qu'ils out Sru\", 0.06646114148363362),\n",
       "  ('fut', 0.06771678728633244),\n",
       "  (\"rue d'Argeni ixellea\", 0.06795878962731165),\n",
       "  ('visiteurs', 0.0700874380461356),\n",
       "  ('hommes', 0.07198343152781589),\n",
       "  (\"maîtres qu'ils obtien\", 0.0732270707670453),\n",
       "  (\"nom d'un\", 0.07344637932889118),\n",
       "  ('rues', 0.07458767358454249),\n",
       "  ('VIEUX DOMESTIQUES', 0.07481074021820261),\n",
       "  ('vieux braves serviteurs', 0.0759595325253067),\n",
       "  ('fois', 0.07617135785874536),\n",
       "  (\"vieux hasselt qu'il\", 0.07677189715384393),\n",
       "  ('maîtres', 0.07839309630159925),\n",
       "  ('service général', 0.07841787651101065),\n",
       "  ('Ferrer', 0.07912366810476722),\n",
       "  ('bons vieux', 0.08172049304546637),\n",
       "  ('Palais', 0.08261737850588843),\n",
       "  ('comète', 0.08294105788268245),\n",
       "  (\"Bruxelles D'un relevé\", 0.08302961467871471)]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_1909 = {\n",
    "    fname: kws for fname, kws in all_token_keywords.items()\n",
    "    if fname.split('_')[2].startswith('1909') #testing with the 10 files\n",
    "}\n",
    "\n",
    "print(len(keywords_1909))\n",
    "keywords_1909 \n",
    "#ok, but I still need to filter out stop words... Before or after keywords? I think before yeah duh cause math "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
