{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229a73ca",
   "metadata": {},
   "source": [
    "# Analyse de la dist. du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c004a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9734c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\casia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') #liste pré-définie de stopwords\n",
    "from nltk.corpus import stopwords #attention de bien nettoyer les stopwords !! sinon résultats bizarres\n",
    "\n",
    "import os\n",
    "import yake #(Yet Another Keyword Extractor) \"that uses text statistical features to select the most important keywords from a document\"\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud #besoin de compilateur c++ pour l'installer\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import re #pour les expressions régulières afin de nettoyer l OCRisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7441efd",
   "metadata": {},
   "source": [
    "### Gerer les stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6e5c",
   "metadata": {},
   "source": [
    "#### Créer une liste des stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284e0c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 stopwords:\n",
      " ['-', ':', 'DRAPEAU', 'LE', 'ROUGE', 'a', 'abord', 'absolument', 'afin', 'ah', 'ai', 'aie', 'aient', 'aies', 'ailleurs', 'ainsi', 'ait', 'allaient', 'allo', 'allons', 'allô', 'alors', 'année', 'ans', 'anterieur', 'anterieure', 'anterieures', 'apres', 'après', 'as', 'assez', 'attendu', 'au', 'aucun', 'aucune', 'aucuns', 'aujourd', \"aujourd'hui\", 'aupres', 'auquel', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autant', 'autre', 'autrefois', 'autrement', 'autres', 'autrui', 'aux', 'auxquelles', 'auxquels', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayante', 'ayantes', 'ayants', 'ayez', 'ayons', 'bah', 'bas', 'basee', 'bat', 'beau', 'beaucoup', 'bien', 'bigre', 'bon', 'boum', 'bravo', 'c', 'car', 'ce', 'ceci', 'cela', 'celle', 'celle-ci', 'celle-là', 'celles', 'celles-ci', 'celles-là', 'celui', 'celui-ci', 'celui-là', 'celà', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'ces', 'cet', 'cette', 'ceux', 'ceux-ci', 'ceux-là', 'chacun', 'chacune', 'chaque', 'cher', 'chers', 'chez', 'chiche', 'chut', 'chère', 'chères', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantième', 'cinquième', 'clac', 'clic', 'com', 'combien', 'comme', 'comment', 'comparable', 'comparables', 'compris', 'concernant', 'contre', 'couic', 'crac', 'd', \"d'un\", \"d'une\", 'dans', 'de', 'debout', 'dedans', 'dehors', 'deja', 'delà', 'depuis', 'dernier', 'derniere', 'derriere', 'derrière', 'des', 'desormais', 'desquelles', 'desquels', 'dessous', 'dessus', 'deux', 'deuxième', 'deuxièmement', 'devant', 'devers', 'devra', 'devrait', 'different', 'differentes', 'differents', 'différent', 'différente', 'différentes', 'différents', 'dire', 'directe', 'directement', 'dit', 'dite', 'dits', 'divers', 'diverse', 'diverses', 'dix', 'dix-huit', 'dix-neuf', 'dix-sept', 'dixième', 'doit', 'doivent', 'donc', 'dont', 'dos', 'douze', 'douzième', 'drapeau', 'dring', 'droite', 'du', 'duquel', 'durant', 'dès', 'début', 'désormais', 'effet', 'egale', 'egalement', 'egales', 'eh', 'elle', 'elle-même', 'elles', 'elles-mêmes', 'en', 'encore', 'enfin', 'entre', 'envers', 'environ', 'es', 'essai', 'est', 'et', 'etant', 'etc', 'etre', 'eu', 'eue', 'eues', 'euh', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eux-mêmes', 'exactement', 'excepté', 'extenso', 'exterieur', 'eûmes', 'eût', 'eûtes', 'f', 'faire', 'fais', 'faisaient', 'faisant', 'fait', 'faites', 'faut', 'façon', 'feront', 'fi', 'flac', 'floc', 'fois', 'font', 'force', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'gens', 'haut', 'hein', 'hem', 'hep', 'het', 'holà', 'hop', 'hormis', 'hors', 'hou', 'houp', 'hue', 'hui', 'huit', 'huitième', 'hum', 'hurrah', 'hé', 'hélas', 'i', 'ici', 'il', 'ils', 'importe', 'j', 'je', 'jour', 'jusqu', 'jusque', 'juste', 'l', \"l'on\", 'la', 'laisser', 'laquelle', 'las', 'le', 'lequel', 'les', 'lesquelles', 'lesquels', 'leur', 'leurs', 'longtemps', 'lors', 'lorsque', 'lui', 'lui-meme', 'lui-même', 'là', 'lès', 'm', 'ma', 'maint', 'maintenant', 'mais', 'malgre', 'malgré', 'maximale', 'me', 'meme', 'memes', 'merci', 'mes', 'mien', 'mienne', 'miennes', 'miens', 'mille', 'mince', 'mine', 'minimale', 'moi', 'moi-meme', 'moi-même', 'moindres', 'moins', 'mon', 'mot', 'moyennant', 'multiple', 'multiples', 'même', 'mêmes', 'n', \"n'anaturel\", 'na', 'naturelle', 'naturelles', 'ne', 'neanmoins', 'necessaire', 'necessairement', 'neuf', 'neuvième', 'ni', 'nombreuses', 'nombreux', 'nommés', 'non', 'nos', 'notamment', 'notre', 'nous', 'nous-mêmes', 'nouveau', 'nouveaux', 'nul', 'néanmoins', 'nôtre', 'nôtres', 'oh', 'ohé', 'ollé', 'olé', 'on', 'ont', 'onze', 'onzième', 'ore', 'ou', 'ouf', 'ouias', 'oust', 'ouste', 'outre', 'ouvert', 'ouverte', 'ouverts', 'où', 'paf', 'par', 'parce', 'parfois', 'parle', 'parlent', 'parler', 'parmi', 'parole', 'parseme', 'partant', 'particulier', 'particulière', 'particulièrement', 'pas', 'passé', 'pendant', 'pense', 'permet', 'personne', 'personnes', 'peu', 'peut', 'peuvent', 'peux', 'pff', 'pfft', 'pfut', 'pif', 'pire', 'pièce', 'plein', 'plouf', 'plupart', 'plus', 'plusieurs', 'plutôt', 'possessif', 'possessifs', 'possible', 'possibles', 'pouah', 'pour', 'pourquoi', 'pourrais', 'pourrait', 'pouvait', 'prealable', 'precisement', 'premier', 'première', 'premièrement', 'pres', 'probable', 'probante', 'procedant', 'proche', 'près', 'psitt', 'pu', 'puis', 'puisque', 'pur', 'pure', 'qu', \"qu'elle\", \"qu'elles\", \"qu'il\", \"qu'ils\", 'quand', 'quant', 'quant-à-soi', 'quanta', 'quarante', 'quatorze', 'quatre', 'quatre-vingt', 'quatrième', 'quatrièmement', 'que', 'quel', 'quelconque', 'quelle', 'quelles', \"quelqu'un\", 'quelque', 'quelques', 'quels', 'qui', 'quiconque', 'quinze', 'quoi', 'quoique', 'r', 'rare', 'rarement', 'rares', 'relative', 'relativement', 'remarquable', 'rend', 'rendre', 'restant', 'reste', 'restent', 'restrictif', 'retour', 'revoici', 'revoilà', 'rien', 'rue', 's', 'sa', 'sacrebleu', 'sait', 'sans', 'sapristi', 'sauf', 'se', 'sein', 'seize', 'selon', 'semblable', 'semblaient', 'semble', 'semblent', 'sent', 'sept', 'septième', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seul', 'seule', 'seulement', 'si', 'sien', 'sienne', 'siennes', 'siens', 'sinon', 'six', 'sixième', 'soi', 'soi-même', 'soient', 'sois', 'soit', 'soixante', 'sommes', 'son', 'sont', 'sous', 'souvent', 'soyez', 'soyons', 'specifique', 'specifiques', 'speculatif', 'stop', 'strictement', 'subtiles', 'suffisant', 'suffisante', 'suffit', 'suis', 'suit', 'suivant', 'suivante', 'suivantes', 'suivants', 'suivre', 'sujet', 'superpose', 'sur', 'surtout', 't', 'ta', 'tac', 'tandis', 'tant', 'tardive', 'te', 'tel', 'telle', 'tellement', 'telles', 'tels', 'tenant', 'tend', 'tenir', 'tente', 'tes', 'tic', 'tien', 'tienne', 'tiennes', 'tiens', 'toc', 'toi', 'toi-même', 'ton', 'touchant', 'toujours', 'tous', 'tout', 'toute', 'toutefois', 'toutes', 'treize', 'trente', 'tres', 'trois', 'troisième', 'troisièmement', 'trop', 'très', 'tsoin', 'tsouin', 'tu', 'té', 'un', 'une', 'unes', 'uniformement', 'unique', 'uniques', 'uns', 'va', 'vais', 'valeur', 'van', 'vas', 'vers', 'via', 'vif', 'vifs', 'vingt', 'vivat', 'vive', 'vives', 'vlan', 'voici', 'voie', 'voient', 'voilà', 'voir', 'voire', 'vont', 'vos', 'votre', 'vous', 'vous-mêmes', 'vu', 'vé', 'vôtre', 'vôtres', 'y', 'zut', 'à', 'â', 'ça', 'ès', 'étaient', 'étais', 'était', 'étant', 'étante', 'étantes', 'étants', 'état', 'étiez', 'étions', 'été', 'étée', 'étées', 'étés', 'êtes', 'être', 'ô']\n"
     ]
    }
   ],
   "source": [
    "#jouer encore avec cela pour en ajouter des nouveaux en fonction du contexte, modifié de la liste de https://github.com/stopwords-iso/stopwords-fr?tab=readme-ov-file, à verifer sa fidelité\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \"elles\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \":\", \"même\", \"voir\", \"fois\", \"jour\", \"année\", \"ans\", \"faites\", \"le\", \"la\", \"de\"]\n",
    "\n",
    "# New stopwords to add\n",
    "git_stopwords = [\n",
    "    \"a\", \"abord\", \"absolument\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\", \"aies\", \"ailleurs\", \"ainsi\", \"ait\", \"allaient\", \"allo\", \"allons\", \"allô\", \"alors\", \"anterieur\", \"anterieure\", \"anterieures\", \"apres\", \"après\", \"as\", \"assez\", \"attendu\", \"au\", \"aucun\", \"aucune\", \"aucuns\", \"aujourd\", \"aujourd'hui\", \"aupres\", \"auquel\", \"aura\", \"aurai\", \"auraient\", \"aurais\", \"aurait\", \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autant\", \"autre\", \"autrefois\", \"autrement\", \"autres\", \"autrui\", \"aux\", \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\", \"avez\", \"aviez\", \"avions\", \"avoir\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"bah\", \"bas\", \"basee\", \"bat\", \"beau\", \"beaucoup\", \"bien\", \"bigre\", \"bon\", \"boum\", \"bravo\", \"car\", \"ce\", \"ceci\", \"cela\", \"celle\", \"celle-ci\", \"celle-là\", \"celles\", \"celles-ci\", \"celles-là\", \"celui\", \"celui-ci\", \"celui-là\", \"celà\", \"cent\", \"cependant\", \"certain\", \"certaine\", \"certaines\", \"certains\", \"certes\", \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\", \"chacun\", \"chacune\", \"chaque\", \"cher\", \"chers\", \"chez\", \"chiche\", \"chut\", \"chère\", \"chères\", \"ci\", \"cinq\", \"cinquantaine\", \"cinquante\", \"cinquantième\", \"cinquième\", \"clac\", \"clic\", \"combien\", \"comme\", \"comment\", \"comparable\", \"comparables\", \"compris\", \"concernant\", \"contre\", \"couic\", \"crac\", \"dans\", \"de\", \"debout\", \"dedans\", \"dehors\", \"deja\", \"delà\", \"depuis\", \"dernier\", \"derniere\", \"derriere\", \"derrière\", \"des\", \"desormais\", \"desquelles\", \"desquels\", \"dessous\", \"dessus\", \"deux\", \"deuxième\", \"deuxièmement\", \"devant\", \"devers\", \"devra\", \"devrait\", \"different\", \"differentes\", \"differents\", \"différent\", \"différente\", \"différentes\", \"différents\", \"dire\", \"directe\", \"directement\", \"dit\", \"dite\", \"dits\", \"divers\", \"diverse\", \"diverses\", \"dix\", \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dixième\", \"doit\", \"doivent\", \"donc\", \"dont\", \"dos\", \"douze\", \"douzième\", \"dring\", \"droite\", \"du\", \"duquel\", \"durant\", \"dès\", \"début\", \"désormais\", \"effet\", \"egale\", \"egalement\", \"egales\", \"eh\", \"elle\", \"elle-même\", \"elles\", \"elles-mêmes\", \"en\", \"encore\", \"enfin\", \"entre\", \"envers\", \"environ\", \"es\", \"essai\", \"est\", \"et\", \"etant\", \"etc\", \"etre\", \"eu\", \"eue\", \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\", \"eusses\", \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eux-mêmes\", \"exactement\", \"excepté\", \"extenso\", \"exterieur\", \"eûmes\", \"eût\", \"eûtes\", \"f\", \"fais\", \"faisaient\", \"faisant\", \"fait\", \"faites\", \"façon\", \"feront\", \"fi\", \"flac\", \"floc\", \"fois\", \"font\", \"force\", \"furent\", \"fus\", \"fusse\", \"fussent\", \"fusses\", \"fussiez\", \"fussions\", \"fut\", \"fûmes\", \"fût\", \"fûtes\", \"gens\", \"haut\", \"hein\", \"hem\", \"hep\", \"holà\", \"hop\", \"hormis\", \"hors\", \"hou\", \"houp\", \"hue\", \"hui\", \"huit\", \"huitième\", \"hum\", \"hurrah\", \"hé\", \"hélas\", \"i\", \"ici\", \"il\", \"ils\", \"importe\", \"j\", \"je\", \"jusqu\", \"jusque\", \"juste\", \"la\", \"laisser\", \"laquelle\", \"las\", \"le\", \"lequel\", \"les\", \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"longtemps\", \"lors\", \"lorsque\", \"lui\", \"lui-meme\", \"lui-même\", \"là\", \"lès\", \"ma\", \"maint\", \"maintenant\", \"mais\", \"malgre\", \"malgré\", \"maximale\", \"me\", \"meme\", \"memes\", \"merci\", \"mes\", \"mien\", \"mienne\", \"miennes\", \"miens\", \"mille\", \"mince\", \"mine\", \"minimale\", \"moi\", \"moi-meme\", \"moi-même\", \"moindres\", \"moins\", \"mon\", \"mot\", \"moyennant\", \"multiple\", \"multiples\", \"même\", \"mêmes\", \"na\", \"n'a\" \"naturel\", \"naturelle\", \"naturelles\", \"ne\", \"neanmoins\", \"necessaire\", \"necessairement\", \"neuf\", \"neuvième\", \"ni\", \"nombreuses\", \"nombreux\", \"nommés\", \"non\", \"nos\", \"notamment\", \"notre\", \"nous\", \"nous-mêmes\", \"nouveau\", \"nouveaux\", \"nul\", \"néanmoins\", \"nôtre\", \"nôtres\", \"oh\", \"ohé\", \"ollé\", \"olé\", \"on\", \"ont\", \"onze\", \"onzième\", \"ore\", \"ou\", \"ouf\", \"ouias\", \"oust\", \"ouste\", \"outre\", \"ouvert\", \"ouverte\", \"ouverts\", \"où\", \"paf\", \"par\", \"parce\", \"parfois\", \"parle\", \"parlent\", \"parler\", \"parmi\", \"parole\", \"parseme\", \"partant\", \"particulier\", \"particulière\", \"particulièrement\", \"pas\", \"passé\", \"pendant\", \"pense\", \"permet\", \"personne\", \"personnes\", \"peu\", \"peut\", \"peuvent\", \"peux\", \"pff\", \"pfft\", \"pfut\", \"pif\", \"pire\", \"pièce\", \"plein\", \"plouf\", \"plupart\", \"plus\", \"plusieurs\", \"plutôt\", \"possessif\", \"possessifs\", \"possible\", \"possibles\", \"pouah\", \"pour\", \"pourquoi\", \"pourrais\", \"pourrait\", \"pouvait\", \"prealable\", \"precisement\", \"premier\", \"première\", \"premièrement\", \"pres\", \"probable\", \"probante\", \"procedant\", \"proche\", \"près\", \"psitt\", \"pu\", \"puis\", \"puisque\", \"pur\", \"pure\", \"qu\", \"quand\", \"quant\", \"quant-à-soi\", \"quanta\", \"quarante\", \"quatorze\", \"quatre\", \"quatre-vingt\", \"quatrième\", \"quatrièmement\", \"que\", \"quel\", \"quelconque\", \"quelle\", \"quelles\", \"quelqu'un\", \"quelque\", \"quelques\", \"quels\", \"qui\", \"quiconque\", \"quinze\", \"quoi\", \"quoique\", \"r\", \"rare\", \"rarement\", \"rares\", \"relative\", \"relativement\", \"remarquable\", \"rend\", \"rendre\", \"restant\", \"reste\", \"restent\", \"restrictif\", \"retour\", \"revoici\", \"revoilà\", \"rien\", \"sa\", \"sacrebleu\", \"sait\", \"sans\", \"sapristi\", \"sauf\", \"se\", \"sein\", \"seize\", \"selon\", \"semblable\", \"semblaient\", \"semble\", \"semblent\", \"sent\", \"sept\", \"septième\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\", \"seriez\", \"serions\", \"serons\", \"seront\", \"ses\", \"seul\", \"seule\", \"seulement\", \"si\", \"sien\", \"sienne\", \"siennes\", \"siens\", \"sinon\", \"six\", \"sixième\", \"soi\", \"soi-même\", \"soient\", \"sois\", \"soit\", \"soixante\", \"sommes\", \"son\", \"sont\", \"sous\", \"souvent\", \"soyez\", \"soyons\", \"specifique\", \"specifiques\", \"speculatif\", \"stop\", \"strictement\", \"subtiles\", \"suffisant\", \"suffisante\", \"suffit\", \"suis\", \"suit\", \"suivant\", \"suivante\", \"suivantes\", \"suivants\", \"suivre\", \"sujet\", \"superpose\", \"sur\", \"surtout\", \"ta\", \"tac\", \"tandis\", \"tant\", \"tardive\", \"te\", \"tel\", \"telle\", \"tellement\", \"telles\", \"tels\", \"tenant\", \"tend\", \"tenir\", \"tente\", \"tes\", \"tic\", \"tien\", \"tienne\", \"tiennes\", \"tiens\", \"toc\", \"toi\", \"toi-même\", \"ton\", \"touchant\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\", \"treize\", \"trente\", \"tres\", \"trois\", \"troisième\", \"troisièmement\", \"trop\", \"très\", \"tsoin\", \"tsouin\", \"tu\", \"té\", \"un\", \"une\", \"unes\", \"uniformement\", \"unique\", \"uniques\", \"uns\", \"va\", \"vais\", \"valeur\", \"vas\", \"vers\", \"via\", \"vif\", \"vifs\", \"vingt\", \"vivat\", \"vive\", \"vives\", \"vlan\", \"voici\", \"voie\", \"voient\", \"voilà\", \"voire\", \"vont\", \"vos\", \"votre\", \"vous\", \"vous-mêmes\", \"vu\", \"vé\", \"vôtre\", \"vôtres\", \"zut\", \"à\", \"â\", \"ça\", \"ès\", \"étaient\", \"étais\", \"était\", \"étant\", \"état\", \"étiez\", \"étions\", \"été\", \"étée\", \"étées\", \"étés\", \"êtes\", \"être\", \"ô\"\n",
    "]\n",
    "\n",
    "#domaine specific \n",
    "specific_stopwords = [\n",
    "\"rue\", \"-\", \"drapeau\", \"DRAPEAU\", \"LE\", \"ROUGE\", \"com\", \"qu'il\", \"d'une\", \"d'un\", \"l'on\", \"qu'il\", \"qu'ils\", \"qu'elle\", \"qu'elles\"\n",
    "#garder les noms des villes pour faire une analyse spatiale? \n",
    "#does it also include common first names?\n",
    "#does it also include uppercase versions?\n",
    "#à voir : soir, matin = seront pe liés au nom du journal et non du temps de la journée, define what a stopword means in this context\n",
    "]\n",
    "\n",
    "sw += git_stopwords\n",
    "sw += specific_stopwords\n",
    "sw = set(sw)\n",
    "\n",
    "\n",
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b9054",
   "metadata": {},
   "source": [
    "#### Tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbb7d6",
   "metadata": {},
   "source": [
    "Créer un fichier bash qui concatène tous les fichiers txt de halley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19aa408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4858920 words found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['yF',\n",
       " ':',\n",
       " 'LE',\n",
       " 'DRAPEAU',\n",
       " 'ROUGE',\n",
       " 'Edison',\n",
       " 'revendique',\n",
       " 'la',\n",
       " 'priorité',\n",
       " 'de',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'invention',\n",
       " 'du',\n",
       " 'phonographe',\n",
       " '.-',\n",
       " 'On',\n",
       " 'a',\n",
       " 'signalé',\n",
       " 'dernièrement',\n",
       " 'la',\n",
       " 'mort',\n",
       " 'du',\n",
       " 'poète',\n",
       " 'ot',\n",
       " \"'\",\n",
       " 'savant',\n",
       " 'français',\n",
       " 'Charles',\n",
       " 'Croa',\n",
       " 'considéré',\n",
       " 'com',\n",
       " '-',\n",
       " ',',\n",
       " 'mo',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'inventeur',\n",
       " 'du',\n",
       " 'phonographe',\n",
       " ',',\n",
       " 'dont',\n",
       " 'il',\n",
       " 'aurait',\n",
       " 'décrit',\n",
       " ',',\n",
       " 'le',\n",
       " 'premier',\n",
       " ',',\n",
       " 'le']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merci les profs\n",
    "path = \"../../data/halley/halley_txt/\" # Path to the directory containing text files\n",
    "\n",
    "with open(\"../../data/halley/halley_all.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                output_file.write(f.read())\n",
    "\n",
    "# Récupération du contenu du fichier bash\n",
    "path = \"../../data/halley/halley_all.txt\"\n",
    "limit = 10**8\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()[:limit]\n",
    "\n",
    "# Tokenization, sw pas encore appliqués\n",
    "words = nltk.wordpunct_tokenize(text) #tokenized words from the concatenated Halley corpus (halley_all.txt)\n",
    "print(f\"{len(words)} words found\")\n",
    "words[:50] \n",
    "\n",
    "# a big list of all of the keywords from all of the files, and then do a frequency analysis on that list to see what are the most common keywords across all of the texts = kinda loses all temporal analysis but ok for the overarching ALL keywords, \n",
    "# we should expect only the most common elements to come out? but key words isn't most common... careful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f336a",
   "metadata": {},
   "source": [
    "#### Eliminer les stopwords et les termes non alphabétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587064 words kept (220119 different word forms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rouge',\n",
       " 'edison',\n",
       " 'revendique',\n",
       " 'priorité',\n",
       " 'invention',\n",
       " 'phonographe',\n",
       " 'signalé',\n",
       " 'dernièrement',\n",
       " 'mort',\n",
       " 'poète',\n",
       " 'savant',\n",
       " 'français',\n",
       " 'charles',\n",
       " 'croa',\n",
       " 'considéré',\n",
       " 'inventeur',\n",
       " 'phonographe',\n",
       " 'décrit',\n",
       " 'mécanisme',\n",
       " 'edison']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows words after filtering stop words, much better\n",
    "kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "voc = set(kept) #creates a unique vocabulary (no duplicates)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")\n",
    "kept[:20] \n",
    "#1587064 words kept (220119 different word forms) \n",
    "#seems like a small difference but it's actually around 1500 words removed by the new stopword list\n",
    "\n",
    "\n",
    "#these still aren't the words I'd expect... double check the source\n",
    "#AH it's because the WHOLE page of ALL the pages isn't about halleys comet \n",
    "# do we go for an analysis of co-text as well then? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a87720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584198 words kept (217992 different word forms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rouge',\n",
       " 'edison',\n",
       " 'revendique',\n",
       " 'priorité',\n",
       " 'invention',\n",
       " 'phonographe',\n",
       " 'signalé',\n",
       " 'dernièrement',\n",
       " 'mort',\n",
       " 'poète',\n",
       " 'savant',\n",
       " 'français',\n",
       " 'charles',\n",
       " 'croa',\n",
       " 'considéré',\n",
       " 'inventeur',\n",
       " 'phonographe',\n",
       " 'décrit',\n",
       " 'mécanisme',\n",
       " 'edison']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept = [w.lower() for w in words \n",
    "        if len(w) > 2 \n",
    "        and w.isalpha() \n",
    "        and w.lower() not in sw\n",
    "        and not re.search(r'(.)\\1{2,}', w.lower())]\n",
    "\n",
    "voc = set(kept) #creates a unique vocabulary (no duplicates)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")\n",
    "kept[:20] \n",
    "#avec 5 caractères le même, 1586939 words kept (220004 different word forms) quand même une différence mais elle est assez minimale, dc environs 100 mots en moins \n",
    "#avec 3 carartères, 1586631 words kept (219739 different word forms)\n",
    "#avec 2, 1584198 words kept (217992 different word forms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbce334",
   "metadata": {},
   "source": [
    "#### Calculer la taille du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfac870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heures', 4175),\n",
       " ('bruxelles', 3613),\n",
       " ('mai', 3111),\n",
       " ('grand', 3095),\n",
       " ('prix', 2688),\n",
       " ('temps', 2602),\n",
       " ('lieu', 2568),\n",
       " ('comète', 2556),\n",
       " ('francs', 2549),\n",
       " ('soir', 2445)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#récupérer la fréquence des mots sur toute le dataset \n",
    "fdist = nltk.FreqDist(kept) #yes, kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "fdist.most_common(10)\n",
    "\n",
    "#heures might be on the chopping block for sw \n",
    "\n",
    "#voisins plus proches to halley would be useful\n",
    "#or maybe we do a co-text analysis of words around halley?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f98710",
   "metadata": {},
   "source": [
    "#### Variation from structure of modules \n",
    "I don't see what the plot adds, so I'm leaving it out for now\n",
    "I'm also leaving out les mots qui n'apparaissent qu'une fois dans le corpus et les plus longs pour l'instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36610332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wasanmmaâeaanssnnaéémmqwakmnscâéahaai',\n",
       " 'siiïlgipioïinciaiflgstibïagipiibïfgs',\n",
       " 'laterreetsemontrependantquelques',\n",
       " 'vbienterprogressîtomentlesdoses',\n",
       " 'nanisationdestribusalbanaises',\n",
       " 'ffanceestencoursdenégociation',\n",
       " 'reconnaissanceestenflnrendu',\n",
       " 'liqiicsdonnentrégulièrement',\n",
       " 'lechiffredesfréqùentations',\n",
       " 'disccmnsdecircemstajieefut',\n",
       " 'hautenbasaupetitconducleur',\n",
       " 'défenderessesjprétendaient',\n",
       " 'agencemaritimedekeyserthnm',\n",
       " 'respeesèeartueuesenjentle',\n",
       " 'celaetplusquejamaiscesont',\n",
       " 'industrielsetgommerçints',\n",
       " 'faulcroirelesliistoriens',\n",
       " 'curieuxquionteupourobjet',\n",
       " 'trouyalèntilarjueùtenant',\n",
       " 'naireouplutôtsavonpurol',\n",
       " 'pourlasociétéanonymeaes',\n",
       " 'correapondantparuculier',\n",
       " 'milièuxfoitternemsntaux',\n",
       " 'vaccinationantityphique',\n",
       " 'quainfientviesipeintres',\n",
       " 'femmecatherinètuytters',\n",
       " 'adtaiitenemfoqteifyehi',\n",
       " 'intéressantevloutefois',\n",
       " 'conduisentvdirectement',\n",
       " 'àlaoïnectlonoujcullnal',\n",
       " 'mentsnouveauxcapablesd',\n",
       " 'gouverneinéïïfiivavoir',\n",
       " 'approxlniiaitiivamieat',\n",
       " 'variantdelopfennigeàun',\n",
       " 'hésitezpascarlljrtrade',\n",
       " 'professionnelispéciaux',\n",
       " 'reconventionnellejnent',\n",
       " 'ajantdéjaclicnlèloboui',\n",
       " 'ilseufermenlsontmieux',\n",
       " 'avecnouvellesmachines',\n",
       " 'cüiarmeàvosrelationsj',\n",
       " 'cconistliutiotinelles',\n",
       " 'ijimmunaifltetegnmtvg',\n",
       " 'correctionnalisations',\n",
       " 'anticonstitutionnelle',\n",
       " 'cfintérieuriquelquies',\n",
       " 'complesurlasommedueau',\n",
       " 'presatlasuakoiufeolss',\n",
       " 'constitutionnellement',\n",
       " 'vendredtltwpteiaraitb',\n",
       " 'sijukicesttaaitements',\n",
       " 'paléosocloéconomistes',\n",
       " 'exceptiohnellementles',\n",
       " 'lfijuilletconchieavec',\n",
       " 'récentafaillianéantir',\n",
       " 'transactionsmvisibles',\n",
       " 'iiabitilntwfllenccbrt',\n",
       " 'gavageliîerckelïîacfl',\n",
       " 'enseignemenwndustriel',\n",
       " 'tribunalcorrectionnel',\n",
       " 'mléopsychoéconomistes',\n",
       " 'puissancessignataires',\n",
       " 'etdernièresivoüvefcle',\n",
       " 'studentenverbindungen',\n",
       " 'internationalisation',\n",
       " 'circonscriptionnairc',\n",
       " 'fesgiîcntoonstelling',\n",
       " 'masculinssiinnestire',\n",
       " 'intercommunalisation',\n",
       " 'gloeltampeufsbrleken',\n",
       " 'tzÿcanoatainutkmlqub',\n",
       " 'antirévolutionnaires',\n",
       " 'inconstitutionnalité',\n",
       " 'zivilfahndungsdienst',\n",
       " 'zlvilfahndungsdlenst',\n",
       " 'respolisabililiiéiet',\n",
       " 'départementaitdonnél',\n",
       " 'épilatoiraidéolvienf',\n",
       " 'discontogesellschaft',\n",
       " 'auglaisbongrisprompt',\n",
       " 'antistévolutionnaère',\n",
       " 'tribunalrprononiceta',\n",
       " 'raâiotôléppraphiqùes',\n",
       " 'nouvellesjidiciaires',\n",
       " 'ôompasnioradjoplione',\n",
       " 'jlivaitvoyagéavccuno',\n",
       " 'noüyellesluûlciaibës',\n",
       " 'radiotélégrapliiques',\n",
       " 'sacrificeskconsentis',\n",
       " 'partffioulmjïreûmemt',\n",
       " 'iusencordeponserque',\n",
       " 'réapprovisionnement',\n",
       " 'croissaientquelques',\n",
       " 'keprtsiaaiatisiious',\n",
       " 'vrateemiblalbiamqpt',\n",
       " 'pétitiousnombreuses',\n",
       " 'weœstatknffbrebahbé',\n",
       " 'psycliopliysioloaîe',\n",
       " 'gamveriaeurdaiparis',\n",
       " 'administnativeanent',\n",
       " 'ophedechemindeferen',\n",
       " 'devisenschutzkomman',\n",
       " 'interdépartementale',\n",
       " 'fkcaisitlicrsttaors',\n",
       " 'stariationsduracing',\n",
       " 'âbjumcâïïotpubliçue',\n",
       " 'paragravitationnels',\n",
       " 'oondationnelleipent',\n",
       " 'interunlversltalres',\n",
       " 'uixuijeiectnwecoust',\n",
       " 'eïposilioniournirai',\n",
       " 'interuniversitairee',\n",
       " 'dôlssnsiliivsamient',\n",
       " 'pùissancësrôpéennes',\n",
       " 'perpendiculairement',\n",
       " 'processionnellement',\n",
       " 'rommissnirenrijoint',\n",
       " 'furentdécouvertssix',\n",
       " 'conventionnellement',\n",
       " 'qualiléiéquivalente',\n",
       " 'duremensupprimerait',\n",
       " 'ctiavilleiadastriel',\n",
       " 'interuniversotaitre',\n",
       " 'intentjoniicllenivi',\n",
       " 'tbatrachomyomschiej',\n",
       " 'ipouvqnvjudàcittire',\n",
       " 'eleciricitélittoral',\n",
       " 'radicauxsociaiistes',\n",
       " 'plïiilœitliropiqaie',\n",
       " 'dëjphysïcotiiéripîe',\n",
       " 'concertsscolairesne',\n",
       " 'lomodedecérébration',\n",
       " 'earactéristiquement',\n",
       " 'proportionnellement',\n",
       " 'inconstitutionnelle',\n",
       " 'extraordinsirsksîit',\n",
       " 'internationalisatio',\n",
       " 'deffarépressiomellè',\n",
       " 'jjibredelamaisounéa',\n",
       " 'levéspsystématiques',\n",
       " 'individualisntrices',\n",
       " 'transsubstantiation',\n",
       " 'conventionuellement',\n",
       " 'belgescrcnlbelpiquô',\n",
       " 'systèmeperfectionné',\n",
       " 'professionnellement',\n",
       " 'maiutenantconsolidé',\n",
       " 'ssisieimmoeilicreen',\n",
       " 'antiluxembourgeoise',\n",
       " 'administrationjijtj']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#les mots les plus longs\n",
    "n = 150\n",
    "sorted(voc, key=len, reverse=True)[:n]\n",
    "\n",
    "#oh TERRIBLE OCR without the regex, not a single word, but a couple of conjuctions at the end \n",
    "\n",
    "#quand meme une grosse différence avec le regex pour enlever les mots avec des lettres répétées\n",
    "#peut être qu'on peut faire un truc plus fin pour garder les mots légitimes, c'est un peu dommage que c'est mots ne seront pas analysés dans l'état quelles sont mtn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e8806",
   "metadata": {},
   "source": [
    "### Keywords Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66651306",
   "metadata": {},
   "source": [
    "#### Extraire les keywords du texte sans stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32aa95",
   "metadata": {},
   "source": [
    "Statistical approach: the lower the score, the more relevant the keyword is.\n",
    "import yake\n",
    "\n",
    "text = \"xxx \"\n",
    "\n",
    "Simple usage with default parameters\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw, score in keywords:\n",
    "    print(f\"{kw} ({score})\")\n",
    "\n",
    "With custom parameters\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"en\",              # language\n",
    "    n=3,                   # ngram size\n",
    "    dedupLim=0.9,          # deduplication threshold\n",
    "    dedupFunc='seqm',      # deduplication function\n",
    "    windowsSize=1,         # context window\n",
    "    top=10,                # number of keywords to extract\n",
    "    features=None          # custom features\n",
    ")\n",
    "\n",
    "keywords = custom_kw_extractor.extract_keywords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a401d0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yake.yake.KeywordExtractor at 0x1425565eed0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50) #keeping 50 for now, but it could be more concise to do less per document \n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad7629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lister les fichiers, advantage is that we can see which files mention which keywords but we can also do the same with a bash eventually\n",
    "data_path = \"../../data/halley/halley_txt/\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15d606f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 551 files\n",
      "Total tokens across all files: 1584196\n",
      "551\n",
      "\n",
      "KB_JB1051_1927-08-25_01-00004.txt: 1677 tokens\n",
      "First 20 tokens: ['rouge', 'edison', 'revendique', 'priorité', 'invention', 'phonographe', 'signalé', 'dernièrement', 'mort', 'poète', 'savant', 'français', 'charles', 'croa', 'considéré', 'inventeur', 'phonographe', 'décrit', 'mécanisme', 'edison', 'appliquer', 'développer', 'invention', 'savant', 'fronçais', 'interrogé', 'point', 'edison', 'vient', 'donne', 'précisions', 'juillet', 'conçus', 'idée', 'pliu', 'recherche', 'pétroles', 'synthétiques', 'thomas', 'edison', 'grand', 'inventeur', 'américain', 'vient', 'fêter', 'anniversaire', 'nographe', 'août', 'appareil', 'construit', 'côté']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each file individually\n",
    "#I think it's best to first see all of they keywords per file before aggregating and then doing one big word cloud for the whole set, or just a certain period. I would've wanted to compare the wordcloud of the different periods ngl. I mean, at least I could do that for la libre belgique or whatever it was that was also reporting in 1835\n",
    "\n",
    "data_path = \"../../data/halley/halley_txt/\"\n",
    "tokenised_files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "\n",
    "tokens_by_file = {}\n",
    "\n",
    "for filename in tokenised_files:\n",
    "    filepath = os.path.join(data_path, filename)\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read() # Récupérer le texte du fichier sans le jointure \n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.wordpunct_tokenize(text) # Extraire les tokens du texte en cours\n",
    "    \n",
    "    # Filtrer\n",
    "    filtered_tokens = [w.lower() for w in tokens \n",
    "                      if len(w) > 2 # Ne garder que les bigrammes\n",
    "                      and w.isalpha() \n",
    "                      and w.lower() not in sw\n",
    "                      and not re.search(r'(.)\\1{2,}', w.lower())]\n",
    "    \n",
    "    tokens_by_file[filename] = filtered_tokens\n",
    "\n",
    "print(f\"Tokenized {len(tokens_by_file)} files\")\n",
    "print(f\"Total tokens across all files: {sum(len(tokens) for tokens in tokens_by_file.values())}\")\n",
    "print(len(tokenised_files)) #just for me to be sure that indeed they were all read\n",
    "\n",
    "# Show example for first file\n",
    "first_file = tokenised_files[0] # Limiter à 1 fichier pour l'exemple, need to do a bash of all? after checking stop words?\n",
    "print(f\"\\n{first_file}: {len(tokens_by_file[first_file])} tokens\")\n",
    "print(f\"First 20 tokens: {tokens_by_file[first_file][:20]}\") #helllll yeah she's alive and didn't take 15 years to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_JB1051_1927-08-25_01-00004.txt mentions these keywords: ...\n",
      "KB_JB1051_1937-11-14_01-00006.txt mentions these keywords: ...\n",
      "KB_JB1051_1939-08-06_01-00005.txt mentions these keywords: ...\n",
      "KB_JB1051_1957-04-18_01-00004.txt mentions these keywords: ...\n",
      "KB_JB1051_1957-10-14_01-00001.txt mentions these keywords: ...\n",
      "KB_JB421_1902-10-19_01-00001.txt mentions these keywords: ...\n",
      "KB_JB421_1907-10-26_01-00002.txt mentions these keywords: ...\n",
      "KB_JB421_1909-05-07_01-00003.txt mentions these keywords: ...\n",
      "KB_JB421_1909-08-06_01-00001.txt mentions these keywords: ...\n",
      "KB_JB421_1909-10-23_01-00001.txt mentions these keywords: ...\n"
     ]
    }
   ],
   "source": [
    "#start here\n",
    "for f in sorted(tokenised_files)[:10]:\n",
    "    tokenised_text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    token_keywords = kw_extractor.extract_keywords(tokenised_text)\n",
    "    kept_tokens = []\n",
    "    for kw, score in token_keywords:\n",
    "        token_words = kw.split()\n",
    "        if len(token_words) == 2:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept_tokens )}...\")\n",
    "\n",
    "#returns nothing :( but runs through the first 10 files :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137821d1",
   "metadata": {},
   "source": [
    "# FROM HERE DOWN WE'RE IN DRAFT MODE \n",
    "Tokenised all the files, now I'm working on kw extraction for each of the files so we can start comparing, maybe then only take the lowest vectors from the dataset and start making the word cloud from there (that way the keywords of EACH file are preserved, not just the bashed kw (actually wait it'd be interesting to see if they'd be different(I should include that in the analysis as the limits of NLP???)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ligne', 0.006576956516920981),\n",
       " ('grande ligne', 0.010414814932783081),\n",
       " ('jjjjgjs et Jugements', 0.012405281434844163),\n",
       " (\"d'un\", 0.012768895090072173),\n",
       " (\"qu'il\", 0.01404661186692881),\n",
       " ('rue', 0.018646918396135623),\n",
       " ('petite ligne', 0.022057795950928976),\n",
       " ('Belgique', 0.022894960513796105),\n",
       " ('Faits-drt ANNONCES', 0.02459954981037721),\n",
       " ('service', 0.02593999816391375),\n",
       " (\"qu'ils\", 0.032775427689500564),\n",
       " ('bons', 0.03291445281262834),\n",
       " ('faire', 0.03583407481960633),\n",
       " ('bons vieux serviteurs', 0.03631056971969405),\n",
       " ('TRAITE A FORFAIT', 0.03783214099594524),\n",
       " ('bon', 0.0384001949480664),\n",
       " ('prince', 0.039173282293782995),\n",
       " ('Albert', 0.041280351933826676),\n",
       " ('prince Albert', 0.04207603403127464),\n",
       " (\"c'est\", 0.042311261418854614),\n",
       " (\"d'une\", 0.044920043564026534),\n",
       " ('rue Clément XIV', 0.04628643594921162),\n",
       " (\"qu'il possède rue\", 0.05113952925236211),\n",
       " (\"l'on\", 0.053223547434741274),\n",
       " ('VIEUX', 0.054648719572990195),\n",
       " (\"service d'un\", 0.05596176939282363),\n",
       " (\"service d'un réel\", 0.06172615335668248),\n",
       " ('Faits-drt', 0.06344636920368098),\n",
       " ('Jugements', 0.06344636920368098),\n",
       " (\"qu'elle\", 0.06537325387176664),\n",
       " (\"maîtres qu'ils\", 0.06571725664248096),\n",
       " (\"qu'ils out Sru\", 0.06646114148363362),\n",
       " ('fut', 0.06771678728633244),\n",
       " (\"rue d'Argeni ixellea\", 0.06795878962731165),\n",
       " ('visiteurs', 0.0700874380461356),\n",
       " ('hommes', 0.07198343152781589),\n",
       " (\"maîtres qu'ils obtien\", 0.0732270707670453),\n",
       " (\"nom d'un\", 0.07344637932889118),\n",
       " ('rues', 0.07458767358454249),\n",
       " ('VIEUX DOMESTIQUES', 0.07481074021820261),\n",
       " ('vieux braves serviteurs', 0.0759595325253067),\n",
       " ('fois', 0.07617135785874536),\n",
       " (\"vieux hasselt qu'il\", 0.07677189715384393),\n",
       " ('maîtres', 0.07839309630159925),\n",
       " ('service général', 0.07841787651101065),\n",
       " ('Ferrer', 0.07912366810476722),\n",
       " ('bons vieux', 0.08172049304546637),\n",
       " ('Palais', 0.08261737850588843),\n",
       " ('comète', 0.08294105788268245),\n",
       " (\"Bruxelles D'un relevé\", 0.08302961467871471)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraire les mots clés du text bash \n",
    "\n",
    "keywords = kw_extractor.extract_keywords(text) #I think text here is the whole halley text\n",
    "#doesn't look like stopwords were removed here\n",
    "\n",
    "keywords "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb771d6",
   "metadata": {},
   "source": [
    "Keywords Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecc000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yF: LE DRAPEAU ROUGE Edison revendique la priorité de l'invention du phonographe .- On a signalé dernièrement la mort du poète ot'savant français Charles Croa considéré com- , mo l'inventeur du phonographe, dont il aurait décrit, le premier, le mécanisme. Edison n'aurait l'ait qu'appliquer et développer l'invention du savant fronçais. Interrogé sur ce point, Edison vient de donne! les précisions suivantes : « — Jo no puis vous dire que ceci : c'est en ■ juillet 1977 que je conçus l'idée de mon p\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choisir un fichier => actually I think I want this to do all of the files, if this is the analysis I'm thinking of but let's test as is for now\n",
    "#maybe I could do just the texts during may 1910? I could filter the files based on date either using the setup from preparer_halley or just split the names again\n",
    "\n",
    "this_file = files[0]\n",
    "this_file\n",
    "\n",
    "# Récupérer le texte du fichier\n",
    "text = open(os.path.join(data_path, this_file), 'r', encoding='utf-8').read()\n",
    "text[:500] #gets first 500 characters, Just to check the beginning of the text, I can change this later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ts6rq1nt5ck",
   "metadata": {},
   "source": [
    "### Extract Keywords from All Files with Time Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "k0mqw6guetm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_from_halley_texts(specific_years=None, specific_months=None, ngram_size=1):\n",
    "    \"\"\"\n",
    "    Extract keywords from Halley texts with optional time filtering.\n",
    "\n",
    "    Based on the structure from preparer_halley.ipynb for filename parsing\n",
    "    and filtering by time period. Processes files individually (not halley_all.txt)\n",
    "    to maintain document boundaries for YAKE and enable time-based filtering.\n",
    "\n",
    "    Parameters:\n",
    "    - specific_years: list of year strings, e.g., ['1910', '1911']\n",
    "    - specific_months: list of (year, month) tuples, e.g., [('1910', '05'), ('1910', '06')]\n",
    "    - ngram_size: int, controls n-gram filtering (1=unigrams, 2=bigrams, etc.)\n",
    "                  Change this parameter to easily switch between unigrams/bigrams/trigrams\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary: {filename: [(keyword, score), ...]}\n",
    "    \"\"\"\n",
    "    keywords_by_file = {}\n",
    "\n",
    "    # Iterate through all text files in the Halley directory\n",
    "    for f in sorted(files):\n",
    "        if \"_\" in f and f.endswith(\"txt\"):\n",
    "            # Parse filename to extract date metadata (same logic as preparer_halley.ipynb)\n",
    "            elems = f.split(\"_\")\n",
    "            year = elems[2].split(\"-\")[0]\n",
    "            month = elems[2].split(\"-\")[1]\n",
    "\n",
    "            # Apply time filters if specified\n",
    "            if specific_years and year not in specific_years:\n",
    "                continue\n",
    "            if specific_months and (year, month) not in specific_months:\n",
    "                continue\n",
    "\n",
    "            # Read the text file\n",
    "            text = open(os.path.join(data_path, f), 'r', encoding='utf-8').read()\n",
    "\n",
    "            # Extract keywords using YAKE\n",
    "            keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "            # Filter keywords by n-gram size (number of words in the keyword)\n",
    "            # To change from unigrams to bigrams/trigrams, just modify ngram_size parameter\n",
    "            kept = [(kw, score) for kw, score in keywords if len(kw.split()) == ngram_size]\n",
    "\n",
    "            # Store in dictionary organized by filename\n",
    "            keywords_by_file[f] = kept\n",
    "\n",
    "    return keywords_by_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "mog3i0hdll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords from 551 files\n"
     ]
    }
   ],
   "source": [
    "# Extract unigram keywords from all Halley texts, organized by file\n",
    "# To extract bigrams instead, change ngram_size=2\n",
    "keywords_by_file = extract_keywords_from_halley_texts(ngram_size=1)\n",
    "print(f\"Extracted keywords from {len(keywords_by_file)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "husjk9k2cwj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total keyword occurrences: 19655\n",
      "Unique keywords: 4842\n",
      "\n",
      "Top 20 most frequent keywords:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"qu'il\", 274),\n",
       " (\"d'une\", 265),\n",
       " (\"d'un\", 261),\n",
       " ('heures', 242),\n",
       " ('Bruxelles', 228),\n",
       " ('faire', 225),\n",
       " ('rue', 185),\n",
       " ('ans', 185),\n",
       " ('fut', 184),\n",
       " ('jour', 170),\n",
       " ('grand', 156),\n",
       " ('Paris', 153),\n",
       " (\"C'est\", 147),\n",
       " (\"c'est\", 145),\n",
       " ('Belgique', 138),\n",
       " ('francs', 138),\n",
       " ('temps', 132),\n",
       " ('jours', 132),\n",
       " ('lieu', 125),\n",
       " ('dit', 118)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten keywords from all files (keeping duplicates for frequency counting)\n",
    "all_keywords_flat = [kw for kws in keywords_by_file.values() for kw, score in kws]\n",
    "\n",
    "# Calculate frequency distribution\n",
    "keyword_freq = Counter(all_keywords_flat)\n",
    "\n",
    "# Display results\n",
    "print(f\"Total keyword occurrences: {len(all_keywords_flat)}\")\n",
    "print(f\"Unique keywords: {len(keyword_freq)}\")\n",
    "print(f\"\\nTop 20 most frequent keywords:\")\n",
    "keyword_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cmbmqdq8xxk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files from 1910: 242\n",
      "Files from May 1910: 98\n",
      "\n",
      "May 1910 top keywords: [('heures', 61), (\"d'un\", 60), ('Bruxelles', 58), (\"d'une\", 58), (\"qu'il\", 55), ('jour', 43), ('faire', 41), (\"C'est\", 36), ('rue', 35), ('ans', 34)]\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Keywords from 1910 only\n",
    "keywords_1910 = extract_keywords_from_halley_texts(specific_years=['1910'], ngram_size=1)\n",
    "print(f\"Files from 1910: {len(keywords_1910)}\")\n",
    "\n",
    "# Example 2: Keywords from May 1910 (peak Halley's passage)\n",
    "may_1910_keywords = extract_keywords_from_halley_texts(specific_months=[('1910', '05')], ngram_size=1)\n",
    "print(f\"Files from May 1910: {len(may_1910_keywords)}\")\n",
    "\n",
    "# Example 3: Compare May vs. all of 1910\n",
    "freq_may = Counter([kw for kws in may_1910_keywords.values() for kw, score in kws])\n",
    "print(f\"\\nMay 1910 top keywords: {freq_may.most_common(10)}\")\n",
    "\n",
    "#ok need to figure out why the stopwords lists aren't working here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gwinuarb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 3 files and their keywords to verify results\n",
    "for i, (filename, keywords) in enumerate(list(keywords_by_file.items())[:3]):\n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(f\"  Top 10 keywords: {keywords[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
