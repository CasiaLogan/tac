{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229a73ca",
   "metadata": {},
   "source": [
    "# Analyse de la dist. du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c004a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9734c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\casia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') #liste pré-définie de stopwords\n",
    "from nltk.corpus import stopwords #attention de bien nettoyer les stopwords !! sinon résultats bizarres\n",
    "\n",
    "import os\n",
    "import yake #(Yet Another Keyword Extractor) \"that uses text statistical features to select the most important keywords from a document\"\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud #besoin de compilateur c++ pour l'installer\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import re #pour les expressions régulières afin de nettoyer l OCRisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7441efd",
   "metadata": {},
   "source": [
    "### Gerer les stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6e5c",
   "metadata": {},
   "source": [
    "#### Créer une liste des stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "284e0c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 stopwords:\n",
      " ['-', ':', 'DRAPEAU', 'LE', 'ROUGE', 'a', 'abord', 'absolument', 'afin', 'ah', 'ai', 'aie', 'aient', 'aies', 'ailleurs', 'ainsi', 'ait', 'allaient', 'allo', 'allons', 'allô', 'alors', 'année', 'ans', 'anterieur', 'anterieure', 'anterieures', 'apres', 'après', 'as', 'assez', 'attendu', 'au', 'aucun', 'aucune', 'aucuns', 'aujourd', \"aujourd'hui\", 'aupres', 'auquel', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autant', 'autre', 'autrefois', 'autrement', 'autres', 'autrui', 'aux', 'auxquelles', 'auxquels', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayante', 'ayantes', 'ayants', 'ayez', 'ayons', 'bah', 'bas', 'basee', 'bat', 'beau', 'beaucoup', 'bien', 'bigre', 'bon', 'boum', 'bravo', 'c', 'car', 'ce', 'ceci', 'cela', 'celle', 'celle-ci', 'celle-là', 'celles', 'celles-ci', 'celles-là', 'celui', 'celui-ci', 'celui-là', 'celà', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'ces', 'cet', 'cette', 'ceux', 'ceux-ci', 'ceux-là', 'chacun', 'chacune', 'chaque', 'cher', 'chers', 'chez', 'chiche', 'chut', 'chère', 'chères', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantième', 'cinquième', 'clac', 'clic', 'com', 'combien', 'comme', 'comment', 'comparable', 'comparables', 'compris', 'concernant', 'contre', 'couic', 'crac', 'd', \"d'un\", \"d'une\", 'dans', 'de', 'debout', 'dedans', 'dehors', 'deja', 'delà', 'depuis', 'dernier', 'derniere', 'derriere', 'derrière', 'des', 'desormais', 'desquelles', 'desquels', 'dessous', 'dessus', 'deux', 'deuxième', 'deuxièmement', 'devant', 'devers', 'devra', 'devrait', 'different', 'differentes', 'differents', 'différent', 'différente', 'différentes', 'différents', 'dire', 'directe', 'directement', 'dit', 'dite', 'dits', 'divers', 'diverse', 'diverses', 'dix', 'dix-huit', 'dix-neuf', 'dix-sept', 'dixième', 'doit', 'doivent', 'donc', 'dont', 'dos', 'douze', 'douzième', 'drapeau', 'dring', 'droite', 'du', 'duquel', 'durant', 'dès', 'début', 'désormais', 'effet', 'egale', 'egalement', 'egales', 'eh', 'elle', 'elle-même', 'elles', 'elles-mêmes', 'en', 'encore', 'enfin', 'entre', 'envers', 'environ', 'es', 'essai', 'est', 'et', 'etant', 'etc', 'etre', 'eu', 'eue', 'eues', 'euh', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eux-mêmes', 'exactement', 'excepté', 'extenso', 'exterieur', 'eûmes', 'eût', 'eûtes', 'f', 'faire', 'fais', 'faisaient', 'faisant', 'fait', 'faites', 'faut', 'façon', 'feront', 'fi', 'flac', 'floc', 'fois', 'font', 'force', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'gens', 'haut', 'hein', 'hem', 'hep', 'het', 'holà', 'hop', 'hormis', 'hors', 'hou', 'houp', 'hue', 'hui', 'huit', 'huitième', 'hum', 'hurrah', 'hé', 'hélas', 'i', 'ici', 'il', 'ils', 'importe', 'j', 'je', 'jour', 'jusqu', 'jusque', 'juste', 'l', \"l'on\", 'la', 'laisser', 'laquelle', 'las', 'le', 'lequel', 'les', 'lesquelles', 'lesquels', 'leur', 'leurs', 'longtemps', 'lors', 'lorsque', 'lui', 'lui-meme', 'lui-même', 'là', 'lès', 'm', 'ma', 'maint', 'maintenant', 'mais', 'malgre', 'malgré', 'maximale', 'me', 'meme', 'memes', 'merci', 'mes', 'mien', 'mienne', 'miennes', 'miens', 'mille', 'mince', 'mine', 'minimale', 'moi', 'moi-meme', 'moi-même', 'moindres', 'moins', 'mon', 'mot', 'moyennant', 'multiple', 'multiples', 'même', 'mêmes', 'n', \"n'anaturel\", 'na', 'naturelle', 'naturelles', 'ne', 'neanmoins', 'necessaire', 'necessairement', 'neuf', 'neuvième', 'ni', 'nombreuses', 'nombreux', 'nommés', 'non', 'nos', 'notamment', 'notre', 'nous', 'nous-mêmes', 'nouveau', 'nouveaux', 'nul', 'néanmoins', 'nôtre', 'nôtres', 'oh', 'ohé', 'ollé', 'olé', 'on', 'ont', 'onze', 'onzième', 'ore', 'ou', 'ouf', 'ouias', 'oust', 'ouste', 'outre', 'ouvert', 'ouverte', 'ouverts', 'où', 'paf', 'par', 'parce', 'parfois', 'parle', 'parlent', 'parler', 'parmi', 'parole', 'parseme', 'partant', 'particulier', 'particulière', 'particulièrement', 'pas', 'passé', 'pendant', 'pense', 'permet', 'personne', 'personnes', 'peu', 'peut', 'peuvent', 'peux', 'pff', 'pfft', 'pfut', 'pif', 'pire', 'pièce', 'plein', 'plouf', 'plupart', 'plus', 'plusieurs', 'plutôt', 'possessif', 'possessifs', 'possible', 'possibles', 'pouah', 'pour', 'pourquoi', 'pourrais', 'pourrait', 'pouvait', 'prealable', 'precisement', 'premier', 'première', 'premièrement', 'pres', 'probable', 'probante', 'procedant', 'proche', 'près', 'psitt', 'pu', 'puis', 'puisque', 'pur', 'pure', 'qu', \"qu'elle\", \"qu'elles\", \"qu'il\", \"qu'ils\", 'quand', 'quant', 'quant-à-soi', 'quanta', 'quarante', 'quatorze', 'quatre', 'quatre-vingt', 'quatrième', 'quatrièmement', 'que', 'quel', 'quelconque', 'quelle', 'quelles', \"quelqu'un\", 'quelque', 'quelques', 'quels', 'qui', 'quiconque', 'quinze', 'quoi', 'quoique', 'r', 'rare', 'rarement', 'rares', 'relative', 'relativement', 'remarquable', 'rend', 'rendre', 'restant', 'reste', 'restent', 'restrictif', 'retour', 'revoici', 'revoilà', 'rien', 'rue', 's', 'sa', 'sacrebleu', 'sait', 'sans', 'sapristi', 'sauf', 'se', 'sein', 'seize', 'selon', 'semblable', 'semblaient', 'semble', 'semblent', 'sent', 'sept', 'septième', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seul', 'seule', 'seulement', 'si', 'sien', 'sienne', 'siennes', 'siens', 'sinon', 'six', 'sixième', 'soi', 'soi-même', 'soient', 'sois', 'soit', 'soixante', 'sommes', 'son', 'sont', 'sous', 'souvent', 'soyez', 'soyons', 'specifique', 'specifiques', 'speculatif', 'stop', 'strictement', 'subtiles', 'suffisant', 'suffisante', 'suffit', 'suis', 'suit', 'suivant', 'suivante', 'suivantes', 'suivants', 'suivre', 'sujet', 'superpose', 'sur', 'surtout', 't', 'ta', 'tac', 'tandis', 'tant', 'tardive', 'te', 'tel', 'telle', 'tellement', 'telles', 'tels', 'tenant', 'tend', 'tenir', 'tente', 'tes', 'tic', 'tien', 'tienne', 'tiennes', 'tiens', 'toc', 'toi', 'toi-même', 'ton', 'touchant', 'toujours', 'tous', 'tout', 'toute', 'toutefois', 'toutes', 'treize', 'trente', 'tres', 'trois', 'troisième', 'troisièmement', 'trop', 'très', 'tsoin', 'tsouin', 'tu', 'té', 'un', 'une', 'unes', 'uniformement', 'unique', 'uniques', 'uns', 'va', 'vais', 'valeur', 'van', 'vas', 'vers', 'via', 'vif', 'vifs', 'vingt', 'vivat', 'vive', 'vives', 'vlan', 'voici', 'voie', 'voient', 'voilà', 'voir', 'voire', 'vont', 'vos', 'votre', 'vous', 'vous-mêmes', 'vu', 'vé', 'vôtre', 'vôtres', 'y', 'zut', 'à', 'â', 'ça', 'ès', 'étaient', 'étais', 'était', 'étant', 'étante', 'étantes', 'étants', 'état', 'étiez', 'étions', 'été', 'étée', 'étées', 'étés', 'êtes', 'être', 'ô']\n"
     ]
    }
   ],
   "source": [
    "#jouer encore avec cela pour en ajouter des nouveaux en fonction du contexte, modifié de la liste de https://github.com/stopwords-iso/stopwords-fr?tab=readme-ov-file, à verifer sa fidelité\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \"elles\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \":\", \"même\", \"voir\", \"fois\", \"jour\", \"année\", \"ans\", \"faites\", \"le\", \"la\", \"de\"]\n",
    "\n",
    "# New stopwords to add\n",
    "git_stopwords = [\n",
    "    \"a\", \"abord\", \"absolument\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\", \"aies\", \"ailleurs\", \"ainsi\", \"ait\", \"allaient\", \"allo\", \"allons\", \"allô\", \"alors\", \"anterieur\", \"anterieure\", \"anterieures\", \"apres\", \"après\", \"as\", \"assez\", \"attendu\", \"au\", \"aucun\", \"aucune\", \"aucuns\", \"aujourd\", \"aujourd'hui\", \"aupres\", \"auquel\", \"aura\", \"aurai\", \"auraient\", \"aurais\", \"aurait\", \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autant\", \"autre\", \"autrefois\", \"autrement\", \"autres\", \"autrui\", \"aux\", \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\", \"avez\", \"aviez\", \"avions\", \"avoir\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"bah\", \"bas\", \"basee\", \"bat\", \"beau\", \"beaucoup\", \"bien\", \"bigre\", \"bon\", \"boum\", \"bravo\", \"car\", \"ce\", \"ceci\", \"cela\", \"celle\", \"celle-ci\", \"celle-là\", \"celles\", \"celles-ci\", \"celles-là\", \"celui\", \"celui-ci\", \"celui-là\", \"celà\", \"cent\", \"cependant\", \"certain\", \"certaine\", \"certaines\", \"certains\", \"certes\", \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\", \"chacun\", \"chacune\", \"chaque\", \"cher\", \"chers\", \"chez\", \"chiche\", \"chut\", \"chère\", \"chères\", \"ci\", \"cinq\", \"cinquantaine\", \"cinquante\", \"cinquantième\", \"cinquième\", \"clac\", \"clic\", \"combien\", \"comme\", \"comment\", \"comparable\", \"comparables\", \"compris\", \"concernant\", \"contre\", \"couic\", \"crac\", \"dans\", \"de\", \"debout\", \"dedans\", \"dehors\", \"deja\", \"delà\", \"depuis\", \"dernier\", \"derniere\", \"derriere\", \"derrière\", \"des\", \"desormais\", \"desquelles\", \"desquels\", \"dessous\", \"dessus\", \"deux\", \"deuxième\", \"deuxièmement\", \"devant\", \"devers\", \"devra\", \"devrait\", \"different\", \"differentes\", \"differents\", \"différent\", \"différente\", \"différentes\", \"différents\", \"dire\", \"directe\", \"directement\", \"dit\", \"dite\", \"dits\", \"divers\", \"diverse\", \"diverses\", \"dix\", \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dixième\", \"doit\", \"doivent\", \"donc\", \"dont\", \"dos\", \"douze\", \"douzième\", \"dring\", \"droite\", \"du\", \"duquel\", \"durant\", \"dès\", \"début\", \"désormais\", \"effet\", \"egale\", \"egalement\", \"egales\", \"eh\", \"elle\", \"elle-même\", \"elles\", \"elles-mêmes\", \"en\", \"encore\", \"enfin\", \"entre\", \"envers\", \"environ\", \"es\", \"essai\", \"est\", \"et\", \"etant\", \"etc\", \"etre\", \"eu\", \"eue\", \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\", \"eusses\", \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eux-mêmes\", \"exactement\", \"excepté\", \"extenso\", \"exterieur\", \"eûmes\", \"eût\", \"eûtes\", \"f\", \"fais\", \"faisaient\", \"faisant\", \"fait\", \"faites\", \"façon\", \"feront\", \"fi\", \"flac\", \"floc\", \"fois\", \"font\", \"force\", \"furent\", \"fus\", \"fusse\", \"fussent\", \"fusses\", \"fussiez\", \"fussions\", \"fut\", \"fûmes\", \"fût\", \"fûtes\", \"gens\", \"haut\", \"hein\", \"hem\", \"hep\", \"holà\", \"hop\", \"hormis\", \"hors\", \"hou\", \"houp\", \"hue\", \"hui\", \"huit\", \"huitième\", \"hum\", \"hurrah\", \"hé\", \"hélas\", \"i\", \"ici\", \"il\", \"ils\", \"importe\", \"j\", \"je\", \"jusqu\", \"jusque\", \"juste\", \"la\", \"laisser\", \"laquelle\", \"las\", \"le\", \"lequel\", \"les\", \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"longtemps\", \"lors\", \"lorsque\", \"lui\", \"lui-meme\", \"lui-même\", \"là\", \"lès\", \"ma\", \"maint\", \"maintenant\", \"mais\", \"malgre\", \"malgré\", \"maximale\", \"me\", \"meme\", \"memes\", \"merci\", \"mes\", \"mien\", \"mienne\", \"miennes\", \"miens\", \"mille\", \"mince\", \"mine\", \"minimale\", \"moi\", \"moi-meme\", \"moi-même\", \"moindres\", \"moins\", \"mon\", \"mot\", \"moyennant\", \"multiple\", \"multiples\", \"même\", \"mêmes\", \"na\", \"n'a\" \"naturel\", \"naturelle\", \"naturelles\", \"ne\", \"neanmoins\", \"necessaire\", \"necessairement\", \"neuf\", \"neuvième\", \"ni\", \"nombreuses\", \"nombreux\", \"nommés\", \"non\", \"nos\", \"notamment\", \"notre\", \"nous\", \"nous-mêmes\", \"nouveau\", \"nouveaux\", \"nul\", \"néanmoins\", \"nôtre\", \"nôtres\", \"oh\", \"ohé\", \"ollé\", \"olé\", \"on\", \"ont\", \"onze\", \"onzième\", \"ore\", \"ou\", \"ouf\", \"ouias\", \"oust\", \"ouste\", \"outre\", \"ouvert\", \"ouverte\", \"ouverts\", \"où\", \"paf\", \"par\", \"parce\", \"parfois\", \"parle\", \"parlent\", \"parler\", \"parmi\", \"parole\", \"parseme\", \"partant\", \"particulier\", \"particulière\", \"particulièrement\", \"pas\", \"passé\", \"pendant\", \"pense\", \"permet\", \"personne\", \"personnes\", \"peu\", \"peut\", \"peuvent\", \"peux\", \"pff\", \"pfft\", \"pfut\", \"pif\", \"pire\", \"pièce\", \"plein\", \"plouf\", \"plupart\", \"plus\", \"plusieurs\", \"plutôt\", \"possessif\", \"possessifs\", \"possible\", \"possibles\", \"pouah\", \"pour\", \"pourquoi\", \"pourrais\", \"pourrait\", \"pouvait\", \"prealable\", \"precisement\", \"premier\", \"première\", \"premièrement\", \"pres\", \"probable\", \"probante\", \"procedant\", \"proche\", \"près\", \"psitt\", \"pu\", \"puis\", \"puisque\", \"pur\", \"pure\", \"qu\", \"quand\", \"quant\", \"quant-à-soi\", \"quanta\", \"quarante\", \"quatorze\", \"quatre\", \"quatre-vingt\", \"quatrième\", \"quatrièmement\", \"que\", \"quel\", \"quelconque\", \"quelle\", \"quelles\", \"quelqu'un\", \"quelque\", \"quelques\", \"quels\", \"qui\", \"quiconque\", \"quinze\", \"quoi\", \"quoique\", \"r\", \"rare\", \"rarement\", \"rares\", \"relative\", \"relativement\", \"remarquable\", \"rend\", \"rendre\", \"restant\", \"reste\", \"restent\", \"restrictif\", \"retour\", \"revoici\", \"revoilà\", \"rien\", \"sa\", \"sacrebleu\", \"sait\", \"sans\", \"sapristi\", \"sauf\", \"se\", \"sein\", \"seize\", \"selon\", \"semblable\", \"semblaient\", \"semble\", \"semblent\", \"sent\", \"sept\", \"septième\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\", \"seriez\", \"serions\", \"serons\", \"seront\", \"ses\", \"seul\", \"seule\", \"seulement\", \"si\", \"sien\", \"sienne\", \"siennes\", \"siens\", \"sinon\", \"six\", \"sixième\", \"soi\", \"soi-même\", \"soient\", \"sois\", \"soit\", \"soixante\", \"sommes\", \"son\", \"sont\", \"sous\", \"souvent\", \"soyez\", \"soyons\", \"specifique\", \"specifiques\", \"speculatif\", \"stop\", \"strictement\", \"subtiles\", \"suffisant\", \"suffisante\", \"suffit\", \"suis\", \"suit\", \"suivant\", \"suivante\", \"suivantes\", \"suivants\", \"suivre\", \"sujet\", \"superpose\", \"sur\", \"surtout\", \"ta\", \"tac\", \"tandis\", \"tant\", \"tardive\", \"te\", \"tel\", \"telle\", \"tellement\", \"telles\", \"tels\", \"tenant\", \"tend\", \"tenir\", \"tente\", \"tes\", \"tic\", \"tien\", \"tienne\", \"tiennes\", \"tiens\", \"toc\", \"toi\", \"toi-même\", \"ton\", \"touchant\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\", \"treize\", \"trente\", \"tres\", \"trois\", \"troisième\", \"troisièmement\", \"trop\", \"très\", \"tsoin\", \"tsouin\", \"tu\", \"té\", \"un\", \"une\", \"unes\", \"uniformement\", \"unique\", \"uniques\", \"uns\", \"va\", \"vais\", \"valeur\", \"vas\", \"vers\", \"via\", \"vif\", \"vifs\", \"vingt\", \"vivat\", \"vive\", \"vives\", \"vlan\", \"voici\", \"voie\", \"voient\", \"voilà\", \"voire\", \"vont\", \"vos\", \"votre\", \"vous\", \"vous-mêmes\", \"vu\", \"vé\", \"vôtre\", \"vôtres\", \"zut\", \"à\", \"â\", \"ça\", \"ès\", \"étaient\", \"étais\", \"était\", \"étant\", \"état\", \"étiez\", \"étions\", \"été\", \"étée\", \"étées\", \"étés\", \"êtes\", \"être\", \"ô\"\n",
    "]\n",
    "\n",
    "#domaine specific \n",
    "specific_stopwords = [\n",
    "\"rue\", \"-\", \"drapeau\", \"DRAPEAU\", \"LE\", \"ROUGE\", \"com\", \"qu'il\", \"d'une\", \"d'un\", \"l'on\", \"qu'il\", \"qu'ils\", \"qu'elle\", \"qu'elles\", \"qu\", \"il\", \"elle\", \"c\", \"est\", \"elles\", \"ils\"\n",
    "#garder les noms des villes pour faire une analyse spatiale? \n",
    "#does it also include common first names?\n",
    "#does it also include uppercase versions?\n",
    "#à voir : soir, matin = seront pe liés au nom du journal et non du temps de la journée, define what a stopword means in this context\n",
    "]\n",
    "\n",
    "sw += git_stopwords\n",
    "sw += specific_stopwords\n",
    "sw = set(sw)\n",
    "\n",
    "\n",
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b9054",
   "metadata": {},
   "source": [
    "#### Tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbb7d6",
   "metadata": {},
   "source": [
    "Créer un fichier bash qui concatène tous les fichiers txt de halley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19aa408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4858920 words found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['yF',\n",
       " ':',\n",
       " 'LE',\n",
       " 'DRAPEAU',\n",
       " 'ROUGE',\n",
       " 'Edison',\n",
       " 'revendique',\n",
       " 'la',\n",
       " 'priorité',\n",
       " 'de',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'invention',\n",
       " 'du',\n",
       " 'phonographe',\n",
       " '.-',\n",
       " 'On',\n",
       " 'a',\n",
       " 'signalé',\n",
       " 'dernièrement',\n",
       " 'la',\n",
       " 'mort',\n",
       " 'du',\n",
       " 'poète',\n",
       " 'ot',\n",
       " \"'\",\n",
       " 'savant',\n",
       " 'français',\n",
       " 'Charles',\n",
       " 'Croa',\n",
       " 'considéré',\n",
       " 'com',\n",
       " '-',\n",
       " ',',\n",
       " 'mo',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'inventeur',\n",
       " 'du',\n",
       " 'phonographe',\n",
       " ',',\n",
       " 'dont',\n",
       " 'il',\n",
       " 'aurait',\n",
       " 'décrit',\n",
       " ',',\n",
       " 'le',\n",
       " 'premier',\n",
       " ',',\n",
       " 'le']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merci les profs\n",
    "path = \"../../data/halley/halley_txt/\" # Path to the directory containing text files\n",
    "\n",
    "with open(\"../../data/halley/halley_all.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                output_file.write(f.read())\n",
    "\n",
    "# Récupération du contenu du fichier bash\n",
    "path = \"../../data/halley/halley_all.txt\"\n",
    "limit = 10**8\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()[:limit]\n",
    "\n",
    "# Tokenization, sw pas encore appliqués\n",
    "words = nltk.wordpunct_tokenize(text) #tokenized words from the concatenated Halley corpus (halley_all.txt)\n",
    "print(f\"{len(words)} words found\")\n",
    "words[:50] \n",
    "\n",
    "# a big list of all of the keywords from all of the files, and then do a frequency analysis on that list to see what are the most common keywords across all of the texts = kinda loses all temporal analysis but ok for the overarching ALL keywords, \n",
    "# we should expect only the most common elements to come out? but key words isn't most common... careful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f336a",
   "metadata": {},
   "source": [
    "#### Eliminer les stopwords et les termes non alphabétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0a87720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584198 words kept (217992 different word forms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rouge',\n",
       " 'edison',\n",
       " 'revendique',\n",
       " 'priorité',\n",
       " 'invention',\n",
       " 'phonographe',\n",
       " 'signalé',\n",
       " 'dernièrement',\n",
       " 'mort',\n",
       " 'poète',\n",
       " 'savant',\n",
       " 'français',\n",
       " 'charles',\n",
       " 'croa',\n",
       " 'considéré',\n",
       " 'inventeur',\n",
       " 'phonographe',\n",
       " 'décrit',\n",
       " 'mécanisme',\n",
       " 'edison']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows words after filtering stop words, much better\n",
    "kept = [w.lower() for w in words \n",
    "        if len(w) > 2 \n",
    "        and w.isalpha() \n",
    "        and w.lower() not in sw\n",
    "        and not re.search(r'(.)\\1{2,}', w.lower())]\n",
    "\n",
    "voc = set(kept) #creates a unique vocabulary (no duplicates)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")\n",
    "kept[:20] \n",
    "\n",
    "#filtering iterations (notes)\n",
    "#1587064 words kept (220119 different word forms) \n",
    "#seems like a small difference but it's actually around 1500 words removed by the new stopword list\n",
    "\n",
    "#avec 5 caractères le même, 1586939 words kept (220004 different word forms) quand même une différence mais elle est assez minimale, dc environs 100 mots en moins \n",
    "#avec 3 carartères, 1586631 words kept (219739 different word forms)\n",
    "#avec 2, 1584198 words kept (217992 different word forms)\n",
    "\n",
    "\n",
    "#so these words are still the WHOLE page of ALL the pages isn't about halleys comet \n",
    "# do we go for an analysis of co-text as well then? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbce334",
   "metadata": {},
   "source": [
    "#### Calculer la taille du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfac870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heures', 4175),\n",
       " ('bruxelles', 3613),\n",
       " ('mai', 3111),\n",
       " ('grand', 3095),\n",
       " ('prix', 2688),\n",
       " ('temps', 2602),\n",
       " ('lieu', 2568),\n",
       " ('comète', 2556),\n",
       " ('francs', 2549),\n",
       " ('soir', 2445)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#récupérer la fréquence des mots sur toute le dataset \n",
    "fdist = nltk.FreqDist(kept) #yes, kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "fdist.most_common(10)\n",
    "\n",
    "#heures might be on the chopping block for sw \n",
    "\n",
    "#voisins plus proches to halley would be useful\n",
    "#or maybe we do a co-text analysis of words around halley?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f98710",
   "metadata": {},
   "source": [
    "#### Variation from structure of modules \n",
    "I don't see what the plot adds, so I'm leaving it out for now\n",
    "I'm also leaving out les mots qui n'apparaissent qu'une fois dans le corpus et les plus longs pour l'instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36610332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les mots les plus longs\n",
    "n = 150\n",
    "sorted(voc, key=len, reverse=True)[:n]\n",
    "\n",
    "#oh TERRIBLE OCR without the regex, not a single word, but a couple of conjuctions at the end \n",
    "\n",
    "#quand meme une grosse différence avec le regex pour enlever les mots avec des lettres répétées\n",
    "#peut être qu'on peut faire un truc plus fin pour garder les mots légitimes, c'est un peu dommage que c'est mots ne seront pas analysés dans l'état quelles sont mtn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e8806",
   "metadata": {},
   "source": [
    "### Keywords Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66651306",
   "metadata": {},
   "source": [
    "#### Extraire les keywords du texte sans stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a401d0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yake.yake.KeywordExtractor at 0x1425565eed0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50) #keeping 50 for now, but it could be more concise to do less per document \n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2ad7629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lister les fichiers, advantage is that we can see which files mention which keywords but we can also do the same with a bash eventually\n",
    "data_path = \"../../data/halley/halley_txt/\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80f03b",
   "metadata": {},
   "source": [
    "## Combiner les étapes d'avant afin d'analyser tout les fichiers à la fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d606f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 551 files\n",
      "Total tokens across all files: 1584196\n",
      "551\n",
      "\n",
      "KB_JB1051_1927-08-25_01-00004.txt: 1677 tokens\n",
      "First 20 tokens: ['rouge', 'edison', 'revendique', 'priorité', 'invention', 'phonographe', 'signalé', 'dernièrement', 'mort', 'poète', 'savant', 'français', 'charles', 'croa', 'considéré', 'inventeur', 'phonographe', 'décrit', 'mécanisme', 'edison']\n",
      "\n",
      "KB_JB1051_1937-11-14_01-00006.txt: 1305 tokens\n",
      "First 20 tokens: ['péchons', 'baleine', 'mini', 'îuii', 'iin', 'iriririnur', 'rateiinier', 'soviétiques', 'baleine', 'pèse', 'tonnes', 'bord', 'bateau', 'immédiatement', 'dépecée', 'préparée', 'etoiles', 'filantes', 'novembre', 'temps']\n",
      "\n",
      "KB_JB1051_1939-08-06_01-00005.txt: 2164 tokens\n",
      "First 20 tokens: ['page', 'famille', 'snfants', 'chronique', 'medicale', 'enfants', 'lacher', 'monstre', 'ballonnets', 'naangerca', 'excel', 'lento', 'crème', 'glace', 'vou', 'coin', 'amuser', 'mickcy', 'ravi', 'revoir']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize each file individually\n",
    "#I think it's best to first see all of they keywords per file before aggregating and then doing one big word cloud for the whole set, or just a certain period. I would've wanted to compare the wordcloud of the different periods ngl. I mean, at least I could do that for la libre belgique or whatever it was that was also reporting in 1835\n",
    "\n",
    "data_path = \"../../data/halley/halley_txt/\"\n",
    "tokenised_files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "\n",
    "tokens_by_file = {}\n",
    "\n",
    "for filename in tokenised_files:\n",
    "    filepath = os.path.join(data_path, filename)\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read() # Récupérer le texte du fichier individualement \n",
    "\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.wordpunct_tokenize(text) # Extraire les tokens du texte en cours\n",
    "    \n",
    "    # Filtrer\n",
    "    filtered_tokens = [w.lower() for w in tokens \n",
    "                      if len(w) > 2 # Ne garder que les bigrammes\n",
    "                      and w.isalpha() \n",
    "                      and w.lower() not in sw #il est censé enlevé les stopwords, mais \"c'est\" et \"qu'il\" apparait tjrs?? \n",
    "                      and not re.search(r'(.)\\1{2,}', w.lower())]\n",
    "    \n",
    "    tokens_by_file[filename] = filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97983ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this is just for my own interpretability so I know the output is good \n",
    "print(f\"Tokenized {len(tokens_by_file)} files\")\n",
    "print(f\"Total tokens across all files: {sum(len(tokens) for tokens in tokens_by_file.values())}\")\n",
    "print(len(tokenised_files)) #just for me to be sure that indeed they were all read\n",
    "\n",
    "# Show example for first files\n",
    "first_file = tokenised_files[0] # Limiter à 1 fichier pour l'exemple, need to do a bash of all? after checking stop words?\n",
    "second_file = tokenised_files[1]\n",
    "third_file = tokenised_files[2]\n",
    "print(f\"\\n{first_file}: {len(tokens_by_file[first_file])} tokens\")\n",
    "print(f\"First 20 tokens: {tokens_by_file[first_file][:20]}\") \n",
    "\n",
    "print(f\"\\n{second_file}: {len(tokens_by_file[second_file])} tokens\")\n",
    "print(f\"First 20 tokens: {tokens_by_file[second_file][:20]}\") \n",
    "\n",
    "print(f\"\\n{third_file}: {len(tokens_by_file[third_file])} tokens\")\n",
    "print(f\"First 20 tokens: {tokens_by_file[third_file][:20]}\") #ok I had to check that I was actually getting tokens by file, and so far it looks good even if snfants is in there lol "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4282080",
   "metadata": {},
   "source": [
    "#### Keyword extractor\n",
    "Statistical approach: the lower the score, the more relevant the keyword is.\n",
    "import yake\n",
    "\n",
    "##### Custom parameters\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"en\",              # language\n",
    "    n=3,                   # ngram size\n",
    "    dedupLim=0.9,          # deduplication threshold\n",
    "    dedupFunc='seqm',      # deduplication function\n",
    "    windowsSize=1,         # context window\n",
    "    top=10,                # number of keywords to extract\n",
    "    features=None          # custom features\n",
    ")\n",
    "\n",
    "keywords = custom_kw_extractor.extract_keywords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd967ea",
   "metadata": {},
   "source": [
    "Start here: review the kw code below to make sure it's clean and doing what i expect it to before moving on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa664052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_JB1051_1927-08-25_01-00004.txt mentions these bigrams: août, heures, rayon, local, jeudi, étoiles, soir, filantes, habituel, tomes, federation, grand, fer, chant, vendredi, ghetto, terre, académie, samedi, dimanche, orateur, fragments, edison, phonographe, vient, accu, météorites, cliant, sciences, charles, maison, gaz, canot, cellule, piano, petits, comètes, bornes, recherches, rouge, obtenir, listes, sol, rêveurs, tome, cros, poussière, petit, comète, pétroles...\n",
      "KB_JB1051_1937-11-14_01-00006.txt mentions these bigrams: étoiles, record, filantes, baleine, soviétique, temps, nage, mondial, nageur, brasse, mètres, boïtchenko, pigeons, union, point, comète, comètes, monde, natation, concours, soleil, kilomètres, grit, novembre, mer, grand, épreuve, amateurs, essaim, jean, aleoute, pêche, voyages, bateau, terre, trouvent, produits, également, sportif, style, village, rivière, flotte, moscou, pigeonnier, visibles, grande, sport, capitaine, sarva...\n",
      "KB_JB1051_1939-08-06_01-00005.txt mentions these bigrams: qualité, charles, froment, prix, grandet, eugénie, moyen, amour, août, gale, traitement, bonne, mieux, pavillon, tourteaux, bruxelles, garçon, seconde, avoine, commerciale, terre, pays, moscou, tête, vie, jamais, moment, boutons, gros, cousin, voix, semaine, peuple, drap, tirlemont, regard, déjà, oes, petit, tra, alexeevskoe, partie, exposés, chaussée, exposition, ment, oui, robe, chevaux, cousine...\n",
      "KB_JB1051_1957-04-18_01-00004.txt mentions these bigrams: duc, monseigneur, monsieur, ment, poulain, mort, comète, oui, arend, questions, roi, avril, congrès, chine, fédéral, fit, pilule, agit, cas, heures, roland, point, carthy, francs, bruxelles, saint, epemon, seigneur, écus, cruciales, paris, appareil, crime, belgique, français, visible, con, aui, libre, pourra, unité, action, petit, jeudi, duchesse, symph, parti, variations, service, américain...\n",
      "KB_JB1051_1957-10-14_01-00001.txt mentions these bigrams: congo, octobre, presse, communiste, ment, jacquemotte, bruxelles, terre, heures, ministre, fusée, billets, berlin, kanze, part, maison, capitalistes, hier, travail, déjà, également, tre, samedi, pays, matin, guerre, question, con, nom, spoutnik, monument, ekatou, hommage, harmonie, soviétique, satellite, mineur, prise, vie, parti, ouest, section, autour, moulin, ouvrière, banque, marks, belgique, moment, travailleurs...\n",
      "KB_JB421_1902-10-19_01-00001.txt mentions these bigrams: ligne, mort, page, juges, paix, belgique, boers, hier, ville, comète, frassem, bruxelles, journal, paris, zola, roi, coup, petite, grande, généraux, henry, nouvelle, eglise, luxembourg, monde, charité, compatriotes, fort, création, amis, frais, vient, avenir, politique, grand, don, joue, train, village, ecole, union, haras, francs, cheminé, filles, forge, jeunes, billets, abonnement, rédaction...\n",
      "KB_JB421_1907-10-26_01-00002.txt mentions these bigrams: rubens, art, grand, peintre, los, religieux, quo, temps, pays, religion, new, york, peinture, renaissance, uno, artistes, haute, chrétien, grande, maison, vient, général, société, fin, grands, maître, nouvelle, décadence, belgique, anvers, trouve, femmes, service, cas, comète, colonel, américains, heure, heures, reine, musiciens, propreté, cascade, armée, chef, enfant, vit, soleil, secours, christ...\n",
      "KB_JB421_1909-05-07_01-00003.txt mentions these bigrams: matin, arlon, police, enfant, temps, point, grande, jeune, soleil, lundi, petit, commissaire, heures, pris, nuit, saint, midi, grand, village, agent, vaucoret, marche, suite, hier, passion, avril, gouvernement, mort, voiture, place, comète, catholique, bourgmestre, paysan, coup, eau, mardi, fer, enquête, belgique, amateurs, gare, pauvre, epinettes, parti, câble, servin, simone, marie, bois...\n",
      "KB_JB421_1909-08-06_01-00001.txt mentions these bigrams: cologne, marks, ministre, congrès, francs, affaires, cardinal, nom, traitement, monde, cathédrale, temps, étrangères, demi, vue, heures, roi, albert, général, grand, belgique, salésiens, chancelier, annuités, service, vicinaux, ville, tasses, société, pension, arrivée, eau, cardinaux, président, également, déjà, nombre, bruxelles, empire, conseil, budget, nouvelles, prince, guerre, frais, centimes, additionnels, grande, catholiques, dom...\n",
      "KB_JB421_1909-10-23_01-00001.txt mentions these bigrams: belgique, service, prince, ligne, visiteurs, vieux, albert, hommes, fer, projet, comète, grand, jamais, ville, ferrer, devient, santé, adjoint, grande, publique, terre, palais, général, loi, gouvernement, duc, consul, pays, maison, peuple, dôme, contingent, chemins, consulat, vice, septembre, simple, nom, noms, maîtres, clément, justice, domestiques, conseil, petite, garanties, luxembourg, serviteurs, gaspard, modèle...\n"
     ]
    }
   ],
   "source": [
    "# Unigram keyword extractor with stopwords\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"fr\", \n",
    "    top=50,\n",
    "    stopwords=sw,   # Still helps YAKE's internal scoring\n",
    "    n=1 #yake defaults to 3 ngrams \n",
    ")\n",
    "\n",
    "n1_token_keywords = {}\n",
    "\n",
    "for filename in sorted(tokens_by_file.keys())[:10]:\n",
    "    filtered_tokens = tokens_by_file[filename] # Get pre-filtered tokens (already has stopwords, length, OCR filters)\n",
    "    \n",
    "    \n",
    "    text_from_tokens = ' '.join(filtered_tokens) # Convert token list back to text for YAKE\n",
    "    \n",
    "    \n",
    "    token_keywords = kw_extractor.extract_keywords(text_from_tokens) # Extract keywords from filtered text\n",
    "    all_token_keywords[filename] = token_keywords\n",
    "    \n",
    "    \n",
    "    kept_tokens = [] # Filter by n-gram size (change the number to get different sizes)\n",
    "    for kw, score in token_keywords:\n",
    "        token_words = kw.split()\n",
    "        if len(token_words) == 1:  # Change to 1 for unigrams, 3 for trigrams\n",
    "            kept_tokens.append(kw)\n",
    "    \n",
    "    print(f\"{filename} mentions these bigrams: {', '.join(kept_tokens)}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b960bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_JB1051_1927-08-25_01-00004.txt mentions these bigrams: août heures, jeudi août, local habituel, vendredi août, heures local, étoiles filantes, août soir, samedi août, dimanche août, académie sciences, rêveurs ghetto, ghetto tome, soir local, août maison, heures soir, samedi heures, charles cros, heures rayon, heures matin, habituel cellule, tome rêveurs, orateur jacquemotte, piano chant, orateur overstraeten, maison peuple, federation charleroi, gaz carbonique, canot insubmersible, jacquemotte député, priés rentrer, révolutionnaire russe, oxyde carbone, comite federal, dimanche heures, heures maison, août appareil, août rencontre, ciel août, août comité, matin local, habituel federation, nographe août, précisément août, août ouvrent, udi août, août jocal, août cal, août uic...\n",
      "KB_JB1051_1937-11-14_01-00006.txt mentions these bigrams: étoiles filantes, record mondial, capitaine sarva, union soviétique, nage brasse, essaim étoiles, asie centrale, école usine, améliorer record, canon harpons, armée rouge, jean blume, nage libre, distance soleil, altitude mètres, mètres brasse, bord aleoute, grit gravier, kilomètres heure, point radiant, meilleurs nageurs, temps clair, orbite comète, radiant étoiles, pêche baleine, seconde étoiles, pluies étoiles, origine étoiles, fiantes étoiles, lncandescfnts étoiles, étoiles riant, olympiade ouvrière, ouvrière anvers, olympiades ouvrières, mondial vitesse, mondial officiel, mondial invraisemblable, mondial reconnaissent, mondial carionnet, mondial spécialité, record union, filantes novemore, filantes eourt, filantes appelons, filantes présentent, filantes petits, filantes appartenant, union mètres, comptée meilleur, libre meilleurs...\n",
      "KB_JB1051_1939-08-06_01-00005.txt mentions these bigrams: bonne qualité, qualité commerciale, froment bonne, prix moyen, tirlemont thuin, indig froment, voix peuple, seconde qualité, qualité avoine, acren eghezée, eghezée fleurus, froment quai, brodés plumetis, expo prix, drap noir, étoiles filantes, kilo pied, gares belges, foire porcs, ville nouvelle, tourteaux lin, jetant regard, sarcopte gale, publicom bruxelles, traitement réussit, ler août, pavillon mécanisation, père grandet, poids qualité, qualité lea, répondit charles, commerciale froment, soignivs tirlemont, commerciale avoine, commerciale promeut, commerciale ment, moyen kilo, vêtements draps, thuin pailles, rencontre ami, fleurus huy, pied foire, thuin verviers, thuin aliments, beiges indig, disp indig, igne eghezée, eghezée jodoi, froment seconde, annon voix...\n",
      "KB_JB1051_1957-04-18_01-00004.txt mentions these bigrams: arend roland, monsieur duc, comète arend, hong kong, bém maj, congrès fédéral, questions cruciales, appa reil, fédéral bruxellois, extrême orient, othello ministère, contient transistor, toux asthme, men songe, uccle comète, maison blanche, nouvel accord, françoise sagan, hôpital anciens, maurice moppès, jeune romancière, demandent agit, cas ivresse, visible belgique, pilule américaine, réactions questions, mise point, duc henri, mort antibes, heure avril, découverte arend, appa reils, arend lgnd, cle arend, roland visible, roland clichés, roland donnèrent, roland brillante, cruciales réactions, cruciales question, alpinistes hollandais, romancière françoise, ivresse toux, orient maison, obligation série, série obligations, antibes maurice, américaine demandent, reils fin, kong déjà...\n",
      "KB_JB1051_1957-10-14_01-00001.txt mentions these bigrams: maison presse, monument mineur, kanze ekatou, harmonie ouvrière, presse communiste, billets banque, apposée maison, demandons représentants, représentants syndicaux, centre atomique, fraternel hommage, double pécule, hollande australie, brabant wallon, bois cazier, bur nelle, hommage rendu, change berlin, revendications ouvrieres, coup sévère, gaston moulin, berlin ouest, coût vie, bruxelles ville, atomique soviétique, gros capitalistes, marks orientaux, autour terre, section bruxelles, parti communiste, terre hier, joseph jacquemotte, fusée porteuse, notam ment, congo lais, propriétaires maison, réunissant maison, nelle gaston, berlin notam, engins atomiques, atomiques thermo, hommage section, presse ouvrière, orientaux berlin, bruxelles fraternel, mathieu ekatou, concours harmonie, pionniers harmonie, serrés monument, flcurir monument...\n",
      "KB_JB421_1902-10-19_01-00001.txt mentions these bigrams: juges paix, ligne page, jeunes filles, henry forge, nouvelle eglise, petite ligne, grande ligne, mort cheminé, généraux boers, paix demain, unioa juges, paix célébrera, hommage juges, paix réunissent, fjfs rjo, paix mensuellement, aïijti abonnement, page petite, page grande, filles esterai, cheminé fax, forge lelles, page œnt, page oen, page esml, forge grève, jolie page, journal juges, œnt grande, vaincus grande, crucis généraux, gueule généraux, généraux assourdis, rené henry, henry iim, maçonuerie billet, ensanglanté nouvelle, nouvelle affaire, condorcetï généraux, petite folle, ement nouvelle, proceranfc billets, billets dépôi, souscription nouvelle, eglise gnîfique, eglise situation, imposant billets, raison jeune, jeune hommage, avoué ligne...\n",
      "KB_JB421_1907-10-26_01-00002.txt mentions these bigrams: new york, art chrétien, haute société, grand peintre, société américaine, maître hôtel, fra angelico, shake hand, edouard vii, haute cascade, instincts propreté, bijoux reine, religion armée, art religieux, rubens grand, york san, vanderbilt new, york bustes, etat new, york comète, york engagea, muoieipalïté new, york itaîiâos, ville new, société escalier, exposé société, société allemande, enfants prodigues, enfant prodigue, accorder société, chefs œuvre, sorte maître, hôtel romancier, roi edouard, ies bijoux, prodigues quittèrent, visage instincts, chrétien inspira, chrétiens âme, idéal chrétien, soldats chrétiens, reine basiliques, célèbres musiciens, œuvres raphaël, rapport propreté, maître padoue, imagination maître, maître froid, mystérieux maître, roman haute...\n",
      "KB_JB421_1909-05-07_01-00003.txt mentions these bigrams: commissaire police, jeune vaucoret, coston commissaire, police quartier, quartier epinettes, catholique belge, jos isaye, lundi matin, passage chatelet, petit garçon, eau bouillante, gare hasselt, exemple belgique, amateurs cerises, enquête établi, fil fer, pauvre petit, parti catholique, rapproché soleil, village erkner, agent donies, point orbite, mardi midi, saints glace, jeudi matin, catholiques belges, secousse sismique, secousses sismiques, orbite rapproché, décision commissaire, vitriol commissaire, commissaire atteint, belge connue, belges frontières, chronique régionale, isaye concorda, mazurka jos, isaye mélange, glace craint, bouillante mère, violent secousse, sismique sentir, forio secousses, sismiques accompagnées, chatelet établie, chatelet accompagné, orbite éloigné, garçon pqijj, erkner ban, erkner paye...\n",
      "KB_JB421_1909-08-06_01-00001.txt mentions these bigrams: affaires étrangères, demi tasses, ministre guerre, centimes additionnels, prince albert, marks frais, ministre affaires, commission spéciale, hellebaut ministre, général hellebaut, nom commission, spéciale vicinaux, classe ouvrière, voudront aider, chemins fer, dom bosco, moyen age, conseiller légation, prolonger séances, solder annuités, service général, secrétaire etat, fonds spécial, porte nom, marks traitement, traitement marks, cardinal fischer, secrétaire affaires, etat affaires, général ministre, gagnants lots, gagnant lot, novembre général, carnets demi, tasse paie, droit demi, tasses prix, étrangères qualité, étrangères émargera, tasses application, allemagne demi, percevront demi, tasses ser, étrangères chambre, étrangères purement, rendus palais, nommé ministre, demande ministre, situation ministre, guerre visité...\n",
      "KB_JB421_1909-10-23_01-00001.txt mentions these bigrams: chemins fer, vice consul, santé duc, consul adjoint, ecclésiastiques inscrites, duc karl, adjoint consulat, inscrites loi, septembre visiteurs, jules poncin, bureaux télégraphiques, lieutenant rendolet, lésions internes, clément xiv, visiteurs ascension, conseil communal, pommes terre, hygiène publique, palais justice, grande ligne, comète halley, projet gouvernement, consulat belgique, vieux domestiques, vrais noms, service général, prince albert, immunités ecclésiastiques, duc charles, consul pékin, consulat général, appliquer chemins, fer etat, administration chemins, fer lea, commis chemins, fer continuent, accident chemin, rendolet lieutenant, internes lésions, septembre comète, attagé consulat, nommé vice, bioiley vice, biervliet vice, observée septembre, fer annonce, propos santé, gand adjoint, nouvelles santé...\n"
     ]
    }
   ],
   "source": [
    "# Bigram keyword extractor with stopwords\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"fr\", \n",
    "    top=50,\n",
    "    stopwords=sw, \n",
    "    n = 2\n",
    ")\n",
    "\n",
    "n2_token_keywords = {}\n",
    "\n",
    "for filename in sorted(tokens_by_file.keys())[:10]:\n",
    "    # Use pre-filtered tokens (already has stopwords, length, OCR filters removed)\n",
    "    filtered_tokens = tokens_by_file[filename]\n",
    "    \n",
    "    \n",
    "    text_from_tokens = ' '.join(filtered_tokens) # Convert token list back to text for YAKE\n",
    "    \n",
    "    \n",
    "    token_keywords = kw_extractor.extract_keywords(text_from_tokens) # Extract keywords from filtered text\n",
    "    n2_token_keywords[filename] = token_keywords\n",
    "    \n",
    "    # Filter by n-gram size\n",
    "    kept_tokens = []\n",
    "    for kw, score in token_keywords:\n",
    "        token_words = kw.split()\n",
    "        if len(token_words) == 2:\n",
    "            kept_tokens.append(kw)\n",
    "    \n",
    "    print(f\"{filename} mentions these bigrams: {', '.join(kept_tokens)}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1e98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KB_JB1051_1927-08-25_01-00004.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('août heures local', 1.7653326018916262e-05), ('heures local habituel', 2.2456262165831175e-05), ('dimanche août heures', 2.5478943673289053e-05), ('vendredi août heures', 3.4054200134340086e-05), ('août heures matin', 4.460268299594382e-05)]\n",
      "    'août heures local' has 3 words\n",
      "    'heures local habituel' has 3 words\n",
      "    'dimanche août heures' has 3 words\n",
      "    'vendredi août heures' has 3 words\n",
      "    'août heures matin' has 3 words\n",
      "    'rêveurs ghetto tome' has 3 words\n",
      "    'local habituel cellule' has 3 words\n",
      "    'août maison peuple' has 3 words\n",
      "    'jeudi août heures' has 3 words\n",
      "    'août soir local' has 3 words\n",
      "    'ghetto tome rêveurs' has 3 words\n",
      "    'tome rêveurs ghetto' has 3 words\n",
      "    'samedi août heures' has 3 words\n",
      "    'août heures maison' has 3 words\n",
      "    'jeudi août soir' has 3 words\n",
      "    'samedi heures local' has 3 words\n",
      "    'soir local habituel' has 3 words\n",
      "    'vendredi août soir' has 3 words\n",
      "    'août heures rayon' has 3 words\n",
      "    'août heures assemblée' has 3 words\n",
      "    'samedi août soir' has 3 words\n",
      "    'cellule jeudi août' has 3 words\n",
      "    'août heures ruo' has 3 words\n",
      "    'vendredi août maison' has 3 words\n",
      "    'local habituel federation' has 3 words\n",
      "    'heures matin local' has 3 words\n",
      "    'rayon jeudi août' has 3 words\n",
      "    'cellule vendredi août' has 3 words\n",
      "    'udi août heures' has 3 words\n",
      "    'lundi août heures' has 3 words\n",
      "    'août heures désiré' has 3 words\n",
      "    'août heures macsschalk' has 3 words\n",
      "    'samedi heures soir' has 3 words\n",
      "    'local habituel rayon' has 3 words\n",
      "    'cellule samedi août' has 3 words\n",
      "    'rayon vendredi août' has 3 words\n",
      "    'orateur jacquemotte député' has 3 words\n",
      "    'matin local habituel' has 3 words\n",
      "    'local habituel comite' has 3 words\n",
      "    'local habituel camarades' has 3 words\n",
      "    'marchienne jeudi août' has 3 words\n",
      "    'programme jeudi août' has 3 words\n",
      "    'heures local lii' has 3 words\n",
      "    'heures soir federation' has 3 words\n",
      "    'pont vendredi août' has 3 words\n",
      "    'août soir comité' has 3 words\n",
      "    'hornu jeudi août' has 3 words\n",
      "    'namur jeudi août' has 3 words\n",
      "    'jcamps jeudi août' has 3 words\n",
      "    'jeudi août jocal' has 3 words\n",
      "  Bigrams found: []\n",
      "\n",
      "KB_JB1051_1937-11-14_01-00006.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('essaim étoiles filantes', 0.0001425155208387245), ('seconde étoiles filantes', 0.000285031041677449), ('pluies étoiles filantes', 0.0003010563483907973), ('étoiles filantes novemore', 0.0003050105875076482), ('étoiles filantes eourt', 0.0003050105875076482)]\n",
      "    'essaim étoiles filantes' has 3 words\n",
      "    'seconde étoiles filantes' has 3 words\n",
      "    'pluies étoiles filantes' has 3 words\n",
      "    'étoiles filantes novemore' has 3 words\n",
      "    'étoiles filantes eourt' has 3 words\n",
      "    'origine étoiles filantes' has 3 words\n",
      "    'étoiles filantes appelons' has 3 words\n",
      "    'fiantes étoiles filantes' has 3 words\n",
      "    'étoiles filantes présentent' has 3 words\n",
      "    'étoiles filantes petits' has 3 words\n",
      "    'lncandescfnts étoiles filantes' has 3 words\n",
      "    'étoiles filantes appartenant' has 3 words\n",
      "    'améliorer record mondial' has 3 words\n",
      "    'comptée record mondial' has 3 words\n",
      "    'radiant étoiles riant' has 3 words\n",
      "    'olympiade ouvrière anvers' has 3 words\n",
      "    'record mondial vitesse' has 3 words\n",
      "    'point radiant étoiles' has 3 words\n",
      "    'naissance essaim étoiles' has 3 words\n",
      "    'atteint seconde étoiles' has 3 words\n",
      "    'établi record mondial' has 3 words\n",
      "    'battre record mondial' has 3 words\n",
      "    'record mondial officiel' has 3 words\n",
      "    'record mondial invraisemblable' has 3 words\n",
      "    'record mondial reconnaissent' has 3 words\n",
      "    'record mondial carionnet' has 3 words\n",
      "    'record mondial spécialité' has 3 words\n",
      "    'visible essaim étoiles' has 3 words\n",
      "    'tournes capitaine sarva' has 3 words\n",
      "    'capitaine sarva commande' has 3 words\n",
      "    'capitaine sarva dépassé' has 3 words\n",
      "    'mmfp capitaine sarva' has 3 words\n",
      "    'capitaine sarva auprès' has 3 words\n",
      "    'filantes eourt orbite' has 3 words\n",
      "    'étoiles riant observe' has 3 words\n",
      "    'abondantes pluies étoiles' has 3 words\n",
      "    'vraisemblablement origine étoiles' has 3 words\n",
      "    'cam fiantes étoiles' has 3 words\n",
      "    'atmosdhère lncandescfnts étoiles' has 3 words\n",
      "    'ouvrière anvers couvert' has 3 words\n",
      "    'filantes appartenant essaim' has 3 words\n",
      "    'record union mètres' has 3 words\n",
      "    'mals olympiades ouvrières' has 3 words\n",
      "    'patrie olympiade ouvrière' has 3 words\n",
      "    'ouvrières anvers minutes' has 3 words\n",
      "    'mondial invraisemblable puisse' has 3 words\n",
      "    'mondial vitesse avîon' has 3 words\n",
      "    'mondial officiel ioo' has 3 words\n",
      "    'mondial reconnaissent catégoriquement' has 3 words\n",
      "    'mondial carionnet doute' has 3 words\n",
      "  Bigrams found: []\n",
      "\n",
      "KB_JB1051_1939-08-06_01-00005.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('bonne qualité commerciale', 3.60237448457627e-05), ('froment bonne qualité', 5.41141864482518e-05), ('seconde qualité avoine', 0.00013895131826567987), ('indig froment quai', 0.00016005004359262876), ('acren eghezée fleurus', 0.00016657297124780433)]\n",
      "    'bonne qualité commerciale' has 3 words\n",
      "    'froment bonne qualité' has 3 words\n",
      "    'seconde qualité avoine' has 3 words\n",
      "    'indig froment quai' has 3 words\n",
      "    'acren eghezée fleurus' has 3 words\n",
      "    'expo prix moyen' has 3 words\n",
      "    'qualité commerciale froment' has 3 words\n",
      "    'qualité commerciale avoine' has 3 words\n",
      "    'qualité commerciale ment' has 3 words\n",
      "    'froment seconde qualité' has 3 words\n",
      "    'charleroi froment bonne' has 3 words\n",
      "    'courtrai froment bonne' has 3 words\n",
      "    'qualité avoine liege' has 3 words\n",
      "    'bruxelles froment bonne' has 3 words\n",
      "    'qualité avoine pommes' has 3 words\n",
      "    'vendus poids qualité' has 3 words\n",
      "    'qualité lea nourrains' has 3 words\n",
      "    'tirlemont thuin pailles' has 3 words\n",
      "    'poids qualité lea' has 3 words\n",
      "    'soignivs tirlemont thuin' has 3 words\n",
      "    'tirlemont thuin verviers' has 3 words\n",
      "    'tirlemont thuin aliments' has 3 words\n",
      "    'naamur proment bonne' has 3 words\n",
      "    'commerciale froment seconde' has 3 words\n",
      "    'prix moyen kilo' has 3 words\n",
      "    'beiges indig froment' has 3 words\n",
      "    'disp indig froment' has 3 words\n",
      "    'indig froment rendus' has 3 words\n",
      "    'oeux acren eghezée' has 3 words\n",
      "    'moyen kilo pied' has 3 words\n",
      "    'kilo pied foire' has 3 words\n",
      "    'eghezée fleurus flobecq' has 3 words\n",
      "    'eghezée fleurus hannut' has 3 words\n",
      "    'gares beiges indig' has 3 words\n",
      "    'belges disp indig' has 3 words\n",
      "    'trond soignivs tirlemont' has 3 words\n",
      "    'commerciale ment seconde' has 3 words\n",
      "    'bruxelles acren eghezée' has 3 words\n",
      "    'annon voix peuple' has 3 words\n",
      "    'voix peuple trouvère' has 3 words\n",
      "    'feuilleton voix peuple' has 3 words\n",
      "    'voix peuple mvww' has 3 words\n",
      "    'voix peuple pont' has 3 words\n",
      "    'août indig froment' has 3 words\n",
      "    'commerciale avoine bruxelles' has 3 words\n",
      "    'froment quai choix' has 3 words\n",
      "    'froment quai moy' has 3 words\n",
      "    'prix moyen vaches' has 3 words\n",
      "    'froment rendus gares' has 3 words\n",
      "    'peau vêtements draps' has 3 words\n",
      "  Bigrams found: []\n",
      "\n",
      "KB_JB1051_1957-04-18_01-00004.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('comète arend roland', 0.00010928397887496835), ('uccle comète arend', 0.00012682720036242688), ('arend roland visible', 0.00022684405739031093), ('arend roland clichés', 0.00024262657022217646), ('découverte arend roland', 0.00024320000070321234)]\n",
      "    'comète arend roland' has 3 words\n",
      "    'uccle comète arend' has 3 words\n",
      "    'arend roland visible' has 3 words\n",
      "    'arend roland clichés' has 3 words\n",
      "    'découverte arend roland' has 3 words\n",
      "    'cle arend roland' has 3 words\n",
      "    'arend roland donnèrent' has 3 words\n",
      "    'arend roland brillante' has 3 words\n",
      "    'comète arend lgnd' has 3 words\n",
      "    'exactes comète arend' has 3 words\n",
      "    'arend lgnd visible' has 3 words\n",
      "    'roland visible belgique' has 3 words\n",
      "    'appa reils fin' has 3 words\n",
      "    'hong kong déjà' has 3 words\n",
      "    'piano bém maj' has 3 words\n",
      "    'mal appa reil' has 3 words\n",
      "    'congrès fédéral bruxellois' has 3 words\n",
      "    'bruxellois congrès fédéral' has 3 words\n",
      "    'juin découverte arend' has 3 words\n",
      "    'bcheututes hong kong' has 3 words\n",
      "    'puissants appa reils' has 3 words\n",
      "    'grippé hong kong' has 3 words\n",
      "    'gazeuses appa reil' has 3 words\n",
      "    'mélodute bém maj' has 3 words\n",
      "    'ancienne bém maj' has 3 words\n",
      "    'hong kong singapour' has 3 words\n",
      "    'hong kong rush' has 3 words\n",
      "    'appa reil digestif' has 3 words\n",
      "    'appa reil digne' has 3 words\n",
      "    'bém maj harpe' has 3 words\n",
      "    'bém maj motart' has 3 words\n",
      "    'bém maj concort' has 3 words\n",
      "    'observatoire cle arend' has 3 words\n",
      "    'ivresse toux asthme' has 3 words\n",
      "    'extrême orient maison' has 3 words\n",
      "    'obligation série obligations' has 3 words\n",
      "    'romancière françoise sagan' has 3 words\n",
      "    'série obligations série' has 3 words\n",
      "    'réactions questions cruciales' has 3 words\n",
      "    'cruciales réactions questions' has 3 words\n",
      "    'questions cruciales réactions' has 3 words\n",
      "    'savants français demandent' has 3 words\n",
      "    'officiel congrès fédéral' has 3 words\n",
      "    'fédéral bruxellois pferre' has 3 words\n",
      "    'grès fédéral bruxellois' has 3 words\n",
      "    'fédéral bruxellois approuve' has 3 words\n",
      "    'questions cruciales question' has 3 words\n",
      "    'congrès fédéral élu' has 3 words\n",
      "    'néfaste congrès fédéral' has 3 words\n",
      "    'congrès fédéral rédigé' has 3 words\n",
      "  Bigrams found: []\n",
      "\n",
      "KB_JB1051_1957-10-14_01-00001.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('apposée maison presse', 0.0001724969415329487), ('demandons représentants syndicaux', 0.00017674291081881972), ('maison presse communiste', 0.0001933907758568288), ('fraternel hommage rendu', 0.00020140629230721972), ('centre atomique soviétique', 0.00023521757015421542)]\n",
      "    'apposée maison presse' has 3 words\n",
      "    'demandons représentants syndicaux' has 3 words\n",
      "    'maison presse communiste' has 3 words\n",
      "    'fraternel hommage rendu' has 3 words\n",
      "    'centre atomique soviétique' has 3 words\n",
      "    'section bruxelles ville' has 3 words\n",
      "    'hommage réunissant maison' has 3 words\n",
      "    'maison presse ncs' has 3 words\n",
      "    'réunissant maison presse' has 3 words\n",
      "    'plaque apposée maison' has 3 words\n",
      "    'bronze apposée maison' has 3 words\n",
      "    'représentants syndicaux pression' has 3 words\n",
      "    'réalité demandons représentants' has 3 words\n",
      "    'représentants syndicaux échelons' has 3 words\n",
      "    'bur nelle gaston' has 3 words\n",
      "    'militaires demandons représentants' has 3 words\n",
      "    'bruxelles fraternel hommage' has 3 words\n",
      "    'ile centre atomique' has 3 words\n",
      "    'nelle gaston moulin' has 3 words\n",
      "    'rou fraternel hommage' has 3 words\n",
      "    'change berlin ouest' has 3 words\n",
      "    'vailleurs propriétaires maison' has 3 words\n",
      "    'hollande australie majorité' has 3 words\n",
      "    'hollande australie élément' has 3 words\n",
      "    'commune bois cazier' has 3 words\n",
      "    'bois cazier chose' has 3 words\n",
      "    'bretagne hollande australie' has 3 words\n",
      "    'firme brabant wallon' has 3 words\n",
      "    'maintenues brabant wallon' has 3 words\n",
      "    'ernest bur nelle' has 3 words\n",
      "    'bur nelle antoine' has 3 words\n",
      "    'capitalistes double pécule' has 3 words\n",
      "    'kanze mathieu ekatou' has 3 words\n",
      "    'monument mineur inauguré' has 3 words\n",
      "    'serrés monument mineur' has 3 words\n",
      "    'monument mineur oeuvre' has 3 words\n",
      "    'flcurir monument mineur' has 3 words\n",
      "    'monument mineur traits' has 3 words\n",
      "    'kiri kanze ekatou' has 3 words\n",
      "    'kanze ekatou protestent' has 3 words\n",
      "    'kanze ekatou réfléchi' has 3 words\n",
      "    'reprochée kanze ekatou' has 3 words\n",
      "    'kanze ekatou souvient' has 3 words\n",
      "    'hommage rendu ménioire' has 3 words\n",
      "    'ville associée hommage' has 3 words\n",
      "    'hommage section bruxelles' has 3 words\n",
      "    'jamais double pécule' has 3 words\n",
      "    'double pécule diminution' has 3 words\n",
      "    'double pécule semaines' has 3 words\n",
      "    'bois cazier marcinelle' has 3 words\n",
      "  Bigrams found: []\n",
      "\n",
      "KB_JB421_1902-10-19_01-00001.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('petite ligne page', 0.0002833677758520875), ('grande ligne page', 0.0002833677758520875), ('juges paix demain', 0.0004332570278638252), ('unioa juges paix', 0.0004332570278638252), ('juges paix célébrera', 0.0004332570278638252)]\n",
      "    'petite ligne page' has 3 words\n",
      "    'grande ligne page' has 3 words\n",
      "    'juges paix demain' has 3 words\n",
      "    'unioa juges paix' has 3 words\n",
      "    'juges paix célébrera' has 3 words\n",
      "    'hommage juges paix' has 3 words\n",
      "    'juges paix réunissent' has 3 words\n",
      "    'juges paix mensuellement' has 3 words\n",
      "    'journal juges paix' has 3 words\n",
      "    'jeune hommage juges' has 3 words\n",
      "    'uelle unioa juges' has 3 words\n",
      "    'paix célébrera anniversaire' has 3 words\n",
      "    'paix réunissent discutent' has 3 words\n",
      "    'ressortir fjfs rjo' has 3 words\n",
      "    'paix demain assemblée' has 3 words\n",
      "    'paix mensuellement publie' has 3 words\n",
      "    'aïijti abonnement anfr' has 3 words\n",
      "    'jeunes filles esterai' has 3 words\n",
      "    'henry forge lelles' has 3 words\n",
      "    'ligne page petite' has 3 words\n",
      "    'page petite ligne' has 3 words\n",
      "    'ligne page grande' has 3 words\n",
      "    'page grande ligne' has 3 words\n",
      "    'page œnt grande' has 3 words\n",
      "    'filles esterai découragent' has 3 words\n",
      "    'ligne page œnt' has 3 words\n",
      "    'ligne page oen' has 3 words\n",
      "    'page oen exploit' has 3 words\n",
      "    'page esml trouvé' has 3 words\n",
      "    'braves jeunes filles' has 3 words\n",
      "    'œil henry forge' has 3 words\n",
      "    'henry forge grève' has 3 words\n",
      "    'parisien henry forge' has 3 words\n",
      "    'cheminé fax pays' has 3 words\n",
      "    'ement nouvelle eglise' has 3 words\n",
      "    'souscription nouvelle eglise' has 3 words\n",
      "    'nouvelle eglise gnîfique' has 3 words\n",
      "    'jolie page esml' has 3 words\n",
      "    'crochets jolie page' has 3 words\n",
      "    'œnt grande ligne' has 3 words\n",
      "    'jeunes filles monde' has 3 words\n",
      "    'misère vaincus grande' has 3 words\n",
      "    'europe crucis généraux' has 3 words\n",
      "    'gueule généraux assourdis' has 3 words\n",
      "    'généraux assourdis acclamations' has 3 words\n",
      "    'raeha rené henry' has 3 words\n",
      "    'rené henry iim' has 3 words\n",
      "    'henry iim tarati' has 3 words\n",
      "    'frauc maçonuerie billet' has 3 words\n",
      "    'égorgé ensanglanté nouvelle' has 3 words\n",
      "  Bigrams found: []\n",
      "\n",
      "KB_JB421_1907-10-26_01-00002.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('new york san', 0.0002637786593825319), ('vanderbilt new york', 0.0002637786593825319), ('new york bustes', 0.0002637786593825319), ('etat new york', 0.00026480488569656583), ('new york comète', 0.0002696523933577)]\n",
      "    'new york san' has 3 words\n",
      "    'vanderbilt new york' has 3 words\n",
      "    'new york bustes' has 3 words\n",
      "    'etat new york' has 3 words\n",
      "    'new york comète' has 3 words\n",
      "    'muoieipalïté new york' has 3 words\n",
      "    'new york engagea' has 3 words\n",
      "    'new york itaîiâos' has 3 words\n",
      "    'ville new york' has 3 words\n",
      "    'haute société américaine' has 3 words\n",
      "    'york san francisco' has 3 words\n",
      "    'york engagea butler' has 3 words\n",
      "    'york bustes frédéric' has 3 words\n",
      "    'haute société escalier' has 3 words\n",
      "    'introduit etat new' has 3 words\n",
      "    'exposé société américaine' has 3 words\n",
      "    'york comète astronomes' has 3 words\n",
      "    'demain muoieipalïté new' has 3 words\n",
      "    'york itaîiâos assiégé' has 3 words\n",
      "    'roman haute société' has 3 words\n",
      "    'entré haute société' has 3 words\n",
      "    'société américaine documenter' has 3 words\n",
      "    'société américaine éloctro' has 3 words\n",
      "    'don ville new' has 3 words\n",
      "    'maître hôtel romancier' has 3 words\n",
      "    'sorte maître hôtel' has 3 words\n",
      "    'roi edouard vii' has 3 words\n",
      "    'haute cascade monde' has 3 words\n",
      "    'ies bijoux reine' has 3 words\n",
      "    'maître hôtel majordome' has 3 words\n",
      "    'mystérieux maître hôtel' has 3 words\n",
      "    'vii riant haute' has 3 words\n",
      "    'visage instincts propreté' has 3 words\n",
      "    'rendus fra angelico' has 3 words\n",
      "    'suave fra angelico' has 3 words\n",
      "    'fiq shake hand' has 3 words\n",
      "    'hand connaissez shake' has 3 words\n",
      "    'connaissez shake hand' has 3 words\n",
      "    'royaux edouard vii' has 3 words\n",
      "    'fra angelico continuait' has 3 words\n",
      "    'fra angelico lesncur' has 3 words\n",
      "    'shake hand connaissez' has 3 words\n",
      "    'shake hand excusable' has 3 words\n",
      "    'edouard vii aime' has 3 words\n",
      "    'edouard vii riant' has 3 words\n",
      "    'riant haute cascade' has 3 words\n",
      "    'haute cascade présent' has 3 words\n",
      "    'cascade présent haute' has 3 words\n",
      "    'présent haute cascade' has 3 words\n",
      "    'reine vente bijoux' has 3 words\n",
      "  Bigrams found: []\n",
      "\n",
      "KB_JB421_1909-05-07_01-00003.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('commissaire police quartier', 0.00018258490986682938), ('coston commissaire police', 0.00020983084074652678), ('police quartier epinettes', 0.0002274901973896996), ('enquête coston commissaire', 0.0004453079359689883), ('commissaire police jeté', 0.00046684931806686704)]\n",
      "    'commissaire police quartier' has 3 words\n",
      "    'coston commissaire police' has 3 words\n",
      "    'police quartier epinettes' has 3 words\n",
      "    'enquête coston commissaire' has 3 words\n",
      "    'commissaire police jeté' has 3 words\n",
      "    'décision commissaire police' has 3 words\n",
      "    'catholique belge connue' has 3 words\n",
      "    'sphères catholiques belges' has 3 words\n",
      "    'quartier epinettes arriva' has 3 words\n",
      "    'quartier epinettes revenu' has 3 words\n",
      "    'jos isaye concorda' has 3 words\n",
      "    'jos isaye mélange' has 3 words\n",
      "    'secousse sismique sentir' has 3 words\n",
      "    'secousses sismiques accompagnées' has 3 words\n",
      "    'mazurka jos isaye' has 3 words\n",
      "    'violent secousse sismique' has 3 words\n",
      "    'forio secousses sismiques' has 3 words\n",
      "    'attribuait décision commissaire' has 3 words\n",
      "    'catholiques belges frontières' has 3 words\n",
      "    'direction jos isaye' has 3 words\n",
      "    'flacon vitriol commissaire' has 3 words\n",
      "    'vitriol commissaire atteint' has 3 words\n",
      "    'commissaire atteint légèrement' has 3 words\n",
      "    'passage chatelet établie' has 3 words\n",
      "    'passage chatelet accompagné' has 3 words\n",
      "    'parti catholique belge' has 3 words\n",
      "    'chronique régionale mémorandum' has 3 words\n",
      "    'belges frontières belgique' has 3 words\n",
      "    'petit garçon pqijj' has 3 words\n",
      "    'jeter petit garçon' has 3 words\n",
      "    'louise mazurka jos' has 3 words\n",
      "    'isaye mélange pavart' has 3 words\n",
      "    'glace craint gelées' has 3 words\n",
      "    'bouillante mère rentra' has 3 words\n",
      "    'millions violent secousse' has 3 words\n",
      "    'asseà forio secousses' has 3 words\n",
      "    'chatelet établie ébéniste' has 3 words\n",
      "    'chatelet accompagné eschwege' has 3 words\n",
      "    'garçon pqijj assouvir' has 3 words\n",
      "    'erkner ban berlinoise' has 3 words\n",
      "    'remplie eau bouillante' has 3 words\n",
      "    'eau bouillante mère' has 3 words\n",
      "    'débattait eau bouillante' has 3 words\n",
      "    'poursuivant enquête coston' has 3 words\n",
      "    'vaucoret tenait main' has 3 words\n",
      "    'électrocuté passage chatelet' has 3 words\n",
      "    'catholiques amateurs cerises' has 3 words\n",
      "    'cerises amateurs cerises' has 3 words\n",
      "    'vaucoret électrocuté passage' has 3 words\n",
      "    'orbite rapproché soleil' has 3 words\n",
      "  Bigrams found: []\n",
      "\n",
      "KB_JB421_1909-08-06_01-00001.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('ministre affaires étrangères', 0.00012009809581470953), ('hellebaut ministre guerre', 0.00014146167579269476), ('général hellebaut ministre', 0.0001497752577622115), ('nom commission spéciale', 0.00015162302339207073), ('commission spéciale vicinaux', 0.00015828533939126526)]\n",
      "    'ministre affaires étrangères' has 3 words\n",
      "    'hellebaut ministre guerre' has 3 words\n",
      "    'général hellebaut ministre' has 3 words\n",
      "    'nom commission spéciale' has 3 words\n",
      "    'commission spéciale vicinaux' has 3 words\n",
      "    'traitement marks frais' has 3 words\n",
      "    'marks traitement marks' has 3 words\n",
      "    'secrétaire affaires étrangères' has 3 words\n",
      "    'etat affaires étrangères' has 3 words\n",
      "    'affaires étrangères qualité' has 3 words\n",
      "    'affaires étrangères émargera' has 3 words\n",
      "    'affaires étrangères chambre' has 3 words\n",
      "    'affaires étrangères purement' has 3 words\n",
      "    'demande ministre affaires' has 3 words\n",
      "    'situation ministre affaires' has 3 words\n",
      "    'général ministre guerre' has 3 words\n",
      "    'secrétaire etat affaires' has 3 words\n",
      "    'novembre général hellebaut' has 3 words\n",
      "    'carnets demi tasses' has 3 words\n",
      "    'droit demi tasses' has 3 words\n",
      "    'demi tasse paie' has 3 words\n",
      "    'demi tasses prix' has 3 words\n",
      "    'demi tasses application' has 3 words\n",
      "    'allemagne demi tasse' has 3 words\n",
      "    'percevront demi tasses' has 3 words\n",
      "    'demi tasses ser' has 3 words\n",
      "    'ensuite secrétaire affaires' has 3 words\n",
      "    'ministre guerre visité' has 3 words\n",
      "    'ministre guerre heurté' has 3 words\n",
      "    'ministre guerre envisageait' has 3 words\n",
      "    'centimes additionnels minorité' has 3 words\n",
      "    'centimes additionnels affecté' has 3 words\n",
      "    'centimes additionnels donne' has 3 words\n",
      "    'produit centimes additionnels' has 3 words\n",
      "    'majoration centimes additionnels' has 3 words\n",
      "    'utenant général hellebaut' has 3 words\n",
      "    'service général ministre' has 3 words\n",
      "    'nombre centimes additionnels' has 3 words\n",
      "    'prince albert mande' has 3 words\n",
      "    'prince albert fixée' has 3 words\n",
      "    'leone prince albert' has 3 words\n",
      "    'session nom commission' has 3 words\n",
      "    'moffarts nom commission' has 3 words\n",
      "    'rentrée prince albert' has 3 words\n",
      "    'billets gagnants lots' has 3 words\n",
      "    'spéciale vicinaux montré' has 3 words\n",
      "    'nom moyen age' has 3 words\n",
      "    'arrivée prince albert' has 3 words\n",
      "    'frais dereprésen secrétaire' has 3 words\n",
      "    'bruxelles gagnant lot' has 3 words\n",
      "  Bigrams found: []\n",
      "\n",
      "KB_JB421_1909-10-23_01-00001.txt:\n",
      "  Total keywords extracted: 50\n",
      "  First 5 keywords: [('vice consul adjoint', 0.00012115762787490096), ('santé duc karl', 0.00013219429309481), ('ecclésiastiques inscrites loi', 0.00013967463811696216), ('septembre visiteurs ascension', 0.00016321309580085366), ('adjoint consulat belgique', 0.00019154632409039033)]\n",
      "    'vice consul adjoint' has 3 words\n",
      "    'santé duc karl' has 3 words\n",
      "    'ecclésiastiques inscrites loi' has 3 words\n",
      "    'septembre visiteurs ascension' has 3 words\n",
      "    'adjoint consulat belgique' has 3 words\n",
      "    'immunités ecclésiastiques inscrites' has 3 words\n",
      "    'appliquer chemins fer' has 3 words\n",
      "    'administration chemins fer' has 3 words\n",
      "    'chemins fer etat' has 3 words\n",
      "    'chemin fer lea' has 3 words\n",
      "    'commis chemins fer' has 3 words\n",
      "    'chemins fer continuent' has 3 words\n",
      "    'accident chemin fer' has 3 words\n",
      "    'nommé vice consul' has 3 words\n",
      "    'bioiley vice consul' has 3 words\n",
      "    'biervliet vice consul' has 3 words\n",
      "    'vice consul pékin' has 3 words\n",
      "    'propos santé duc' has 3 words\n",
      "    'nouvelles santé duc' has 3 words\n",
      "    'lantsheere santé duc' has 3 words\n",
      "    'santé duc charles' has 3 words\n",
      "    'consul adjoint légation' has 3 words\n",
      "    'consul adjoint smyme' has 3 words\n",
      "    'duc karl théodore' has 3 words\n",
      "    'chemins fer annonce' has 3 words\n",
      "    'duc karl tliéqdoie' has 3 words\n",
      "    'gand adjoint consulat' has 3 words\n",
      "    'lieutenant rendolet lieutenant' has 3 words\n",
      "    'rendolet lieutenant rendolet' has 3 words\n",
      "    'lésions internes lésions' has 3 words\n",
      "    'internes lésions internes' has 3 words\n",
      "    'smyme adjoint consulat' has 3 words\n",
      "    'permettaient immunités ecclésiastiques' has 3 words\n",
      "    'condition immunité ecclésiastiques' has 3 words\n",
      "    'inscrites loi majorité' has 3 words\n",
      "    'duc charles théodore' has 3 words\n",
      "    'adjoint smyme adjoint' has 3 words\n",
      "    'consul pékin fils' has 3 words\n",
      "    'inscrites loi fassent' has 3 words\n",
      "    'fer etat prochainement' has 3 words\n",
      "    'attagé consulat général' has 3 words\n",
      "    'adversaires appliquer chemins' has 3 words\n",
      "    'traitement commis chemins' has 3 words\n",
      "    'tapissières administration chemins' has 3 words\n",
      "    'fer lea wagons' has 3 words\n",
      "    'public bureaux télégraphiques' has 3 words\n",
      "    'congo lieutenant rendolet' has 3 words\n",
      "    'communal paraît jules' has 3 words\n",
      "    'paraît jules poncin' has 3 words\n",
      "    'avenue jules poncin' has 3 words\n",
      "  Bigrams found: []\n"
     ]
    }
   ],
   "source": [
    "# Keyword extractor trouble shooting : the error was the n-gram settings\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"fr\", \n",
    "    top=50,\n",
    "    stopwords=sw, \n",
    "    n= 3 #yake defaults to 3 ngrams \n",
    ")\n",
    "\n",
    "all_token_keywords = {}\n",
    "\n",
    "for filename in sorted(tokens_by_file.keys())[:10]:\n",
    "    # Use pre-filtered tokens (already has stopwords, length, OCR filters removed)\n",
    "    filtered_tokens = tokens_by_file[filename]\n",
    "    \n",
    "    # Convert token list back to text for YAKE\n",
    "    text_from_tokens = ' '.join(filtered_tokens)\n",
    "    \n",
    "    # Extract keywords from filtered text\n",
    "    token_keywords = kw_extractor.extract_keywords(text_from_tokens)\n",
    "    all_token_keywords[filename] = token_keywords\n",
    "    \n",
    "    # DEBUG: Print what we got\n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(f\"  Total keywords extracted: {len(token_keywords)}\")\n",
    "    if len(token_keywords) > 0:\n",
    "        print(f\"  First 5 keywords: {token_keywords[:5]}\")\n",
    "    \n",
    "    # Filter by n-gram size\n",
    "    kept_tokens = []\n",
    "    for kw, score in token_keywords:\n",
    "        token_words = kw.split()\n",
    "        print(f\"    '{kw}' has {len(token_words)} words\")  # DEBUG\n",
    "        if len(token_words) == 2:\n",
    "            kept_tokens.append(kw)\n",
    "    \n",
    "    print(f\"  Bigrams found: {kept_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "726fca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_JB1051_1927-08-25_01-00004.txt mentions these keywords: Charles Croa, Croa considéré, étoiles filantes, Edison revendique, local habituel, signalé dernièrement, poète ot'savant, Thomas EDISON, ROUGE Edison, Edison vient, Edison n'aurait...\n",
      "KB_JB1051_1937-11-14_01-00006.txt mentions these keywords: record mondial, étoiles filantes, l'Union Soviétique, iiiiii ggggggg, d'étoiles filantes, record soviétique, nageur soviétique, meilleurs nageurs, capitaine Sarva, Jean Blume, mètres brasse...\n",
      "KB_JB1051_1939-08-06_01-00005.txt mentions these keywords: CHRONIQUE MEDICALE, Snfants CHRONIQUE, LACHER MONSTRE, prix moyen, bonne qualité, qualité commerciale, répondit Charles, père Grandet...\n",
      "KB_JB1051_1957-04-18_01-00004.txt mentions these keywords: duc Henri, congrès fédéral, Chine populaire, Maison Blanche, foi catholique...\n",
      "KB_JB1051_1957-10-14_01-00001.txt mentions these keywords: Presse Communiste, PARTI COMMUNISTE, Joseph Jacquemotte, REVENDICATIONS OUVRIERES, l'Harmonie ouvrière, Isa Ufania, Gaston Moulin...\n",
      "KB_JB421_1902-10-19_01-00001.txt mentions these keywords: grande ligne, petite ligne, généraux boers, qu'on annonce, qu'on vient, Quint C'est, Zola qu'on, nouvelle Eglise, c'est décidément, oen Exploit, Semplairea parviendront, Exploit d'huissier...\n",
      "KB_JB421_1907-10-26_01-00002.txt mentions these keywords: grand peintre, l'art chrétien, inoins sombre, vitrail vit, grand artiste, grands artistes, quo c'est, peintre religieux, qu'une teinto, enfants Rubens, Fra Angelico, sombre d'après...\n",
      "KB_JB421_1909-05-07_01-00003.txt mentions these keywords: Chronique Régionale, Régionale Mémorandum, Mémorandum VENDREDI, jeune Vaucoret, lundi matin, jeudi matin, parti catholique...\n",
      "KB_JB421_1909-08-06_01-00001.txt mentions these keywords: Dom Bosco, prince Albert, affaires étrangères, cardinal Fischer, Moyen Age, congrès eucharistique, Caaille JOSET...\n",
      "KB_JB421_1909-10-23_01-00001.txt mentions these keywords: grande ligne, petite ligne, Faits-drt ANNONCES, prince Albert, VIEUX DOMESTIQUES, service général, vrais noms, Clément XIV, o'bscm Havas, FORFAIT ttffett, Camille ciOSET, vieux serviteurs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Charles Croa considéré', 0.0029370312473949597),\n",
       " ('français Charles Croa', 0.007372207109217219),\n",
       " ('août', 0.009226624320164533),\n",
       " ('heures', 0.011097201220840047),\n",
       " ('rayon', 0.012074540896682629),\n",
       " ('Charles Croa', 0.013159482370496954),\n",
       " ('Croa considéré', 0.0141123525169677),\n",
       " ('ROUGE Edison revendique', 0.014362501024402955),\n",
       " (\"c'est\", 0.014604325631629743),\n",
       " (\"ot'savant français Charles\", 0.025929797378944734),\n",
       " ('étoiles filantes', 0.02748617855962668),\n",
       " ('Jeudi', 0.027524975695742946),\n",
       " (\"poète ot'savant français\", 0.0280275387270016),\n",
       " ('Edison', 0.0302702276085155),\n",
       " ('Edison revendique', 0.033120346362984984),\n",
       " ('local habituel', 0.033341098927339774),\n",
       " (\"étoiles filantes C'est\", 0.03656490903713678),\n",
       " ('FEDERATION', 0.03786731375020705),\n",
       " ('étoiles', 0.03877539687205935),\n",
       " ('Charles Cros déposa', 0.045843631060012185),\n",
       " ('revendique la priorité', 0.046973420429231486),\n",
       " ('signalé dernièrement', 0.046973420429231486),\n",
       " ('dernièrement la mort', 0.046973420429231486),\n",
       " ('mort du poète', 0.046973420429231486),\n",
       " (\"poète ot'savant\", 0.046973420429231486),\n",
       " ('Tomes', 0.047795612382732625),\n",
       " ('synthétiques Thomas EDISON', 0.04821109142895892),\n",
       " (\"Charles Cros n'ait\", 0.048380820835442234),\n",
       " ('local', 0.04860618682510445),\n",
       " ('soir', 0.04869149279040057),\n",
       " ('Charles', 0.04942491403235117),\n",
       " (\"Edison n'aurait l'ait\", 0.05102132621611343),\n",
       " ('terre', 0.055363574389345194),\n",
       " ('filantes', 0.055643580740740654),\n",
       " ('grand', 0.05623230341585566),\n",
       " ('Thomas EDISON', 0.056568617763353486),\n",
       " ('Vendredi', 0.059756209267992924),\n",
       " ('ROUGE Edison', 0.06143413884031184),\n",
       " ('Samedi', 0.0625959614009114),\n",
       " ('Ghetto', 0.06331775737426223),\n",
       " ('Dimanche', 0.06486475967446324),\n",
       " ('Croa', 0.0656984433006752),\n",
       " ('phonographe', 0.07058045846764091),\n",
       " ('fer', 0.07079739875843562),\n",
       " ('habituel', 0.07144967584574388),\n",
       " ('Edison vient', 0.07227825083461525),\n",
       " (\"Edison n'aurait\", 0.07882621789555411),\n",
       " ('chant', 0.07891067186951502),\n",
       " ('fragments', 0.08030238131967903),\n",
       " ('rêveurs du Ghetto', 0.08117688802300661)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OLDEST version bc I'm scared even if versionning exists I like having it here \n",
    "#keep it for now just in case even though it'll be on github but whatever\n",
    "\n",
    "all_token_keywords = {}  # Store keywords for each file in a dictionary \n",
    "\n",
    "for f in sorted(tokenised_files)[:10]:\n",
    "    tokenised_text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    token_keywords = kw_extractor.extract_keywords(tokenised_text)\n",
    "    all_token_keywords[f] = token_keywords  # Save to dictionary\n",
    "    #print(token_keywords) #this seems to have the scores associated with it, and seems to consider trigrams as well, idk what to do with that yet but I'm leaving it here for now \n",
    "    \n",
    "    kept_tokens = []\n",
    "    for kw, score in token_keywords:\n",
    "        token_words = kw.split()\n",
    "        if len(token_words) == 2: # when i tried all_token_keywords here AND when i use token words again it gives it to me in a different order, wtf\n",
    "            kept_tokens.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join((kept_tokens))}...\")\n",
    "    \n",
    "\n",
    "#fixed! I used kept and not kept_tokens lol \n",
    "\n",
    "\n",
    "#see the kw and score of a file \n",
    "all_token_keywords['KB_JB1051_1927-08-25_01-00004.txt']\n",
    "\n",
    "\n",
    "#oh this is SO weird, KB_JB1051_1937-11-14_01-00006.txt is showing kw (first ones Baleine, Soviétique, record mondial, comète,) \n",
    "#BUT none of the others are, and that's not even one that I've ran individually before as far as I'm aware\n",
    "#ah yeah bc I wasn't paying attention and I used the wrong variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137821d1",
   "metadata": {},
   "source": [
    "# FROM HERE DOWN WE'RE IN DRAFT MODE \n",
    "Tokenised all the files, now I'm working on kw extraction for each of the files so we can start comparing, maybe then only take the lowest vectors from the dataset and start making the word cloud from there (that way the keywords of EACH file are preserved, not just the bashed kw (actually wait it'd be interesting to see if they'd be different(I should include that in the analysis as the limits of NLP???)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b4d305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KB_JB421_1909-05-07_01-00003.txt': [('Chronique Régionale Mémorandum',\n",
       "   0.0002689124983344946),\n",
       "  ('Régionale Mémorandum VENDREDI', 0.002386733429198891),\n",
       "  ('Chronique Régionale', 0.004150330458462885),\n",
       "  ('Régionale Mémorandum', 0.004150330458462885),\n",
       "  ('mais', 0.013958866031261659),\n",
       "  (\"D'un\", 0.01929719188917196),\n",
       "  (\"d'une\", 0.02018759615058267),\n",
       "  ('Arlon', 0.023584878032698226),\n",
       "  ('ans', 0.0246003628612249),\n",
       "  ('jour', 0.02690405234751007),\n",
       "  ('matin', 0.027702988285563952),\n",
       "  ('jours', 0.030267058890948825),\n",
       "  (\"qu'elle\", 0.032446463309300795),\n",
       "  (\"C'est\", 0.032944929355833284),\n",
       "  ('temps', 0.035042085839226515),\n",
       "  ('Mémorandum VENDREDI', 0.035807270629173875),\n",
       "  (\"qu'il\", 0.0382299828576165),\n",
       "  ('grande', 0.041327147604164755),\n",
       "  (\"s'est\", 0.047010353723982996),\n",
       "  ('saint', 0.049186221326980764),\n",
       "  (\"d'une grande\", 0.05153615966742675),\n",
       "  ('jeune Vaucoret', 0.05160650341857592),\n",
       "  ('Point', 0.05243635803513611),\n",
       "  (\"d'une grande maison\", 0.056975265432775514),\n",
       "  ('lundi matin', 0.05727133296873441),\n",
       "  ('ENFANT', 0.060799705865179444),\n",
       "  ('Chronique', 0.06428978465134347),\n",
       "  ('Régionale', 0.06428978465134347),\n",
       "  ('Mémorandum', 0.06428978465134347),\n",
       "  ('jeune', 0.06555799422479636),\n",
       "  ('marche', 0.06630093637373444),\n",
       "  ('jeudi matin', 0.06963786072563034),\n",
       "  ('MAI', 0.06979433015630829),\n",
       "  ('grand', 0.06995185316632929),\n",
       "  ('soleil', 0.07160622628873936),\n",
       "  ('nuit', 0.07161723195027286),\n",
       "  ('police', 0.07190063449125743),\n",
       "  ('heures', 0.07257092823245526),\n",
       "  (\"suite d'un\", 0.0726277018112244),\n",
       "  ('Belgique', 0.07282867340359367),\n",
       "  ('pris', 0.07331881367994549),\n",
       "  ('Servin', 0.0769212680012506),\n",
       "  (\"bourgmestre d'un petit\", 0.08020818032111693),\n",
       "  ('hier', 0.08190373494434151),\n",
       "  ('bourgmestre', 0.08211939626804769),\n",
       "  ('Epinettes', 0.08304424139003161),\n",
       "  (\"qu'il fut\", 0.084073682760569),\n",
       "  ('mort', 0.08488205448059288),\n",
       "  ('Vaucoret', 0.08520688279873276),\n",
       "  ('midi', 0.08528643295740043)],\n",
       " 'KB_JB421_1909-08-06_01-00001.txt': [('Cologne', 0.01278982588241806),\n",
       "  (\"c'est\", 0.015946951287173276),\n",
       "  ('Congrès', 0.025073930485691523),\n",
       "  (\"qu'il\", 0.03273510258557793),\n",
       "  (\"s'est\", 0.034325348044540634),\n",
       "  ('francs', 0.03965049046173783),\n",
       "  ('Roi', 0.04138784142488191),\n",
       "  ('LUXEM Organe Quotidien', 0.04334744877318239),\n",
       "  ('Organe Quotidien Catholique', 0.04412679789736237),\n",
       "  ('marks', 0.04553426957268652),\n",
       "  ('Belgique', 0.04764405050497673),\n",
       "  ('Salésiens', 0.05152114030001903),\n",
       "  ('Dom Bosco', 0.05285040669573859),\n",
       "  ('cathédrale', 0.05586989452566482),\n",
       "  ('heures', 0.057584849334965216),\n",
       "  ('vue', 0.060910628088114954),\n",
       "  ('NONCES', 0.06241272328876263),\n",
       "  ('ville', 0.0625867102955465),\n",
       "  ('monde', 0.06349367165792075),\n",
       "  (\"d'une\", 0.06523273091111352),\n",
       "  ('ministre', 0.06588428605805847),\n",
       "  ('prince Albert', 0.06716601407686225),\n",
       "  ('temps', 0.0674437987327909),\n",
       "  ('jour', 0.07336326246183583),\n",
       "  ('cardinal Fischer', 0.07738016163387736),\n",
       "  ('Albert', 0.07762035033220036),\n",
       "  ('affaires étrangères', 0.07801557482839135),\n",
       "  ('Bruxelles', 0.0785700352326191),\n",
       "  ('cardinal', 0.079095212877052),\n",
       "  ('catholiques', 0.08128388197047456),\n",
       "  (\"d'un\", 0.08402124822090382),\n",
       "  ('nom', 0.08461229083323706),\n",
       "  ('faire', 0.08553554172019345),\n",
       "  ('Moyen Age', 0.08590291283247567),\n",
       "  ('traitement', 0.08706338384655588),\n",
       "  ('chancelier', 0.08820857671854174),\n",
       "  (\"qu'on\", 0.08853593427838431),\n",
       "  ('TKAITB A FORFAIT', 0.09294821752900179),\n",
       "  ('congrès eucharistique', 0.09345579846010814),\n",
       "  ('Dom', 0.0944817150847845),\n",
       "  ('maisons', 0.0945563106070927),\n",
       "  ('société', 0.09626309888751855),\n",
       "  ('année', 0.09781279595821184),\n",
       "  ('grande', 0.09857105319762716),\n",
       "  ('nouvelles', 0.1031491917706524),\n",
       "  ('villes', 0.10431118382591081),\n",
       "  ('monde Hpr Dar', 0.10481458057256489),\n",
       "  ('pension', 0.1068147402240219),\n",
       "  ('conseil', 0.11252698130215232),\n",
       "  ('arable Dom Bosco', 0.11268344674415999)],\n",
       " 'KB_JB421_1909-10-23_01-00001.txt': [('ligne', 0.006576956516920981),\n",
       "  ('grande ligne', 0.010414814932783081),\n",
       "  ('jjjjgjs et Jugements', 0.012405281434844163),\n",
       "  (\"d'un\", 0.012768895090072173),\n",
       "  (\"qu'il\", 0.01404661186692881),\n",
       "  ('rue', 0.018646918396135623),\n",
       "  ('petite ligne', 0.022057795950928976),\n",
       "  ('Belgique', 0.022894960513796105),\n",
       "  ('Faits-drt ANNONCES', 0.02459954981037721),\n",
       "  ('service', 0.02593999816391375),\n",
       "  (\"qu'ils\", 0.032775427689500564),\n",
       "  ('bons', 0.03291445281262834),\n",
       "  ('faire', 0.03583407481960633),\n",
       "  ('bons vieux serviteurs', 0.03631056971969405),\n",
       "  ('TRAITE A FORFAIT', 0.03783214099594524),\n",
       "  ('bon', 0.0384001949480664),\n",
       "  ('prince', 0.039173282293782995),\n",
       "  ('Albert', 0.041280351933826676),\n",
       "  ('prince Albert', 0.04207603403127464),\n",
       "  (\"c'est\", 0.042311261418854614),\n",
       "  (\"d'une\", 0.044920043564026534),\n",
       "  ('rue Clément XIV', 0.04628643594921162),\n",
       "  (\"qu'il possède rue\", 0.05113952925236211),\n",
       "  (\"l'on\", 0.053223547434741274),\n",
       "  ('VIEUX', 0.054648719572990195),\n",
       "  (\"service d'un\", 0.05596176939282363),\n",
       "  (\"service d'un réel\", 0.06172615335668248),\n",
       "  ('Faits-drt', 0.06344636920368098),\n",
       "  ('Jugements', 0.06344636920368098),\n",
       "  (\"qu'elle\", 0.06537325387176664),\n",
       "  (\"maîtres qu'ils\", 0.06571725664248096),\n",
       "  (\"qu'ils out Sru\", 0.06646114148363362),\n",
       "  ('fut', 0.06771678728633244),\n",
       "  (\"rue d'Argeni ixellea\", 0.06795878962731165),\n",
       "  ('visiteurs', 0.0700874380461356),\n",
       "  ('hommes', 0.07198343152781589),\n",
       "  (\"maîtres qu'ils obtien\", 0.0732270707670453),\n",
       "  (\"nom d'un\", 0.07344637932889118),\n",
       "  ('rues', 0.07458767358454249),\n",
       "  ('VIEUX DOMESTIQUES', 0.07481074021820261),\n",
       "  ('vieux braves serviteurs', 0.0759595325253067),\n",
       "  ('fois', 0.07617135785874536),\n",
       "  (\"vieux hasselt qu'il\", 0.07677189715384393),\n",
       "  ('maîtres', 0.07839309630159925),\n",
       "  ('service général', 0.07841787651101065),\n",
       "  ('Ferrer', 0.07912366810476722),\n",
       "  ('bons vieux', 0.08172049304546637),\n",
       "  ('Palais', 0.08261737850588843),\n",
       "  ('comète', 0.08294105788268245),\n",
       "  (\"Bruxelles D'un relevé\", 0.08302961467871471)]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_1909 = {\n",
    "    fname: kws for fname, kws in all_token_keywords.items()\n",
    "    if fname.split('_')[2].startswith('1909') #testing with the 10 files\n",
    "}\n",
    "\n",
    "print(len(keywords_1909))\n",
    "keywords_1909 \n",
    "#ok, but I still need to filter out stop words... Before or after keywords? I think before yeah duh cause math "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
