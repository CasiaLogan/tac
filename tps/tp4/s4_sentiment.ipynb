{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis \n",
    "\n",
    "## 1. Textblob-FR\n",
    "\n",
    "Documentation: https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity_perc}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is 65% positive and 75% subjective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"Ce journal est vraiment super intéressant.\")\n",
    "#so this is great, but even if I had figured out how to get the transformers to work, I'm not sure how I would have isolated individual phrases. I initally had it in mind to use the whole page, but then there's different segments by different authors, and even if I had figured out just how to analyse the sections on halleys commet on a large scale, I guess I could have just isolated the phase with halleys commet it in but then there could be multiple sentences with that in it per article etc etc problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilisation de transformers\n",
    "\n",
    "Couldn't get it to work and didn't want to use collab :(\n",
    "\n",
    "Documentation: https://github.com/TheophileBlard/french-sentiment-analysis-with-bert\n",
    "\n",
    "**!!** Si le code ne tourne pas sur votre machine, vous pouvez le tester directement sur Google Colab en utilisant [ce lien](https://colab.research.google.com/github/TheophileBlard/french-sentiment-analysis-with-bert/blob/master/colab/french_sentiment_analysis_with_bert.ipynb) **!!**\n",
    "\n",
    "Le modèle peut également être testé en ligne sur [HuggingFace](https://huggingface.co/tblard/tf-allocine)\n",
    "\n",
    "### Installation des librairies et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch  # IN THIS ORDER\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du modèle STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtblard/tf-allocine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m model = TFAutoModelForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mtblard/tf-allocine\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m sentiment_analyser = pipeline(\u001b[33m'\u001b[39m\u001b[33msentiment-analysis\u001b[39m\u001b[33m'\u001b[39m, model=model, tokenizer=tokenizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:709\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    707\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    711\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1825\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   1822\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1823\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1828\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1829\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1830\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1833\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1834\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1835\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1836\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1988\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   1986\u001b[39m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[32m   1987\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1988\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1989\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1990\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m   1991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load vocabulary from file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\transformers\\models\\camembert\\tokenization_camembert_fast.py:128\u001b[39m, in \u001b[36mCamembertTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, additional_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    113\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m ):\n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[32m    126\u001b[39m     mask_token = AddedToken(mask_token, lstrip=\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.can_save_slow_tokenizer = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vocab_file \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:114\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m     fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslow_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.slow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[32m    117\u001b[39m     slow_tokenizer = \u001b[38;5;28mself\u001b[39m.slow_tokenizer_class(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1307\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer)\u001b[39m\n\u001b[32m   1299\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1300\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn instance of tokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1301\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1302\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1303\u001b[39m     )\n\u001b[32m   1305\u001b[39m converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m)\u001b[49m.converted()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:445\u001b[39m, in \u001b[36mSpmConverter.__init__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    441\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprotobuf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    443\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(*args)\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sentencepiece_model_pb2 \u001b[38;5;28;01mas\u001b[39;00m model_pb2\n\u001b[32m    447\u001b[39m m = model_pb2.ModelProto()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.original_tokenizer.vocab_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\transformers\\utils\\sentencepiece_model_pb2.py:91\u001b[39m\n\u001b[32m     25\u001b[39m _sym_db = _symbol_database.Default()\n\u001b[32m     28\u001b[39m DESCRIPTOR = _descriptor.FileDescriptor(\n\u001b[32m     29\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33msentencepiece_model.proto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     package=\u001b[33m\"\u001b[39m\u001b[33msentencepiece\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     ),\n\u001b[32m     81\u001b[39m )\n\u001b[32m     84\u001b[39m _TRAINERSPEC_MODELTYPE = _descriptor.EnumDescriptor(\n\u001b[32m     85\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mModelType\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     86\u001b[39m     full_name=\u001b[33m\"\u001b[39m\u001b[33msentencepiece.TrainerSpec.ModelType\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     87\u001b[39m     filename=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     88\u001b[39m     file=DESCRIPTOR,\n\u001b[32m     89\u001b[39m     create_key=_descriptor._internal_create_key,\n\u001b[32m     90\u001b[39m     values=[\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[43m_descriptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEnumValueDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUNIGRAM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m            \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcreate_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_descriptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_internal_create_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     99\u001b[39m         _descriptor.EnumValueDescriptor(\n\u001b[32m    100\u001b[39m             name=\u001b[33m\"\u001b[39m\u001b[33mBPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    101\u001b[39m             index=\u001b[32m1\u001b[39m,\n\u001b[32m    102\u001b[39m             number=\u001b[32m2\u001b[39m,\n\u001b[32m    103\u001b[39m             serialized_options=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    104\u001b[39m             \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    105\u001b[39m             create_key=_descriptor._internal_create_key,\n\u001b[32m    106\u001b[39m         ),\n\u001b[32m    107\u001b[39m         _descriptor.EnumValueDescriptor(\n\u001b[32m    108\u001b[39m             name=\u001b[33m\"\u001b[39m\u001b[33mWORD\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m             index=\u001b[32m2\u001b[39m,\n\u001b[32m    110\u001b[39m             number=\u001b[32m3\u001b[39m,\n\u001b[32m    111\u001b[39m             serialized_options=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    112\u001b[39m             \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    113\u001b[39m             create_key=_descriptor._internal_create_key,\n\u001b[32m    114\u001b[39m         ),\n\u001b[32m    115\u001b[39m         _descriptor.EnumValueDescriptor(\n\u001b[32m    116\u001b[39m             name=\u001b[33m\"\u001b[39m\u001b[33mCHAR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    117\u001b[39m             index=\u001b[32m3\u001b[39m,\n\u001b[32m    118\u001b[39m             number=\u001b[32m4\u001b[39m,\n\u001b[32m    119\u001b[39m             serialized_options=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    120\u001b[39m             \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    121\u001b[39m             create_key=_descriptor._internal_create_key,\n\u001b[32m    122\u001b[39m         ),\n\u001b[32m    123\u001b[39m     ],\n\u001b[32m    124\u001b[39m     containing_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    125\u001b[39m     serialized_options=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m     serialized_start=\u001b[32m1294\u001b[39m,\n\u001b[32m    127\u001b[39m     serialized_end=\u001b[32m1347\u001b[39m,\n\u001b[32m    128\u001b[39m )\n\u001b[32m    129\u001b[39m _sym_db.RegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)\n\u001b[32m    131\u001b[39m _MODELPROTO_SENTENCEPIECE_TYPE = _descriptor.EnumDescriptor(\n\u001b[32m    132\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mType\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    133\u001b[39m     full_name=\u001b[33m\"\u001b[39m\u001b[33msentencepiece.ModelProto.SentencePiece.Type\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    190\u001b[39m     serialized_end=\u001b[32m2184\u001b[39m,\n\u001b[32m    191\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\casia\\testTAC\\tac\\.venv\\Lib\\site-packages\\google\\protobuf\\descriptor.py:1038\u001b[39m, in \u001b[36mEnumValueDescriptor.__new__\u001b[39m\u001b[34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   1030\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1036\u001b[39m     create_key=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1037\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m   \u001b[43m_message\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m   \u001b[38;5;66;03m# There is no way we can build a complete EnumValueDescriptor with the\u001b[39;00m\n\u001b[32m   1040\u001b[39m   \u001b[38;5;66;03m# given parameters (the name of the Enum is not known, for example).\u001b[39;00m\n\u001b[32m   1041\u001b[39m   \u001b[38;5;66;03m# Fortunately generated files just pass it to the EnumDescriptor()\u001b[39;00m\n\u001b[32m   1042\u001b[39m   \u001b[38;5;66;03m# constructor, which will ignore it, so returning None is good enough.\u001b[39;00m\n\u001b[32m   1043\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"tblard/tf-allocine\")\n",
    "sentiment_analyser = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "#first I got a name error, then type, tried importing torch and tensorflow in different orders, working in the terminal, restarting everything, downgrading different versions of transformers, tensorflow, and torch, nothing seems to work so I'm calling it a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\", use_pt=False)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"tblard/tf-allocine\")\n",
    "sentiment_analyser = pipeline('sentiment-analysis', \n",
    "                              model=model, \n",
    "                              tokenizer=tokenizer,\n",
    "                              framework='tf')  # Explicitly use TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser le sentiment d'une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Ce journal est vraiment super intéressant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Cette phrase est négative et je ne suis pas content !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
